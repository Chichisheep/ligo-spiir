#!/usr/bin/python

import sys
import numpy
from scipy import interpolate
from glue.ligolw import lsctables
from glue.ligolw import utils
from glue.ligolw.utils import process as ligolw_process
from glue.ligolw.utils import segments as ligolw_segments
from pylal import inject
from pylal import rate
from optparse import OptionParser
from gstlal import ligolw_output as gstlal_likelihood
from gstlal.svd_bank import read_bank
try:
	import sqlite3
except ImportError:
	# pre 2.5.x
	from pysqlite2 import dbapi2 as sqlite3

def smooth_bins(bA, stride = 5):
	wn = rate.gaussian_window2d(3*stride,3*stride, sigma=10)
	rate.filter_array(bA.array.T,wn)

def decimate_array(arr, stride=5):
	return arr[::stride, ::stride]

def linearize_array(arr):
	return arr.reshape((1,arr.shape[0] * arr.shape[1]))

def get_nonzero(arr):
	return arr[arr != 0]

def possible_ranks_array(A, B, Alt, Blt, delta_t):
	out = numpy.outer(A, B) * 2. * delta_t / Alt / Blt
	out = out.reshape((out.shape[0] * out.shape[1],))
	out.sort()
	return out

def FAP_from_ranks(ranks):
	"""
	ranks should be sorted
	"""
	FAP = (numpy.arange(len(ranks))+1.) / len(ranks)
	return interpolate.interp1d(ranks, FAP, fill_value=0., bounds_error=False)
	
def FAR_from_FAP(faps, t):
	return 0. - numpy.log(1.-faps) / t

def parse_command_line():
	parser = OptionParser(
		description = __doc__
	)
	parser.add_option("--background-bins-file", metavar = "filename", action = "append", help = "Set the name of the xml file containing the snr / chisq background distributions")
	parser.add_option("--segments-file", metavar = "filename", help = "Set the name of the xml file containing analysis segments.")
	parser.add_option("--segments-name", metavar = "name", default = "RESULT", help = "Set the name of the analysis segments (default = 'RESULT').")
	parser.add_option("--vetoes-file", metavar = "filename", help = "Set the name of the xml file containing the veto segments.")
	parser.add_option("--vetoes-name", metavar = "name", default = "vetoes", help = "Set the name of the vetoes segments (default = 'vetoes').")
	parser.add_option("--stride", metavar = "int", type="int", default=5, help = "set the stride to decimate the bins, default 5")
	parser.add_option("--additional-trials-factor", metavar = "int", type="int", default=1, help = "set an additional trials factor to apply to the rate, beyond what is measured from the SVD.  Default is 1.")
	parser.add_option("--tmp-space", metavar = "dir", help = "Set the name of the tmp space if working with sqlite")
	parser.add_option("--verbose", "-v", action = "store_true", help = "Be verbose.")

	options, filenames = parser.parse_args()
	return options, filenames


# Parse command line
options, filenames = parse_command_line()


# load segment data
segments = ligolw_segments.segmenttable_get_by_name(utils.load_filename(options.segments_file, verbose = options.verbose), options.segments_name).coalesce()
vetoes = ligolw_segments.segmenttable_get_by_name(utils.load_filename(options.vetoes_file, verbose = options.verbose), options.vetoes_name).coalesce()

#FIXME for now our live time is defined by this
H1L1livetime = float(abs((segments - vetoes).intersection(set(("H1","L1")))))

# retrieve rank data
coincparamsdistributions, likelihood_seglists = gstlal_likelihood.load_likelihood_data(options.background_bins_file, verbose = options.verbose)
print >>sys.stderr, "smoothing bin counts ..."
#coincparamsdistributions.finish(filters = gstlal_likelihood.DistributionsStats.filters, verbose = options.verbose)
# FIXME:  the smoothing should be done with the .finish() method.
# FIXME:  make sure the smoothing is what is intended:  what's coded here
# is neither computing a sliding average of bin counts, nor an event rate
# density.
counts = coincparamsdistributions.background_rates
for binned_array in counts.values():
	smooth_bins(binned_array, stride = options.stride)

# compute FAP mapping preliminaries
# FIXME only works for H1 / L1, dont' know how to handle correlated H2, not a problem for *this* search?
print >>sys.stderr, "computing FAP map preliminaries..."
H1nonzero = get_nonzero(linearize_array(decimate_array(counts["H1_snr_chi"].array, stride = options.stride)))
L1nonzero = get_nonzero(linearize_array(decimate_array(counts["L1_snr_chi"].array, stride = options.stride)))

# iterate over files to rank
for f in filenames:
	
	#
	# get the trigger document
	#

	# check for sqlite
	if f.endswith('.sqlite'):
		from glue.ligolw import dbtables
		working_filename = dbtables.get_connection_filename(f, tmp_path = options.tmp_space, verbose = options.verbose)
		connection = sqlite3.connect(working_filename)
		triggerdoc = dbtables.get_xml(connection)
	else:
		triggerdoc = utils.load_filename(f, verbose = options.verbose)

	print >>sys.stderr, "computing live time ..."
	# intersect segments with search_summary segments and remove vetoes
	search_summary_segs = lsctables.table.get_table(triggerdoc, lsctables.SearchSummaryTable.tableName).get_in_segmentlistdict(lsctables.table.get_table(triggerdoc, lsctables.ProcessTable.tableName).get_ids_by_program("gstlal_inspiral")).coalesce()
	livetime = dict((instrument, float(abs(seglist))) for instrument, seglist in ((segments & search_summary_segs) - vetoes).items())


	print >>sys.stderr, "indexing tables ..."
	# index for rapid sngl_inspiral and coinc_inspiral row look-up
	index = dict((row.event_id, row) for row in lsctables.table.get_table(triggerdoc, lsctables.SnglInspiralTable.tableName))
	index.update(dict((row.coinc_event_id, row) for row in lsctables.table.get_table(triggerdoc, lsctables.CoincInspiralTable.tableName)))
	# index for rapid coinc tuple look-up
	coinc_index = {}
	for row in lsctables.table.get_table(triggerdoc, lsctables.CoincMapTable.tableName):
		if row.table_name == "sngl_inspiral":
			coinc_index.setdefault(row.coinc_event_id, set()).add(index[row.event_id])


	# FIXME only works for H1,L1
	# compute rates
	print >>sys.stderr, "computing rates ..."
	coincidence_threshold, = ligolw_process.get_process_params(triggerdoc, "gstlal_inspiral", "--coincidence-threshold")
	delta_t = coincidence_threshold + inject.light_travel_time("H1", "L1")
	ranks = possible_ranks_array(H1nonzero, L1nonzero, livetime["H1"], livetime["L1"], delta_t)
	faps = FAP_from_ranks(ranks)
	coinc_inspiral_table = lsctables.table.get_table(triggerdoc, lsctables.CoincInspiralTable.tableName)
	N = len(coinc_inspiral_table)
	for n, coinc_inspiral in enumerate(coinc_inspiral_table):
		if not n % 531:
			print >>sys.stderr, "\t%d / %d\r" % (n, N),
		instruments = coinc_inspiral.get_ifos()
		# if H1 and L1 don't participate, skip
		if not set(("H1", "L1")).issubset(instruments):
			coinc_inspiral.false_alarm_rate = None
			continue
		# retrieve \rho and \chi^{2} values
		rho_chi = dict((event.ifo, (event.snr, event.chisq**.5 / event.snr)) for event in coinc_index[coinc_inspiral.coinc_event_id])
		# assign rate
		coinc_inspiral.false_alarm_rate = (counts["H1_snr_chi"][rho_chi["H1"]] / livetime["H1"]) * (counts["L1_snr_chi"][rho_chi["L1"]] / livetime["L1"]) * 2. * delta_t
		# FIXME Hack to avoid going off the edge in interp
		if coinc_inspiral.false_alarm_rate > ranks[-2]:
			coinc_inspiral.false_alarm_rate = ranks[-2]
		coinc_inspiral.combined_far = options.additional_trials_factor * FAR_from_FAP(faps(coinc_inspiral.false_alarm_rate), H1L1livetime)

	if f.endswith('.sqlite'):
		connection.commit()
		connection.close()
		dbtables.put_connection_filename(f, working_filename, verbose = options.verbose)
	else:
		# write output
		utils.write_filename(triggerdoc, f, gz = True, verbose = options.verbose)
