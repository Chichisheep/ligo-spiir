#!/usr/bin/env python
#
# Copyright (C) 2010--2013  Kipp Cannon, Chad Hanna
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

## @file
# A program to compute the likelhood ratios of inspiral triggers
#
# ### Command line interface
#
#	+ `--input-cache` [filename]: Also process the files named in this LAL cache.  See lalapps_path2cache for information on how to produce a LAL cache file.
#	+ `--likelihood-url` [URL]: Set the name of the likelihood ratio data file to use.  Can be given more than once.  Filenames and URLs are accepted.
#	+ `--likelihood-cache` [filename]: Also load the likelihood ratio data files listsed in this LAL cache.  See lalapps_path2cache for information on how to produce a LAL cache file.
#	+ `--tmp-space` [path]: Path to a directory suitable for use as a work area while manipulating the database file.  The database file will be worked on in this directory, and then moved to the final location when complete.  This option is intended to improve performance when running in a networked environment, where there might be a local disk with higher bandwidth than is available to the filesystem on which the final output will reside.
#	+ `--vetoes-name` [name]: Set the name of the segment lists to use as vetoes (default = do not apply vetoes).
#	+ `--verbose`: Be verbose.
#	+ `--synthesize-injections` [N] (int): Synthesize an injection distribution with N injections (default = 0).
#	+ `--background-prior` [N] (float): Include an exponential background prior with the maximum bin count = N, default 0, no additional prior.
#	+ `--write-likelihood` [filename]: Write merged raw likelihood data to this file.
#	+ `--trim-database`: Delete events that are found to be below the pipeline's likelihood ratio threshold (default = do not delete them).  Deleting the events saves a significant amount of disk space but is inconvenient during pipeline development and tuning as it makes it impossible to rerank the events later with a different ranking statistic.

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


import multiprocessing
import multiprocessing.queues	# not needed in >= 2.7
from optparse import OptionParser
try:
	import sqlite3
except ImportError:
	# pre 2.5.x
	from pysqlite2 import dbapi2 as sqlite3
import sys


from glue import iterutils
from glue.lal import CacheEntry
from glue.text_progress_bar import ProgressBar
from glue.ligolw import ligolw
from glue.ligolw import dbtables
from glue.ligolw import lsctables
from glue.ligolw import utils as ligolw_utils
from glue.ligolw.utils import process as ligolw_process
from glue.ligolw.utils import search_summary as ligolw_search_summary
from glue.ligolw.utils import segments as ligolw_segments
from glue import segments
from pylal import ligolw_burca2
from pylal import ligolw_thinca
from pylal import snglcoinc
from gstlal import far
from gstlal import inspiral


__author__ = "Kipp Cannon <kipp.cannon@ligo.org>"
__version__ = "git id %s" % ""	# FIXME
__date__ = ""	# FIXME


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		version = "Name: %%prog\n%s" % "" # FIXME
	)
	parser.add_option("-c", "--input-cache", metavar = "filename", help = "Also process the files named in this LAL cache.  See lalapps_path2cache for information on how to produce a LAL cache file.")
	parser.add_option("-l", "--likelihood-url", metavar = "URL", action = "append", help = "Set the name of the likelihood ratio data file to use.  Can be given more than once.  Filenames and URLs are accepted.")
	parser.add_option("--likelihood-cache", metavar = "filename", help = "Also load the likelihood ratio data files listsed in this LAL cache.  See lalapps_path2cache for information on how to produce a LAL cache file.")
	parser.add_option("-t", "--tmp-space", metavar = "path", help = "Path to a directory suitable for use as a work area while manipulating the database file.  The database file will be worked on in this directory, and then moved to the final location when complete.  This option is intended to improve performance when running in a networked environment, where there might be a local disk with higher bandwidth than is available to the filesystem on which the final output will reside.")
	parser.add_option("--vetoes-name", metavar = "name", help = "Set the name of the segment lists to use as vetoes (default = do not apply vetoes).")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")
	parser.add_option("-s", "--synthesize-injections", metavar = "N", type = "int", default = 0, help = "Synthesize an injection distribution with N injections, default 0")
	parser.add_option("-p", "--background-prior", metavar = "N", default = 0, type = "float", help = "include an exponential background prior with the maximum bin count = N, default 0, no additional prior")
	parser.add_option("--write-likelihood", metavar = "filename", help = "Write merged raw likelihood data to this file.")
	parser.add_option("--trim-database", action = "store_true", help = "Delete events that are found to be below the pipeline's likelihood ratio threshold (default = do not delete them).  Deleting the events saves a significant amount of disk space but is inconvenient during pipeline development and tuning as it makes it impossible to rerank the events later with a different ranking statistic.")
	options, filenames = parser.parse_args()

	options.likelihood_urls = []
	if options.likelihood_urls is not None:
		options.likelihood_urls += options.likelihood_url
	if options.likelihood_cache is not None:
		options.likelihood_urls += [CacheEntry(line).url for line in open(options.likelihood_cache)]
	if not options.likelihood_urls:
		raise ValueError("no likelihood URLs specified")

	if options.input_cache:
		filenames += [CacheEntry(line).path for line in open(options.input_cache)]

	return options, filenames


#
# =============================================================================
#
#                    Generate Ranking Statistic Histograms
#
# =============================================================================
#


def generate_ranking_data(queue, coincparamsdistributions, seglists, verbose = False):
	queue.put(far.RankingData(coincparamsdistributions, seglists.keys(), coincparamsdistributions.process_id, verbose = verbose))


#
# =============================================================================
#
#                   Support Funcs for Likelihood Ratio Code
#
# =============================================================================
#


def sngl_inspiral_events_func(cursor, coinc_event_id, row_from_cols):
	return map(row_from_cols, cursor.execute("""
SELECT
	sngl_inspiral.*
FROM
	sngl_inspiral
	JOIN coinc_event_map ON (
		coinc_event_map.table_name == 'sngl_inspiral'
		AND coinc_event_map.event_id == sngl_inspiral.event_id
	)
WHERE
	coinc_event_map.coinc_event_id == ?
	""", (coinc_event_id,)))


def sngl_inspiral_veto_func(event, vetoseglists):
	# return True if event should be *retained*
	return event.ifo not in vetoseglists or event.get_end() not in vetoseglists[event.ifo]


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#


#
# command line
#


options, filenames = parse_command_line()


#
# retrieve horizon distance data
#


if options.verbose:
	print >>sys.stderr, "retrieving horizon distances ..."

horizon_distances = {}
for n, filename in enumerate(filenames, 1):
	#
	# open the database file, do the query in-place
	#

	if options.verbose:
		print >>sys.stderr, "%d/%d: %s" % (n, len(filenames), filename)

	working_filename = dbtables.get_connection_filename(filename, tmp_path = None, verbose = options.verbose)
	connection = sqlite3.connect(working_filename)

	#
	# retrieve horizon distances
	#

	for instruments, start_time, start_time_ns, name, dist in connection.cursor().execute("""
SELECT
	summ_value.ifo,
	summ_value.start_time,
	summ_value.start_time_ns,
	summ_value.name,
	summ_value.value
FROM
	summ_value
WHERE
	program == "gstlal_inspiral"
	AND name LIKE "Dh:%";
	"""):
		# FIXME:  check that these are what we expect?
		m1, m2, snr_threshold = inspiral.CoincsDocument.summ_value_name_decode(name)
		for instrument in lsctables.instrument_set_from_ifos(instruments):
			horizon_distances.setdefault(instrument, []).append((lsctables.LIGOTimeGPS(start_time, start_time_ns), dist))

	connection.close()
	dbtables.discard_connection_filename(filename, working_filename, verbose = options.verbose)

#
# put horizon distances for each instrument in time order
#

for dists in horizon_distances.values():
	dists.sort()

# FIXME:  for now we just take the average of the distances recorded for
# each instrument.  this needs to be worked out properly so that events are
# ranked with the appropriate distance
horizon_distances = dict((instrument, sum(dist for t, dist in dists) / len(dists)) for instrument, dists in horizon_distances.items())

if options.verbose:
	print >>sys.stderr, "done retrieving horizon distances"
	for instrument, dist in sorted(horizon_distances.items()):
		print >>sys.stderr, "\t%s:  %g Mpc" % (instrument, dist)


#
# load parameter distribution data
#


coincparamsdistributions = None
seglists = segments.segmentlistdict()
for n, likelihood_url in enumerate(options.likelihood_urls, start = 1):
	if options.verbose:
		print >>sys.stderr, "%d/%d:" % (n, len(options.likelihood_urls)),
	xmldoc = ligolw_utils.load_url(likelihood_url, contenthandler = far.ThincaCoincParamsDistributions.LIGOLWContentHandler, verbose = options.verbose)
	this_coincparamsdistributions, ignored, this_seglists = far.parse_likelihood_control_doc(xmldoc)
	xmldoc.unlink()
	if this_coincparamsdistributions is None:
		raise ValueError("%s does not contain parameter distribution data" % likelihood_url)
	if coincparamsdistributions is None:
		coincparamsdistributions = this_coincparamsdistributions
	else:
		coincparamsdistributions += this_coincparamsdistributions
	seglists |= this_seglists
if options.verbose:
	print >>sys.stderr, "total livetime:\n\t%s" % ",\n\t".join("%s = %s s" % (instrument, str(abs(segs))) for instrument, segs in seglists.items())

# calculate injections before writing to disk
if options.synthesize_injections != 0:
	coincparamsdistributions.add_foreground_prior(n = options.synthesize_injections, segs = seglists, horizon_distances = horizon_distances, verbose = options.verbose)

# add a uniform prior to background, by default 0 is added so it has no effect
if options.background_prior != 0:
	coincparamsdistributions.add_background_prior(n = options.background_prior, segs = seglists, verbose = options.verbose)

# compute the instrument combination counts
coincparamsdistributions.add_instrument_combination_counts(segs = seglists, verbose = options.verbose)

#
# rebuild event parameter PDFs (+= method has not constructed these
# correctly, and we might have added additional priors to the histograms),
# then initialize likeihood ratio evaluator
#


coincparamsdistributions.finish(verbose = options.verbose)


coincparamsdistributions.horizon_distances = horizon_distances	# FIXME:  band-aid for coinc params func.  remove
likelihood_ratio_func = snglcoinc.LikelihoodRatio(coincparamsdistributions)


#
# dump combined distribution data file if requested.  we generate
# likelihood ratio histograms at this time, as well, even though we don't
# need them here because this helps parallelize that step in a full-scale
# analysis.  the program that reads these data files will need those
# histograms, and would take ages to build them itself, serially, as a
# single job.  We're about to spend a few hours processing database files
# and can generate those histograms in the background while we do
#


threads = []
if options.write_likelihood is not None:
	# take a moment to make sure we have SNR PDFs for all instrument combinations
	for instruments in [instruments for n in range(2, len(seglists) + 1) for instruments in iterutils.choices(seglists.keys(), n)]:
		coincparamsdistributions.get_snr_joint_pdf(instruments, horizon_distances, progressbar = ProgressBar() if options.verbose else None)
	q = multiprocessing.queues.SimpleQueue()
	p = multiprocessing.Process(target = generate_ranking_data, args = (q, coincparamsdistributions, seglists), kwargs = {"verbose": options.verbose})
	p.start()
	threads.append((p, q, options.write_likelihood))


#
# iterate over files
#


for n, filename in enumerate(filenames, 1):
	#
	# Open the database file.
	#

	if options.verbose:
		print >>sys.stderr, "%d/%d: %s" % (n, len(filenames), filename)

	if sqlite3.connect(filename).cursor().execute("""SELECT EXISTS(SELECT * FROM process WHERE program == ?);""", (u"gstlal_inspiral_calc_likelihood",)).fetchone()[0]:
		if options.verbose:
			print >>sys.stderr, "already processed, skipping"
		continue

	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
	connection = sqlite3.connect(working_filename)
	if options.tmp_space is not None:
		dbtables.set_temp_store_directory(connection, options.tmp_space, verbose = options.verbose)
	xmldoc = dbtables.get_xml(connection)

	#
	# Summarize the database, and record our passage.
	#

	try:
		coinc_def_id = lsctables.CoincDefTable.get_table(xmldoc).get_coinc_def_id(ligolw_thinca.InspiralCoincDef.search, ligolw_thinca.InspiralCoincDef.search_coinc_type, create_new = False)
	except KeyError:
		if options.verbose:
			print >>sys.stderr, "document does not contain inspiral coincidences.  skipping."
		xmldoc.unlink()
		connection.commit()
		connection.close()
		dbtables.discard_connection_filename(filename, working_filename, verbose = options.verbose)
		continue

	process = ligolw_process.register_to_xmldoc(xmldoc, u"gstlal_inspiral_calc_likelihood", {})
	search_summary = ligolw_search_summary.append_search_summary(xmldoc, process, ifos = seglists.keys(), inseg = seglists.extent_all(), outseg = seglists.extent_all())

	sngl_inspiral_table = lsctables.SnglInspiralTable.get_table(xmldoc)

	offset_vectors = lsctables.TimeSlideTable.get_table(xmldoc).as_dict()

	if options.vetoes_name is not None:
		vetoseglists = ligolw_segments.segmenttable_get_by_name(xmldoc, options.vetoes_name).coalesce()
	else:
		vetoseglists = segments.segmentlistdict()

	#
	# Run likelihood ratio calculation.
	#

	ligolw_burca2.assign_likelihood_ratios(
		connection = connection,
		coinc_def_id = coinc_def_id,
		offset_vectors = offset_vectors,
		vetoseglists = vetoseglists,
		events_func = lambda cursor, coinc_event_id: sngl_inspiral_events_func(cursor, coinc_event_id, sngl_inspiral_table.row_from_cols),
		veto_func = sngl_inspiral_veto_func,
		likelihood_ratio_func = likelihood_ratio_func,
		likelihood_params_func = coincparamsdistributions.coinc_params,
		verbose = options.verbose
	)

	#
	# Delete low significance events to reduce database size
	#

	if options.trim_database:
		cursor = connection.cursor()
		cursor.execute("""DELETE FROM coinc_event WHERE coinc_def_id == ? AND likelihood < ?;""", (coinc_def_id, far.RankingData.likelihood_ratio_threshold))
		cursor.execute("""DELETE FROM coinc_inspiral WHERE coinc_event_id NOT IN (SELECT coinc_event_id FROM coinc_event);""")
		cursor.execute("""DELETE FROM coinc_event_map WHERE coinc_event_id NOT IN (SELECT coinc_event_id FROM coinc_event);""")
		# FIXME:  don't hard-code parameter name
		cursor.execute("""DELETE FROM sngl_inspiral WHERE snr < (SELECT value FROM process_params WHERE process_params.process_id == sngl_inspiral.process_id AND param == "--singles-threshold") AND event_id NOT IN (SELECT event_id FROM coinc_event_map WHERE table_name == "sngl_inspiral");""")
		cursor.execute("""VACUUM;""")
		cursor.close()

	#
	# Close out process metadata
	#

	ligolw_process.set_process_end_time(process)
	connection.cursor().execute("UPDATE process SET end_time = ? WHERE process_id == ?", (process.end_time, process.process_id))

	#
	# Clean up.
	#

	xmldoc.unlink()
	connection.commit()
	connection.close()
	dbtables.put_connection_filename(filename, working_filename, verbose = options.verbose)


#
# Collect the PDF writing thread if one was created and write the parameter
# and ranking statistic distribution data to a file
#


while threads:
	p, q, filename = threads.pop(0)
	ranking_data = q.get()
	p.join()
	if p.exitcode:
		raise Exception("far.RankingData() thread failed")
	xmldoc = ligolw.Document()
	xmldoc.appendChild(ligolw.LIGO_LW())
	process = ligolw_process.register_to_xmldoc(xmldoc, u"gstlal_inspiral_calc_likelihood", paramdict = {})
	search_summary = ligolw_search_summary.append_search_summary(xmldoc, process, ifos = seglists.keys(), inseg = seglists.extent_all(), outseg = seglists.extent_all())
	far.gen_likelihood_control_doc(xmldoc, process, coincparamsdistributions, ranking_data, seglists)
	ligolw_process.set_process_end_time(process)
	ligolw_utils.write_filename(xmldoc, filename, gz = (filename or "stdout").endswith(".gz"), verbose = options.verbose)
	assert not threads
