#!/usr/bin/env python
#
# Copyright (C) 2010  Kipp Cannon, Chad Hanna
# Copyright (C) 2009  Kipp Cannon, Chad Hanna
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


import math
import numpy
from optparse import OptionParser
import sys
import os.path

import gobject
import pygst
pygst.require("0.10")
import gst


gobject.threads_init()


from glue import segments
from glue import segmentsUtils
from glue.ligolw import ligolw
from glue.ligolw import lsctables
from glue.ligolw import utils
from glue.ligolw.utils import process as ligolw_process
from pylal import series as lalseries
from pylal.datatypes import LIGOTimeGPS
from pylal.xlal.datatypes.snglinspiraltable import from_buffer as sngl_inspirals_from_buffer


from gstlal import pipeio
from gstlal import pipeparts
from gstlal import cbc_template_fir
from gstlal import misc as gstlalmisc
from gstlal import templates


#
# =============================================================================
#
#                                  Utilities
#
# =============================================================================
#

#
# Read Approximant
#

def read_approximant(xmlfile):
        xmldoc=utils.load_filename(xmlfile, gz = (xmlfile or "stdin").endswith(".gz"))
        approximant=ligolw_process.get_process_params(xmldoc, "tmpltbank", "--approximant")
        #approximant=approximant[0].lstrip("u") #FIXME:Should not need to remove u, where is it coming from?
        approximant=approximant[0]
        print approximant

        supported_approximants=[u'FindChirpSP', u'IMRPhenomB']
        print supported_approximants
        if approximant not in supported_approximants:
                print approximant
                raise ValueError, "unsupported approximant"
        else:
                pass
        return approximant

#
#check final frequency is populated and return a list of the final frequencies
#

def check_ffinal(xmlfile):
	xmldoc=utils.load_filename(xmlfile, gz = (xmlfile or "stdin").endswith(".gz"))
	sngl_inspiral_table=ligolw.table.get_table(xmldoc, "sngl_inspiral")
	frows=[(frows.append(row.f_final)) for row in sngl_inspiral_table]
	if frows[0] == 0.0:
		raise ValueError, "ffinal column needs to be populated"
	return 1
#
#determine max final frequency (ffinal)
#

def max_ffinal(xmlfile, check_ffinal_return):
	if check_ffinal_return==1:
		xmldoc=utils.load_filename(xmlfile, gz = (xmlfile or "stdin").endswith(".gz"))
        	sngl_inspiral_table=ligolw.table.get_table(xmldoc, "sngl_inspiral")
		sngl_inspiral_table.sort(lambda a,b: cmp(a.ffinal,b.ffinal)
		return sngl_inspiral_table[0].f_final
	else:
		raise ValueError, "ffinal column must be populated"

#
# sum-of-squares false alarm probability
#


def sum_of_squares_threshold_from_fap(fap, coefficients):
	return gstlalmisc.cdf_weighted_chisq_Pinv(coefficients, numpy.zeros(coefficients.shape, dtype = "double"), numpy.ones(coefficients.shape, dtype = "int"), 0.0, 1.0 - fap, -1, fap / 16.0)


#
# add metadata to an xml document in the style of lalapps_inspiral
#


def add_cbc_metadata(xmldoc, process, seg):
	#
	# add entry to search_summary table
	#

	try:
		tbl = lsctables.table.get_table(xmldoc, lsctables.SearchSummaryTable.tableName)
	except ValueError:
		tbl = xmldoc.childNodes[-1].appendChild(lsctables.New(lsctables.SearchSummaryTable))
	search_summary = tbl.RowType()
	search_summary.process_id = process.process_id
	search_summary.shared_object = None # FIXME
	search_summary.lalwrapper_cvs_tag = None # FIXME
	search_summary.lal_cvs_tag = None # FIXME
	search_summary.comment = process.comment
	search_summary.set_ifos(process.get_ifos())
	search_summary.set_in(seg) # FIXME
	search_summary.set_out(seg) # FIXME
	search_summary.nevents = None # FIXME
	search_summary.nnodes = 1
	tbl.append(search_summary)

	#
	# add entry to filter table
	#

	try:
		tbl = lsctables.table.get_table(xmldoc, lsctables.FilterTable.tableName)
	except ValueError:
		tbl = xmldoc.childNodes[-1].appendChild(lsctables.New(lsctables.FilterTable))
	tbl.sync_next_id()
	row = tbl.RowType()
	row.process_id = process.process_id
	row.program = process.program
	row.start_time = int(seg[0])
	row.filter_name = None # FIXME
	row.filter_id = tbl.get_next_id()
	row.param_set = None # FIXME
	row.comment = process.comment
	tbl.append(row)

	#
	# add entries to search_summvars table
	#

	try:
		tbl = lsctables.table.get_table(xmldoc, lsctables.SearchSummVarsTable.tableName)
	except ValueError:
		tbl = xmldoc.childNodes[-1].appendChild(lsctables.New(lsctables.SearchSummVarsTable))
	tbl.sync_next_id()
	# FIXME

	#
	# add entries to summ_value table
	#

	try:
		tbl = lsctables.table.get_table(xmldoc, lsctables.SummValueTable.tableName)
	except ValueError:
		tbl = xmldoc.childNodes[-1].appendChild(lsctables.New(lsctables.SummValueTable))
	tbl.sync_next_id()
	# FIXME

	#
	# done
	#

	return search_summary


#
# =============================================================================
#
#                              Pipeline Metadata
#
# =============================================================================
#


class DetectorData(object):
	# default block_size = 16384 samples/second * 8 bytes/sample * 8
	# second
	def __init__(self, frame_cache, channel, block_size = 16384 * 8 * 8):
		self.frame_cache = frame_cache
		self.channel = channel
		self.block_size = block_size


class Bank(object):
	class BankFragment(object):
		def __init__(self, rate, start, end):
			self.rate = rate
			self.start = start
			self.end = end

		def set_template_bank(self, template_bank, tolerance, verbose = False):
			if verbose:
				print >>sys.stderr, "\t%d templates of %d samples" % template_bank.shape

			self.orthogonal_template_bank, self.singular_values, self.mix_matrix, self.chifacs = cbc_template_fir.decompose_templates(template_bank, tolerance)

			self.sum_of_squares_weights = self.singular_values * math.sqrt(self.chifacs.mean() / numpy.dot(self.singular_values, self.singular_values))

			if verbose:
				print >>sys.stderr, "\tidentified %d components" % self.orthogonal_template_bank.shape[0]
				print >>sys.stderr, "\tsum-of-squares expectation value is %g" % self.chifacs.mean()

	def __init__(self, template_bank_filename, psd, time_freq_boundaries, gate_fap, snr_threshold, tolerance, flow = 40.0, autocorrelation_length = None, logname = None, verbose = False):
		self.template_bank_filename = template_bank_filename	# FIXME:  remove when not needed by trigger generator element
		self.filter_length = max(end for rate,begin,end in time_freq_boundaries)
		self.snr_threshold = snr_threshold
		self.logname = logname

		template_bank_table = lsctables.table.get_table(utils.load_filename(template_bank_filename, gz = (template_bank_filename or "stdin").endswith(".gz"), verbose = verbose), lsctables.SnglInspiralTable.tableName)
		template_bank, self.autocorrelation_bank = cbc_template_fir.generate_templates(template_bank_table, read_approximant(template_bank_filename), psd, flow, time_freq_boundaries, autocorrelation_length = autocorrelation_length, verbose = verbose)

		# Assign template banks to fragments
		self.bank_fragments = [Bank.BankFragment(rate,begin,end) for rate,begin,end in time_freq_boundaries]
		for i, bank_fragment in enumerate(self.bank_fragments):
			if verbose:
				print >>sys.stderr, "constructing template decomposition %d of %d:  %g s ... %g s" % (i + 1, len(self.bank_fragments), -bank_fragment.end, -bank_fragment.start)
			bank_fragment.set_template_bank(template_bank[i], tolerance, verbose = verbose)

		self.gate_threshold = sum_of_squares_threshold_from_fap(gate_fap, numpy.array([weight**2 for bank_fragment in self.bank_fragments for weight in bank_fragment.sum_of_squares_weights], dtype = "double"))
		if verbose:
			print >>sys.stderr, "sum-of-squares threshold for false-alarm probability of %.16g:  %.16g" % (gate_fap, self.gate_threshold)

	def get_rates(self):
		return set(bank_fragment.rate for bank_fragment in self.bank_fragments)


#
# =============================================================================
#
#                              Pipeline Elements
#
# =============================================================================
#


#
# sum-of-squares aggregator
#


def mkcontrolsnksrc(pipeline, rate, verbose = False, suffix = None):
	snk = gst.element_factory_make("lal_adder")
	snk.set_property("sync", True)
	pipeline.add(snk)
	src = pipeparts.mkcapsfilter(pipeline, snk, "audio/x-raw-float, rate=%d" % rate)
	if verbose:
		src = pipeparts.mkprogressreport(pipeline, src, "progress_sumsquares%s" % (suffix and "_%s" % suffix or ""))
	src = pipeparts.mktee(pipeline, src)
	return snk, src


#
# data source
#


def mkLLOIDsrc(pipeline, instrument, detector, rates, psd = None, psd_fft_length = 8, fake_data = False, injection_filename = None, verbose = False, nxydump_segment = None):
	#
	# data source and progress report
	#

	if fake_data:
		head = pipeparts.mkfakeLIGOsrc(pipeline, instrument = instrument, blocksize = detector.block_size, location = detector.frame_cache, channel_name = detector.channel)
	else:
		head = pipeparts.mkframesrc(pipeline, instrument = instrument, blocksize = detector.block_size, location = detector.frame_cache, channel_name = detector.channel)
	if verbose:
		head = pipeparts.mkprogressreport(pipeline, head, "progress_src_%s" % instrument)

	#
	# optional injections
	#

	if injection_filename is not None:
		head = pipeparts.mkinjections(pipeline, head, injection_filename)

	#
	# down-sample to highest of target sample rates.  note:  there is
	# no check that this is, infact, *down*-sampling.  if the source
	# time series has a lower sample rate this will up-sample the data.
	# up-sampling will probably interact poorly with the whitener as it
	# will likely add (possibly significant) numerical noise when it
	# amplifies the non-existant high-frequency components
	#

	source_rate = max(rates)
	head = pipeparts.mkcapsfilter(pipeline, pipeparts.mkresample(pipeline, pipeparts.mkqueue(pipeline, head), quality = 9), "audio/x-raw-float, rate=%d" % source_rate)

	#
	# whiten
	#

	if psd is not None:
		#
		# use fixed PSD
		#

		head = pipeparts.mkwhiten(pipeline, head, fft_length = psd_fft_length, psd_mode = 1)

		#
		# install signal handler to retrieve \Delta f when it is
		# known, resample the user-supplied PSD, and install it
		# into the whitener.
		#

		def f_nyquist_changed(elem, pspec, psd):
			# get frequency resolution and number of bins
			delta_f = elem.get_property("delta-f")
			n = int(round(elem.get_property("f-nyquist") / delta_f) + 1)
			# interpolate and install PSD
			psd = cbc_template_fir.interpolate_psd(psd, delta_f)
			elem.set_property("mean-psd", psd.data[:n])

		head.connect_after("notify::f-nyquist", f_nyquist_changed, psd)
	else:
		#
		# use running average PSD
		#

		head = pipeparts.mkwhiten(pipeline, head, fft_length = psd_fft_length)
	head = pipeparts.mknofakedisconts(pipeline, head)	# FIXME:  remove after basetransform behaviour fixed

	#
	# down-sample whitened time series to remaining target sample rates
	# while applying an amplitude correction to adjust for low-pass
	# filter roll-off.  we also scale by \sqrt{original rate / new
	# rate}.  this is done to preserve the square magnitude of the time
	# series --- the inner product of the time series with itself.
	# really what we want is for
	#
	#	\int v_{1}(t) v_{2}(t) \diff t
	#		\approx \sum v_{1}(t) v_{2}(t) \Delta t
	#
	# to be preserved across different sample rates, i.e. for different
	# \Delta t.  what we do is rescale the time series and ignore
	# \Delta t, so we put 1/2 factor of the ratio of the \Delta t's
	# into the h(t) time series here, and, later, another 1/2 factor
	# into the template when it gets downsampled.
	#

	# FIXME:  re-measure these;  we've changed the quality of the
	# resampler since these were measured.
	correction = {
		64: 1 / 0.961,
		128: 1 / 0.961,
		256: 1 / 0.962,
		512: 1 / 0.962,
		1024: 1 / 0.962,
		2048: 1 / 0.999
	}

	head = {source_rate: pipeparts.mktee(pipeline, head)}
	for rate in sorted(rates, reverse = True)[1:]:	# all but the highest rate
		head[rate] = pipeparts.mktee(pipeline, pipeparts.mkcapsfilter(pipeline, pipeparts.mkresample(pipeline, pipeparts.mkaudioamplify(pipeline, head[source_rate], math.sqrt(correction[rate] * float(source_rate) / rate)), quality = 9), "audio/x-raw-float, rate=%d" % rate))

	#
	# done.  return value is a dictionary of tee elements indexed by
	# sample rate
	#

	#for rate, elem in head.items():
	#	pipeparts.mknxydumpsink(pipeline, pipeparts.mkqueue(pipeline, elem), "src_%d.dump" % rate, segment = nxydump_segment)
	return head


#
# one instrument, one template bank
#


def mkLLOIDbranch(pipeline, src, bank, bank_fragment, (control_snk, control_src), gate_attack_length, gate_hold_length):
	logname = "%s_%d_%d" % (bank.logname, bank_fragment.start, bank_fragment.end)

	#
	# FIR filter bank
	#
	# FIXME:  why the -1?  without it the pieces don't match but I
	# don't understand where this offset comes from.  it might really
	# need to be here, or it might be a symptom of a bug elsewhere.
	# figure this out.

	src = pipeparts.mktee(pipeline, pipeparts.mkreblock(pipeline, pipeparts.mkfirbank(pipeline, src, latency = -int(round(bank_fragment.start * bank_fragment.rate)) - 1, fir_matrix = bank_fragment.orthogonal_template_bank)))
	#pipeparts.mkvideosink(pipeline, pipeparts.mkcapsfilter(pipeline, pipeparts.mkhistogram(pipeline, src), "video/x-raw-rgb, width=640, height=480, framerate=1/4"))
	#pipeparts.mkogmvideosink(pipeline, pipeparts.mkcapsfilter(pipeline, pipeparts.mkchannelgram(pipeline, pipeparts.mkqueue(pipeline, src), plot_width = .125), "video/x-raw-rgb, width=640, height=480, framerate=64/1"), "orthosnr_channelgram_%s.ogv" % logname, verbose = True)

	#
	# compute weighted sum-of-squares, feed to sum-of-squares
	# aggregator
	#

	pipeparts.mkchecktimestamps(pipeline, pipeparts.mkresample(pipeline, pipeparts.mkqueue(pipeline, pipeparts.mksumsquares(pipeline, src, weights = bank_fragment.sum_of_squares_weights)), quality = 9), name = "timestamps_%s_after_sumsquare_resampler" % logname).link(control_snk)

	#
	# use sum-of-squares aggregate as gate control for orthogonal SNRs
	#

	src = pipeparts.mkgate(pipeline, pipeparts.mkqueue(pipeline, src), control = pipeparts.mkqueue(pipeline, control_src), threshold = bank.gate_threshold, attack_length = gate_attack_length, hold_length = gate_hold_length)
	src = pipeparts.mkchecktimestamps(pipeline, src, name = "timestamps_%s_after_gate" % logname)

	#
	# buffer orthogonal SNRs
	#
	# FIXME:  teach the collectpads object not to wait for buffers on
	# pads whose segments have not yet been reached by the input on the
	# other pads.  then this large queue buffer will not be required
	# because streaming can begin through the downstream adders without
	# waiting for input from all upstream elements.

	src = pipeparts.mkqueue(pipeline, src, max_size_buffers = 0, max_size_bytes = 0, max_size_time = int(math.ceil(bank.filter_length - bank_fragment.end)) * gst.SECOND)

	#
	# reconstruct physical SNRs
	#

	src = pipeparts.mkmatrixmixer(pipeline, src, matrix = bank_fragment.mix_matrix)
	src = pipeparts.mkresample(pipeline, src, quality = 9)
	src = pipeparts.mknofakedisconts(pipeline, src)	# FIXME:  remove after basetransform behaviour fixed
	src = pipeparts.mkchecktimestamps(pipeline, src, name = "timestamps_%s_after_snr_resampler" % logname)

	#
	# done
	#
	# FIXME:  find a way to use less memory without this hack

	del bank_fragment.orthogonal_template_bank
	del bank_fragment.sum_of_squares_weights
	del bank_fragment.mix_matrix
	return src


def mkLLOIDsingle(pipeline, hoftdict, instrument, detector, bank, control_snksrc, verbose = False, nxydump_segment = None):
	logname = "%s%s" % (instrument, (bank.logname and "_%s" % bank.logname or ""))

	#
	# parameters
	#

	output_rate = max(bank.get_rates())
	autocorrelation_length = bank.autocorrelation_bank.shape[1]
	autocorrelation_latency = -(autocorrelation_length - 1) / 2

	#
	# snr aggregator
	#

	snr = gst.element_factory_make("lal_adder")
	snr.set_property("sync", True)
	pipeline.add(snr)

	#
	# loop over template bank slices
	#

	for bank_fragment in bank.bank_fragments:
		branch_snr = mkLLOIDbranch(
			pipeline,
			# FIXME:  the size isn't ideal:  the correct value
			# depends on how much data is accumulated in the
			# firbank element, and the value here is only
			# approximate and not tied to the fir bank
			# parameters so might not work if those change
			pipeparts.mkqueue(pipeline, hoftdict[bank_fragment.rate], max_size_bytes = 0, max_size_buffers = 0, max_size_time = 4 * int(math.ceil(bank.filter_length)) * gst.SECOND),
			bank,
			bank_fragment,
			control_snksrc,
			int(math.ceil(-autocorrelation_latency * (float(bank_fragment.rate) / output_rate))),
			int(math.ceil(-autocorrelation_latency * (float(bank_fragment.rate) / output_rate)))
		)
		#branch_snr = pipeparts.mktee(pipeline, branch_snr)
		#pipeparts.mknxydumpsink(pipeline, pipeparts.mkqueue(pipeline, branch_snr), "snr_%s_%02d.dump" % (logname, bank_fragment.start), segment = nxydump_segment)
		branch_snr.link(snr)

	#
	# snr
	#

	snr = pipeparts.mktee(pipeline, pipeparts.mktogglecomplex(pipeline, pipeparts.mkcapsfilter(pipeline, snr, "audio/x-raw-float, rate=%d" % output_rate)))
	#pipeparts.mknxydumpsink(pipeline, pipeparts.mktogglecomplex(pipeline, pipeparts.mkqueue(pipeline, snr)), "snr_%s.dump" % logname, segment = nxydump_segment)
	#pipeparts.mkogmvideosink(pipeline, pipeparts.mkcapsfilter(pipeline, pipeparts.mkchannelgram(pipeline, pipeparts.mkqueue(pipeline, snr), plot_width = .125), "video/x-raw-rgb, width=640, height=480, framerate=64/1"), "snr_channelgram_%s.ogv" % logname, audiosrc = pipeparts.mkaudioamplify(pipeline, pipeparts.mkqueue(pipeline, hoftdict[output_rate], max_size_time = 2 * int(math.ceil(bank.filter_length)) * gst.SECOND), 0.125), verbose = True)

	#
	# \chi^{2}
	#

	chisq = pipeparts.mkautochisq(pipeline, pipeparts.mkqueue(pipeline, snr), autocorrelation_matrix = bank.autocorrelation_bank, latency = autocorrelation_latency)
	#chisq = pipeparts.mktee(pipeline, chisq)
	#pipeparts.mknxydumpsink(pipeline, pipeparts.mkqueue(pipeline, chisq), "chisq_%s.dump" % logname, segment = nxydump_segment)
	# FIXME:  find a way to use less memory without this hack
	del bank.autocorrelation_bank

	#
	# trigger generator and progress report
	#

	head = pipeparts.mktriggergen(pipeline, pipeparts.mkqueue(pipeline, snr), chisq, bank.template_bank_filename, bank.snr_threshold)
	if verbose:
		head = pipeparts.mkprogressreport(pipeline, head, "progress_xml_%s" % logname)

	#
	# done
	#

	return head


#
# many instruments, many template banks
#


def mkLLOIDmulti(pipeline, detectors, banks, psd, psd_fft_length = 8, fake_data = False, injection_filename = None, verbose = False, nxydump_segment = None):
	#
	# xml stream aggregator
	#

	nto1 = gst.element_factory_make("input-selector")
	nto1.set_property("select-all", True)
	pipeline.add(nto1)

	#
	# loop over instruments and template banks
	#

	for instrument in detectors:
		rates = set(rate for bank in banks for rate in bank.get_rates())
		hoftdict = mkLLOIDsrc(pipeline, instrument, detectors[instrument], rates, psd = psd, psd_fft_length = psd_fft_length, fake_data = fake_data, injection_filename = injection_filename, verbose = verbose, nxydump_segment = nxydump_segment)
		for bank in banks:
			control_snksrc = mkcontrolsnksrc(pipeline, max(bank.get_rates()), verbose = verbose, suffix = "%s%s" % (instrument, (bank.logname and "_%s" % bank.logname or "")))
			#pipeparts.mknxydumpsink(pipeline, pipeparts.mkqueue(pipeline, control_snksrc[1]), "control_%s.dump" % bank.logname, segment = nxydump_segment)
			head = mkLLOIDsingle(
				pipeline,
				hoftdict,
				instrument,
				detectors[instrument],
				bank,
				control_snksrc,
				verbose = verbose,
				nxydump_segment = nxydump_segment
			)
			pipeparts.mkqueue(pipeline, head).link(nto1)

	#
	# done
	#

	return nto1


#
# LLOID Pipeline handler
#


class LLOIDHandler(object):
	def __init__(self, mainloop, pipeline, verbose = False):
		self.mainloop = mainloop
		self.pipeline = pipeline
		self.verbose = verbose

		bus = pipeline.get_bus()
		bus.add_signal_watch()
		bus.connect("message", self.on_message)

	def on_message(self, bus, message):
		if message.type == gst.MESSAGE_EOS:
			self.pipeline.set_state(gst.STATE_NULL)
			self.mainloop.quit()
		elif message.type == gst.MESSAGE_ERROR:
			gerr, dbgmsg = message.parse_error()
			print >>sys.stderr, "error (%s:%d '%s'): %s" % (gerr.domain, gerr.code, gerr.message, dbgmsg)
			self.pipeline.set_state(gst.STATE_NULL)
			self.mainloop.quit()
		elif message.type == gst.MESSAGE_ELEMENT:
			if message.structure.get_name() == "spectrum":
				psd = pipeio.parse_spectrum_message(message)


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def make_process_params(options):
	params = {}

	#
	# required options
	#

	for option in ("gps_start_time", "gps_end_time", "instrument", "channel_name", "output"):
		params[option] = getattr(options, option)
	# FIXME:  what about template_bank?

	#
	# optional options
	#

	for option in ("frame_cache", "injections", "flow", "svd_tolerance", "reference_psd", "ortho_gate_fap", "snr_threshold", "write_pipeline", "write_psd", "fake_data", "comment", "verbose"):
		if getattr(options, option) is not None:
			params[option] = getattr(options, option)

	#
	# done
	#

	return list(ligolw_process.process_params_from_dict(params))


def parse_command_line():
	parser = OptionParser(
		version = "%prog ??",
		usage = "%prog [options]",
		description = "Stream-based inspiral analysis tool"
	)
	parser.add_option("--frame-cache", metavar = "filename", help = "Set the name of the LAL cache listing the LIGO-Virgo .gwf frame files (optional).  This is required unless --fake-data is used in which case it must not be set.")
	parser.add_option("--gps-start-time", metavar = "seconds", help = "Set the start time of the segment to analyze in GPS seconds (required).  Can be specified to nanosecond precision.")
	parser.add_option("--gps-end-time", metavar = "seconds", help = "Set the end time of the segment to analyze in GPS seconds (required).  Can be specified to nanosecond precision.")
	parser.add_option("--injections", metavar = "filename", help = "Set the name of the LIGO light-weight XML file from which to load injections (optional).")
	parser.add_option("--instrument", metavar = "name", help = "Set the name of the instrument to analyze, e.g. \"H1\" (required).")
	parser.add_option("--channel-name", metavar = "name", default = "LSC-STRAIN", help = "Set the name of the channel to process (optional).  The default is \"LSC-STRAIN\".")
	parser.add_option("--flow", metavar = "Hz", type = "float", default = 40.0, help = "Set the template low-frequency cut-off (default = 40.0).")
	parser.add_option("--svd-tolerance", metavar = "match", type = "float", default = 0.9995, help = "Set the SVD reconstruction tolerance (default = 0.9995).")
	parser.add_option("--nxydump-segment", metavar = "start:stop", default = ":", help = "Set the time interval to dump from nxydump elments (optional).  The default is \":\", i.e. dump all time.")
	parser.add_option("--output", metavar = "filename", help = "Set the name of the LIGO light-weight XML output file (required).")
	parser.add_option("--reference-psd", metavar = "filename", help = "Instead of measuring the noise spectrum, load the spectrum from this LIGO light-weight XML file (optional).")
	parser.add_option("--template-bank", metavar = "filename", action = "append", help = "Set the name of the LIGO light-weight XML file from which to load the template bank (required).  This option can be given multiple times to process multiple template banks in parallel.")
	parser.add_option("--ortho-gate-fap", metavar = "probability", type = "float", default = 1e-4, help = "Set the orthogonal SNR projection gate false-alarm probability (default = 1e-4).")
	parser.add_option("--snr-threshold", metavar = "SNR", type = "float", default = 5.5, help = "Set the SNR threshold (default = 5.5).")
	parser.add_option("--write-pipeline", metavar = "filename", help = "Write a GStreamer XML description of the as-built pipeline to this file (optional).")
	parser.add_option("--write-psd", metavar = "filename", help = "Write measured noise spectrum to this LIGO light-weight XML file (optional).  This option has no effect if --reference-psd is used.")
	parser.add_option("--fake-data", action = "store_true", help = "Instead of reading data from .gwf files, generate and process coloured Gaussian noise modelling the Initial LIGO design spectrum (optional).")
	parser.add_option("--comment", help = "Set the string to be recorded in comment and tag columns in various places in the output file (optional).")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose (optional).")

	options, filenames = parser.parse_args()

	required_options = ["gps_start_time", "gps_end_time", "instrument", "channel_name", "output", "template_bank"]
	if not options.fake_data:
		required_options += ["frame_cache"]
	elif options.frame_cache is not None:
		raise ValueError, "cannot set --frame-cache with --fake-data"

	# FIXME: should also check for read permissions
	for bankname in options.template_bank:
		if not os.path.exists(bankname):
			raise SystemExit, "Template bank file %s does not exist." % bankname

	missing_options = [option for option in required_options if getattr(options, option) is None]
	if missing_options:
		raise ValueError, "missing required option(s) %s" % ", ".join("--%s" % option.replace("_", "-") for option in sorted(missing_options))

	# do this before converting option types
	process_params = make_process_params(options)

	options.gps_start_time = LIGOTimeGPS(options.gps_start_time)
	options.gps_end_time = LIGOTimeGPS(options.gps_end_time)
	options.seg = segments.segment(options.gps_start_time, options.gps_end_time)
	options.nxydump_segment, = segmentsUtils.from_range_strings([options.nxydump_segment], boundtype = LIGOTimeGPS)

	options.psd_fft_length = 8	# seconds

	return options, filenames, process_params


#
# =============================================================================
#
#                               PSD Measurement
#
# =============================================================================
#


def measure_psd(instrument, detector, seg, rate, fake_data = False, injection_filename = None, psd_fft_length = 8, verbose = False):
	#
	# pipeline handler for PSD measurement
	#

	class PSDHandler(object):
		def __init__(self, mainloop, pipeline, verbose = False):
			self.mainloop = mainloop
			self.pipeline = pipeline
			self.verbose = verbose

			bus = pipeline.get_bus()
			bus.add_signal_watch()
			bus.connect("message", self.on_message)

			self.psd = None

		def on_message(self, bus, message):
			if message.type == gst.MESSAGE_EOS:
				self.pipeline.set_state(gst.STATE_NULL)
				self.mainloop.quit()
			elif message.type == gst.MESSAGE_ERROR:
				gerr, dbgmsg = message.parse_error()
				print >>sys.stderr, "error (%s:%d '%s'): %s" % (gerr.domain, gerr.code, gerr.message, dbgmsg)
				self.pipeline.set_state(gst.STATE_NULL)
				self.mainloop.quit()
			elif message.type == gst.MESSAGE_ELEMENT:
				if message.structure.get_name() == "spectrum":
					self.psd = pipeio.parse_spectrum_message(message)

	#
	# 8 FFT-lengths is just a ball-parky estimate of how much data is
	# needed for a good PSD, this isn't a requirement of the code (the
	# code requires a minimum of 1)
	#

	if float(abs(seg)) < 8 * psd_fft_length:
		raise ValueError, "segment %s too short" % str(seg)

	#
	# build pipeline
	#

	mainloop = gobject.MainLoop()

	pipeline = gst.Pipeline("psd")
	if fake_data:
		head = pipeparts.mkfakeLIGOsrc(pipeline, instrument = instrument, blocksize = detector.block_size, location = detector.frame_cache, channel_name = detector.channel)
	else:
		head = pipeparts.mkframesrc(pipeline, instrument = instrument, blocksize = detector.block_size, location = detector.frame_cache, channel_name = detector.channel)
	if verbose:
		head = pipeparts.mkprogressreport(pipeline, head, "Measuring PSD in %s" % instrument)
	if injection_filename is not None:
		head = pipeparts.mkinjections(pipeline, head, injection_filename)
	head = pipeparts.mkcapsfilter(pipeline, pipeparts.mkresample(pipeline, head, quality = 9), "audio/x-raw-float, rate=%d" % rate)
	head = pipeparts.mkqueue(pipeline, head, max_size_buffers = 8)
	head = pipeparts.mkwhiten(pipeline, head, fft_length = psd_fft_length, average_samples = int(round(float(abs(seg)) / (psd_fft_length / 2) - 1)))
	pipeparts.mkfakesink(pipeline, head)

	handler = PSDHandler(mainloop, pipeline, verbose = verbose)

	#
	# process segment
	#

	pipeline.set_state(gst.STATE_PAUSED)
	pipeline.seek(1.0, gst.Format(gst.FORMAT_TIME), gst.SEEK_FLAG_FLUSH, gst.SEEK_TYPE_SET, seg[0].ns(), gst.SEEK_TYPE_SET, seg[1].ns())
	pipeline.set_state(gst.STATE_PLAYING)
	mainloop.run()

	#
	# done
	#

	return handler.psd


def read_psd(filename, verbose = False):
	return lalseries.parse_REAL8FrequencySeries(utils.load_filename(filename, gz = (filename or "stdin").endswith(".gz"), verbose = verbose))


def write_psd(filename, psd, verbose = False):
	xmldoc = ligolw.Document()
	xmldoc.appendChild(ligolw.LIGO_LW()).appendChild(lalseries.build_REAL8FrequencySeries(psd))
	utils.write_filename(xmldoc, filename, gz = (filename or "stdout").endswith(".gz"), verbose = verbose)


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#


#
# parse command line
#


options, filenames, process_params = parse_command_line()


#
# construct pipeline metadata and measure the PSD
#


detectors = {
	options.instrument: DetectorData(options.frame_cache, options.channel_name)
}


if options.reference_psd is not None:
	psd = read_psd(options.reference_psd, verbose = options.verbose)
else:
	psd = measure_psd(
		options.instrument,
		detectors[options.instrument],
		options.seg,
		2 * 2048,	# Hz;  must not be less than highest bank fragment sample rate (see below)
		psd_fft_length = options.psd_fft_length,
		fake_data = options.fake_data,
		injection_filename = options.injections,
		verbose = options.verbose
	)
	if options.write_psd is not None:
		write_psd(options.write_psd, psd, verbose = options.verbose)


banks = [
	Bank(
		template_bank_filename,
		psd,
		templates.time_frequency_boundaries(template_bank_filename, fhigh=max_ffinal(template_bank_filename, check_ffinal(template_bank_filename)), flow = options.flow,verbose=options.verbose),
		gate_fap = options.ortho_gate_fap,
		snr_threshold = options.snr_threshold,
		tolerance = options.svd_tolerance,
		flow = options.flow,
		autocorrelation_length = 201,	# samples
		logname = "bank%d" % n,
		verbose = options.verbose
	) for n, template_bank_filename in enumerate(options.template_bank)
]

#
# build output document
#


if options.injections is not None:
	xmldoc = utils.load_filename(options.injections, gz = (options.injections or "stdin").endswith(".gz"), verbose = options.verbose)
else:
	xmldoc = ligolw.Document()
	xmldoc.appendChild(ligolw.LIGO_LW())
process = ligolw_process.append_process(xmldoc, program = "gstlal_inspiral", comment = options.comment, ifos = set(detectors))
ligolw_process.append_process_params(xmldoc, process, process_params)
search_summary = add_cbc_metadata(xmldoc, process, options.seg)
try:
	sngl_inspiral_table = lsctables.table.get_table(xmldoc, lsctables.SnglInspiralTable.tableName)
except ValueError:
	# FIXME:  argh, ugly
	sngl_inspiral_table = xmldoc.childNodes[-1].appendChild(lsctables.New(lsctables.SnglInspiralTable, columns = ("process_id", "ifo", "search", "channel", "end_time", "end_time_ns", "end_time_gmst", "impulse_time", "impulse_time_ns", "template_duration", "event_duration", "amplitude", "eff_distance", "coa_phase", "mass1", "mass2", "mchirp", "mtotal", "eta", "kappa", "chi", "tau0", "tau2", "tau3", "tau4", "tau5", "ttotal", "psi0", "psi3", "alpha", "alpha1", "alpha2", "alpha3", "alpha4", "alpha5", "alpha6", "beta", "f_final", "snr", "chisq", "chisq_dof", "bank_chisq", "bank_chisq_dof", "cont_chisq", "cont_chisq_dof", "sigmasq", "rsqveto_duration", "Gamma0", "Gamma1", "Gamma2", "Gamma3", "Gamma4", "Gamma5", "Gamma6", "Gamma7", "Gamma8", "Gamma9", "event_id")))


sngl_inspiral_table.set_next_id(lsctables.SnglInspiralID(0))	# FIXME:  remove when lsctables.py has an ID generator attached to sngl_inspiral table
sngl_inspiral_table.sync_next_id()


#
# build pipeline
#


pipeline = gst.Pipeline("gstlal_inspiral")
mainloop = gobject.MainLoop()

src = mkLLOIDmulti(
	pipeline,
	detectors = detectors,
	banks = banks,
	psd = psd,
	psd_fft_length = options.psd_fft_length,
	fake_data = options.fake_data,
	injection_filename = options.injections,
	verbose = options.verbose,
	nxydump_segment = options.nxydump_segment
)

def appsink_new_buffer(elem, sngl_inspiral_table):
	for row in sngl_inspirals_from_buffer(elem.get_property("last-buffer")):
		sngl_inspiral_table.append(row)

pipeparts.mkappsink(pipeline, src, caps = gst.Caps("application/x-lal-snglinspiral")).connect_after("new-buffer", appsink_new_buffer, sngl_inspiral_table)

if options.write_pipeline is not None:
	gst.xml_write_file(pipeline, file(options.write_pipeline, "w"))

handler = LLOIDHandler(mainloop, pipeline, verbose = options.verbose)


#
# process requested segment
#


pipeline.set_state(gst.STATE_PAUSED)
pipeline.seek(1.0, gst.Format(gst.FORMAT_TIME), gst.SEEK_FLAG_FLUSH, gst.SEEK_TYPE_SET, options.seg[0].ns(), gst.SEEK_TYPE_SET, options.seg[1].ns())
pipeline.set_state(gst.STATE_PLAYING)
mainloop.run()


#
# write output file
#


sngl_inspiral_table.sort(lambda a, b: cmp(a.end_time, b.end_time) or cmp(a.end_time_ns, b.end_time_ns) or cmp(a.ifo, b.ifo))
for row in sngl_inspiral_table:
	row.process_id = process.process_id
	row.event_id = sngl_inspiral_table.get_next_id()
search_summary.nevents = len(sngl_inspiral_table)
ligolw_process.set_process_end_time(process)

utils.write_filename(xmldoc, options.output, gz = (options.output or "stdout").endswith(".gz"), verbose = options.verbose)


#
# done
#
