#!/usr/bin/env python
#
# Copyright (C) 2010  Kipp Cannon, Chad Hanna
# Copyright (C) 2009  Kipp Cannon, Chad Hanna
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


try:
	import sqlite3
except ImportError:
        # pre 2.5.x
	from pysqlite2 import dbapi2 as sqlite3
import math
import numpy
from optparse import OptionParser
import sys
import os.path
import os

from gstlal.pipeutil import *
from gstlal.lloidparts import *

from glue import segments
from glue import segmentsUtils
from glue.ligolw import ligolw
from glue.ligolw import lsctables
from glue.ligolw import utils
from glue.ligolw.utils import process as ligolw_process
from pylal.datatypes import LIGOTimeGPS
from pylal.xlal.datatypes.snglinspiraltable import from_buffer as sngl_inspirals_from_buffer


from gstlal import pipeio
from gstlal import templates
from gstlal.gstlal_svd_bank import build_bank
from gstlal.gstlal_reference_psd import *


#
# =============================================================================
#
#                                  Utilities
#
# =============================================================================
#


#
# add metadata to an xml document in the style of lalapps_inspiral
#


def add_cbc_metadata(xmldoc, process, seg_in, seg_out):
	#
	# add entry to search_summary table
	#

	try:
		tbl = lsctables.table.get_table(xmldoc, lsctables.SearchSummaryTable.tableName)
	except ValueError:
		tbl = xmldoc.childNodes[-1].appendChild(lsctables.New(lsctables.SearchSummaryTable))
	search_summary = tbl.RowType()
	search_summary.process_id = process.process_id
	search_summary.shared_object = None # FIXME
	search_summary.lalwrapper_cvs_tag = None # FIXME
	search_summary.lal_cvs_tag = None # FIXME
	search_summary.comment = process.comment
	search_summary.set_ifos(process.get_ifos())
	search_summary.set_in(seg_in)
	search_summary.set_out(seg_out)
	search_summary.nevents = None # FIXME
	search_summary.nnodes = 1
	tbl.append(search_summary)

	#
	# add entry to filter table
	#

	try:
		tbl = lsctables.table.get_table(xmldoc, lsctables.FilterTable.tableName)
	except ValueError:
		tbl = xmldoc.childNodes[-1].appendChild(lsctables.New(lsctables.FilterTable))
	tbl.sync_next_id()
	row = tbl.RowType()
	row.process_id = process.process_id
	row.program = process.program
	row.start_time = int(seg_in[0])
	row.filter_name = None # FIXME
	row.filter_id = tbl.get_next_id()
	row.param_set = None # FIXME
	row.comment = process.comment
	tbl.append(row)

	#
	# add entries to search_summvars table
	#

	try:
		tbl = lsctables.table.get_table(xmldoc, lsctables.SearchSummVarsTable.tableName)
	except ValueError:
		tbl = xmldoc.childNodes[-1].appendChild(lsctables.New(lsctables.SearchSummVarsTable))
	tbl.sync_next_id()
	# FIXME

	#
	# add entries to summ_value table
	#

	try:
		tbl = lsctables.table.get_table(xmldoc, lsctables.SummValueTable.tableName)
	except ValueError:
		tbl = xmldoc.childNodes[-1].appendChild(lsctables.New(lsctables.SummValueTable))
	tbl.sync_next_id()
	# FIXME

	#
	# done
	#

	return search_summary


#
# =============================================================================
#
#                              Pipeline Metadata
#
# =============================================================================
#


class DetectorData(object):
	# default block_size = 16384 samples/second * 8 bytes/sample * 8
	# second
	def __init__(self, frame_cache, channel, block_size = 16384 * 8 * 8):
		self.frame_cache = frame_cache
		self.channel = channel
		self.block_size = block_size


#
# =============================================================================
#
#                              Pipeline Elements
#
# =============================================================================
#


#
# LLOID Pipeline handler
#


class LLOIDHandler(object):
	def __init__(self, mainloop, pipeline, verbose = False):
		self.mainloop = mainloop
		self.pipeline = pipeline
		self.verbose = verbose

		bus = pipeline.get_bus()
		bus.add_signal_watch()
		bus.connect("message", self.on_message)

	def on_message(self, bus, message):
		if message.type == gst.MESSAGE_EOS:
			self.pipeline.set_state(gst.STATE_NULL)
			self.mainloop.quit()
		elif message.type == gst.MESSAGE_ERROR:
			gerr, dbgmsg = message.parse_error()
			self.pipeline.set_state(gst.STATE_NULL)
			self.mainloop.quit()
			sys.exit("error (%s:%d '%s'): %s" % (gerr.domain, gerr.code, gerr.message, dbgmsg))
		elif message.type == gst.MESSAGE_ELEMENT:
			if message.structure.get_name() == "spectrum":
				psd = pipeio.parse_spectrum_message(message)


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def make_process_params(options):
	params = {}

	#
	# required options
	#

	for option in ("gps_start_time", "gps_end_time", "instrument", "channel_name", "output"):
		params[option] = getattr(options, option)
	# FIXME:  what about template_bank?

	#
	# optional options
	#

	for option in ("frame_cache", "injections", "flow", "svd_tolerance", "reference_psd", "ortho_gate_fap", "snr_threshold", "write_pipeline", "write_psd", "fake_data", "online_data", "comment", "verbose"):
		if getattr(options, option) is not None:
			params[option] = getattr(options, option)

	#
	# done
	#

	return list(ligolw_process.process_params_from_dict(params))


def parse_command_line():
	parser = OptionParser(
		version = "%prog ??",
		usage = "%prog [options]",
		description = "Stream-based inspiral analysis tool"
	)
	parser.add_option("--frame-cache", metavar = "filename", help = "Set the name of the LAL cache listing the LIGO-Virgo .gwf frame files (optional).  This is required unless --fake-data or --online-data is used in which case it must not be set.")
	parser.add_option("--online-data", action = "store_true", help = "Use online DMT-STRAIN instead of a frame file (optional).")
	parser.add_option("--gps-start-time", metavar = "seconds", help = "Set the start time of the segment to analyze in GPS seconds (required).  Can be specified to nanosecond precision.")
	parser.add_option("--gps-end-time", metavar = "seconds", help = "Set the end time of the segment to analyze in GPS seconds (required).  Can be specified to nanosecond precision.")
	parser.add_option("--injections", metavar = "filename", help = "Set the name of the LIGO light-weight XML file from which to load injections (optional).")
	parser.add_option("--instrument", metavar = "name", help = "Set the name of the instrument to analyze, e.g. \"H1\" (required).")
	parser.add_option("--channel-name", metavar = "name", default = "LSC-STRAIN", help = "Set the name of the channel to process (optional).  The default is \"LSC-STRAIN\".")
	parser.add_option("--flow", metavar = "Hz", type = "float", default = 40.0, help = "Set the template low-frequency cut-off (default = 40.0).")
	parser.add_option("--svd-tolerance", metavar = "match", type = "float", default = 0.9995, help = "Set the SVD reconstruction tolerance (default = 0.9995).")
	parser.add_option("--nxydump-segment", metavar = "start:stop", default = ":", help = "Set the time interval to dump from nxydump elments (optional).  The default is \":\", i.e. dump all time.")
	parser.add_option("--output", metavar = "filename", help = "Set the name of the LIGO light-weight XML output file *.{xml,xml.gz} or an SQLite database *.sqlite (required).")
	parser.add_option("--reference-psd", metavar = "filename", help = "Instead of measuring the noise spectrum, load the spectrum from this LIGO light-weight XML file (optional).")
	parser.add_option("--template-bank", metavar = "filename", action = "append", help = "Set the name of the LIGO light-weight XML file from which to load the template bank (required).  This option can be given multiple times to process multiple template banks in parallel.")
	parser.add_option("--ortho-gate-fap", metavar = "probability", type = "float", default = 1e-2, help = "Set the orthogonal SNR projection gate false-alarm probability (default = 1e-2).")
	parser.add_option("--snr-threshold", metavar = "SNR", type = "float", default = 5.5, help = "Set the SNR threshold (default = 5.5).")
	parser.add_option("--write-pipeline", metavar = "filename", help = "Write a DOT graph description of the as-built pipeline to this file (optional).  The environment variable GST_DEBUG_DUMP_DOT_DIR must be set for this option to work.")
	parser.add_option("--write-psd", metavar = "filename", help = "Write measured noise spectrum to this LIGO light-weight XML file (optional).  This option has no effect if --reference-psd is used.")
	parser.add_option("--fake-data", action = "store_true", help = "Instead of reading data from .gwf files, generate and process coloured Gaussian noise modelling the Initial LIGO design spectrum (optional).")
	parser.add_option("--comment", help = "Set the string to be recorded in comment and tag columns in various places in the output file (optional).")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose (optional).")
	parser.add_option("-t", "--tmp-space", metavar = "path", help = "Path to a directory suitable for use as a work area while manipulating the database file.  The database file will be worked on in this directory, and then moved to the final location when complete.  This option is intended to improve performance when running in a networked environment, where there might be a local disk with higher bandwidth than is available to the filesystem on which the final output will reside.")

	options, filenames = parser.parse_args()

	if sum(1 for option in ('frame_cache', 'fake_data', 'online_data') if getattr(options, option) is not None) != 1:
		raise ValueError, "must provide exactly one of --frame-cache, --fake-data, --online-data"

	required_options = ["instrument", "output", "template_bank"]

	if options.frame_cache:
		required_options += ["channel_name", "gps_start_time", "gps_end_time"]

	if options.online_data:
		required_options += ["reference_psd"]

	# FIXME: should also check for read permissions
	for bankname in options.template_bank:
		if not os.path.exists(bankname):
			raise SystemExit, "Template bank file %s does not exist." % bankname

	missing_options = [option for option in required_options if getattr(options, option) is None]
	if missing_options:
		raise ValueError, "missing required option(s) %s" % ", ".join("--%s" % option.replace("_", "-") for option in sorted(missing_options))

	# do this before converting option types
	process_params = make_process_params(options)

	# If --gps-start-time is not specified, assume "beginning of time"
	if options.gps_start_time is None:
		effective_gps_start_time = LIGOTimeGPS(0) # FIXME: should be gobject.G_MININT, but somewhere along the line we have an integer overflow error
	else:
		effective_gps_start_time = LIGOTimeGPS(options.gps_start_time)

	# If --gps-end-time is not specified, assume "end of time"
	if options.gps_end_time is None:
		effective_gps_end_time = LIGOTimeGPS(999999999) # FIXME: should be gobject.G_MAXINT, but somewhere along the line we have an integer overflow error
	else:
		effective_gps_end_time = LIGOTimeGPS(options.gps_end_time)

	options.seg = segments.segment(effective_gps_start_time, effective_gps_end_time)
	options.nxydump_segment, = segmentsUtils.from_range_strings([options.nxydump_segment], boundtype = LIGOTimeGPS)

	options.psd_fft_length = 8	# seconds

	return options, filenames, process_params


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#


#
# parse command line
#


options, filenames, process_params = parse_command_line()


#
# construct pipeline metadata and measure the PSD
#


if options.gps_start_time is None:
	seek_start_type = gst.SEEK_TYPE_NONE
	seek_start_time = -1 # gst.CLOCK_TIME_NONE is exported as unsigned, should have been signed
else:
	seek_start_type = gst.SEEK_TYPE_SET
	seek_start_time = options.seg[0].ns()

if options.gps_end_time is None:
	seek_stop_type = gst.SEEK_TYPE_NONE
	seek_stop_time = -1 # gst.CLOCK_TIME_NONE is exported as unsigned, should have been signed
else:
	seek_stop_type = gst.SEEK_TYPE_SET
	seek_stop_time = options.seg[1].ns()

seekevent = gst.event_new_seek(1.0, gst.Format(gst.FORMAT_TIME),
	gst.SEEK_FLAG_FLUSH | gst.SEEK_FLAG_KEY_UNIT,
	seek_start_type, seek_start_time,
	seek_stop_type, seek_stop_time)


detectors = {
	options.instrument: DetectorData(options.frame_cache, options.channel_name)
}


if options.reference_psd is not None:
	psd = read_psd(options.reference_psd, verbose = options.verbose)
else:
	psd = measure_psd(
		options.instrument,
		seekevent,
		detectors[options.instrument],
		options.seg,
		2 * 2048,	# Hz;  must not be less than highest bank fragment sample rate (see below)
		psd_fft_length = options.psd_fft_length,
		fake_data = options.fake_data,
		online_data = options.online_data,
		injection_filename = options.injections,
		verbose = options.verbose
	)
	if options.write_psd is not None:
		write_psd(options.write_psd, psd, verbose = options.verbose)

#
# Make template banks
#

banks = [
	build_bank(filename, psd, options.flow, options.ortho_gate_fap, options.snr_threshold, options.svd_tolerance, options.verbose)
	for filename in options.template_bank
]
for n, bank in enumerate(banks):
	bank.logname = "bank%d" % n


class Data(object):
	def __init__(self, options):
		self.tmp_space = options.tmp_space
		self.xmldoc = None
		self.output = options.output
		self.seg = options.seg
		self.out_seg = options.out_seg
		self.injections = options.injections
		self.comment = options.comment
		self.verbose = options.verbose
		self.sngl_inspiral_table = None
		self.seg = options.seg
		self.injection_file = options.injections
		self.output = options.output
		self.connection = None

	def prepare_output_file(self):
		xmldoc = ligolw.Document()
		xmldoc.appendChild(ligolw.LIGO_LW())
		self.process = ligolw_process.append_process(xmldoc, program = "gstlal_inspiral", comment = self.comment, ifos = set(detectors))
		ligolw_process.append_process_params(xmldoc, self.process, process_params)
		search_summary = add_cbc_metadata(xmldoc, self.process, self.seg, self.out_seg)
		# FIXME:  argh, ugly
		sngl_inspiral_table = xmldoc.childNodes[-1].appendChild(lsctables.New(lsctables.SnglInspiralTable, columns = ("process_id", "ifo", "search", "channel", "end_time", "end_time_ns", "end_time_gmst", "impulse_time", "impulse_time_ns", "template_duration", "event_duration", "amplitude", "eff_distance", "coa_phase", "mass1", "mass2", "mchirp", "mtotal", "eta", "kappa", "chi", "tau0", "tau2", "tau3", "tau4", "tau5", "ttotal", "psi0", "psi3", "alpha", "alpha1", "alpha2", "alpha3", "alpha4", "alpha5", "alpha6", "beta", "f_final", "snr", "chisq", "chisq_dof", "bank_chisq", "bank_chisq_dof", "cont_chisq", "cont_chisq_dof", "sigmasq", "rsqveto_duration", "Gamma0", "Gamma1", "Gamma2", "Gamma3", "Gamma4", "Gamma5", "Gamma6", "Gamma7", "Gamma8", "Gamma9", "event_id")))

		sngl_inspiral_table.set_next_id(lsctables.SnglInspiralID(0))	# FIXME:  remove when lsctables.py has an ID generator attached to sngl_inspiral table

		if self.output.endswith('.sqlite'):
			from glue.ligolw.utils import ligolw_sqlite
			from glue.ligolw import dbtables
			self.working_filename = dbtables.get_connection_filename(self.output, tmp_path = self.tmp_space, verbose = self.verbose)
			self.connection = sqlite3.connect(self.working_filename, check_same_thread=False)
			# setup id remapping
			dbtables.idmap_create(self.connection)
			dbtables.DBTable.append = dbtables.DBTable._remapping_append
			dbtables.idmap_sync(self.connection)
			ligolw_sqlite.insert_from_xmldoc(self.connection, xmldoc, preserve_ids = False, verbose = self.verbose)
			xmldoc.unlink()
			if self.injection_file is not None:
				ligolw_sqlite.insert_from_url(self.connection, self.injection_file, preserve_ids = False, verbose = self.verbose)
				#utils.load_filename(self.injection_file, gz = (injection_file or "stdin").endswith(".gz"), verbose = self.verbose).unlink()
			self.xmldoc = dbtables.get_xml(self.connection)
			self.sngl_inspiral_table = lsctables.table.get_table(self.xmldoc, lsctables.SnglInspiralTable.tableName)
		else:
			from glue.ligolw.utils import ligolw_add
			self.xmldoc = xmldoc
			self.sngl_inspiral_table = sngl_inspiral_table
			if self.injection_file is not None:
				ligolw_add.ligolw_add(self.xmldoc, [self.injection_file], verbose = self.verbose)
				utils.load_filename(self.injection_file, gz = (self.injection_file or "stdin").endswith(".gz"), verbose = self.verbose)

	def write_output_file(self):
		if self.connection:
			from glue.ligolw import dbtables
			from pylal.date import XLALUTCToGPS
			import time
			self.connection.cursor().execute('UPDATE search_summary SET nevents = (SELECT count(*) FROM sngl_inspiral)')
			self.connection.cursor().execute('UPDATE process SET end_time = ?', (XLALUTCToGPS(time.gmtime()).seconds,))
			self.connection.commit()
			dbtables.build_indexes(self.connection, options.verbose)
			dbtables.put_connection_filename(self.output, self.working_filename, verbose = self.verbose)
		else:
			self.sngl_inspiral_table.sort(lambda a, b: cmp(a.end_time, b.end_time) or cmp(a.end_time_ns, b.end_time_ns) or cmp(a.ifo, b.ifo))
			search_summary = lsctables.table.get_table(self.xmldoc, lsctables.SearchSummaryTable.tableName)
			search_summary.nevents = len(self.sngl_inspiral_table)
			ligolw_process.set_process_end_time(self.process)
			utils.write_filename(self.xmldoc, self.output, gz = (self.output or "stdout").endswith(".gz"), verbose = self.verbose)

#
# build pipeline
#


#
# Write the pipeline to a dot file.
# This option needs the environment variable GST_DEBUG_DUMP_DOT_DIR
# to be set. There are several choices for the "details"
# (second argument). DEBUG_GRAPH_SHOW_ALL is the most verbose.
#
def maybe_dump_dot(pipeline, stage):
	if options.write_pipeline is not None:
		filestem = '%s.%s' % (options.write_pipeline, stage)
		filename = '%s.dot' % filestem
		if "GST_DEBUG_DUMP_DOT_DIR" in os.environ.keys():
			gst.DEBUG_BIN_TO_DOT_FILE( pipeline, gst.DEBUG_GRAPH_SHOW_ALL, filestem)
			print >> sys.stderr, "Wrote pipeline to", os.path.join(os.environ["GST_DEBUG_DUMP_DOT_DIR"], filename)
		else:
			print >> sys.stderr, "***Could not write pipeline, please set GST_DEBUG_DUMP_DOT_DIR in your environment"


pipeline = gst.Pipeline("gstlal_inspiral")
mainloop = gobject.MainLoop()
handler = LLOIDHandler(mainloop, pipeline, verbose = options.verbose)

src = mkLLOIDmulti(
	pipeline,
	seekevent,
	detectors = detectors,
	banks = banks,
	psd = psd,
	psd_fft_length = options.psd_fft_length,
	fake_data = options.fake_data,
	online_data = options.online_data,
	injection_filename = options.injections,
	verbose = options.verbose,
	nxydump_segment = options.nxydump_segment
)

#
# build output document
#

options.out_seg = segments.segment(options.seg[0]+max([b.filter_length for b in banks]), options.seg[1]) #FIXME make better outseg def.
data = Data(options)
data.prepare_output_file()

def appsink_new_buffer(elem, data):
	for row in sngl_inspirals_from_buffer(elem.get_property("last-buffer")):
		if (row.end_time + 1e-9*row.end_time_ns) in data.out_seg:
			row.process_id = data.process.process_id
			row.event_id = data.sngl_inspiral_table.get_next_id()
			data.sngl_inspiral_table.append(row)
	if data.connection: data.connection.commit()

mkelems_fast(pipeline, src, "appsink", {"caps": gst.Caps("application/x-lal-snglinspiral"), "sync": False, "async": False, "emit-signals": True, "max-buffers": 1, "drop": True})[-1].connect_after("new-buffer", appsink_new_buffer, data)



#
# process requested segment
#


maybe_dump_dot(pipeline, "NULL")
pipeline.set_state(gst.STATE_PLAYING)
maybe_dump_dot(pipeline, "PLAYING")
mainloop.run()


#
# write output file
#


data.write_output_file()

#
# done
#
