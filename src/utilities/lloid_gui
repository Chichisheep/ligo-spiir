#!/usr/bin/python
#
# $Id$
#
# Copyright (C) 2009  Kipp Cannon, Chad Hanna
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.


#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


import math
import numpy
from optparse import OptionParser
import sys


import gobject
import pygst
pygst.require("0.10")
import gst


gobject.threads_init()


from glue.ligolw import ligolw
from glue.ligolw import lsctables
from glue.ligolw import utils
from pylal import series as lalseries
from pylal.datatypes import LIGOTimeGPS


from gstlal import pipeio
from gstlal import pipeparts
from gstlal import cbc_template_fir


#
# =============================================================================
#
#                              Pipeline Metadata
#
# =============================================================================
#


class DetectorData(object):
	# default block_size = 16384 samples/second * 8 bytes/sample * 8
	# second
	def __init__(self, frame_cache, channel, block_size = 16384 * 8 * 8):
		self.frame_cache = frame_cache
		self.channel = channel
		self.block_size = block_size


class Bank(object):
	class BankFragment(object):
		def __init__(self, rate, start, end):
			self.rate = rate
			self.start = start
			self.end = end

		def set_template_bank(self, template_bank, tolerance, verbose = False):
			if verbose:
				print >>sys.stderr, "\t%d templates of %d samples" % template_bank.shape

			self.orthogonal_template_bank, self.singular_values, self.mix_matrix, self.chifacs = cbc_template_fir.decompose_templates(template_bank, tolerance)

			self.sum_of_squares_weights = self.singular_values * math.sqrt(self.chifacs.mean() / numpy.dot(self.singular_values, self.singular_values))

			if verbose:
				print >>sys.stderr, "\tidentified %d components" % self.orthogonal_template_bank.shape[0]
				print >>sys.stderr, "\tsum-of-squares expectation value is %g" % self.chifacs.mean()

	def __init__(self, template_bank_filename, psd, bank_fragments, gate_threshold, snr_threshold, tolerance, flow = 40.0, autocorrelation_length = None, logname = None, verbose = False):
		self.template_bank_filename = template_bank_filename	# FIXME:  remove when not needed by trigger generator element
		self.bank_fragments = bank_fragments
		self.filter_length = max(fragment.end for fragment in bank_fragments)
		self.gate_threshold = gate_threshold
		self.snr_threshold = snr_threshold
		self.logname = logname

		sample_rate = max(bank_fragment.rate for bank_fragment in bank_fragments)

		template_bank, self.autocorrelation_bank = cbc_template_fir.generate_templates(lsctables.table.get_table(utils.load_filename(template_bank_filename, gz = (template_bank_filename or "stdin").endswith(".gz"), verbose = verbose), lsctables.SnglInspiralTable.tableName), psd, flow, sample_rate, self.filter_length, autocorrelation_length = autocorrelation_length, verbose = verbose)

		n = template_bank.shape[1]	# number of samples in template
		for i, bank_fragment in enumerate(self.bank_fragments):
			if verbose:
				print >>sys.stderr, "constructing template decomposition %d of %d:  %g s ... %g s" % (i + 1, len(bank_fragments), -bank_fragment.end, -bank_fragment.start)

			# start and end times are measured *backwards* from
			# template end;  subtract from n to convert to
			# start and end index;  end:start is the slice to
			# extract (argh!  Chad!)
			start = n - int(round(bank_fragment.start * sample_rate))
			end = n - int(round(bank_fragment.end * sample_rate))
			stride = int(round(sample_rate / bank_fragment.rate))

			# extract every stride-th sample, multiply by
			# \sqrt{stride} to maintain inner product
			# normalization
			bank_fragment.set_template_bank(template_bank[:, end:start:stride] * math.sqrt(stride), tolerance, verbose = verbose)

	def get_rates(self):
		return set(bank_fragment.rate for bank_fragment in self.bank_fragments)


#
# =============================================================================
#
#                              Pipeline Elements
#
# =============================================================================
#


#
# sum-of-squares aggregator
#


def mkcontrolsnksrc(pipeline, rate, verbose = False, suffix = None):
	snk = gst.element_factory_make("lal_adder")
	snk.set_property("sync", True)
	pipeline.add(snk)
	src = pipeparts.mkcapsfilter(pipeline, snk, "audio/x-raw-float, rate=%d" % rate)
	if verbose:
		src = pipeparts.mkprogressreport(pipeline, src, "progress_sumsquares%s" % (suffix and "_%s" % suffix or ""))
	src = pipeparts.mktee(pipeline, src)
	return snk, src


#
# data source
#


def mkLLOIDsrc(pipeline, instrument, detector, rates, psd = None, psd_fft_length = 8, fake_data = False, injection_filename = None, verbose = False):
	#
	# data source and progress report
	#

	if fake_data:
		head = pipeparts.mkfakeLIGOsrc(pipeline, instrument = instrument, blocksize = detector.block_size, location = detector.frame_cache, channel_name = detector.channel)
	else:
		head = pipeparts.mkframesrc(pipeline, instrument = instrument, blocksize = detector.block_size, location = detector.frame_cache, channel_name = detector.channel)
	if verbose:
		head = pipeparts.mkprogressreport(pipeline, head, "progress_src_%s" % instrument)

	#
	# optional injections
	#

	if injection_filename is not None:
		head = pipeparts.mkinjections(pipeline, head, injection_filename)

	#
	# down-sample to highest of target sample rates.  note:  there is
	# no check that this is, infact, *down*-sampling.  if the source
	# time series has a lower sample rate this will up-sample the data.
	# up-sampling will probably interact poorly with the whitener as it
	# will likely add (possibly significant) numerical noise when it
	# amplifies the non-existant high-frequency components
	#

	head = pipeparts.mkcapsfilter(pipeline, pipeparts.mkresample(pipeline, pipeparts.mkqueue(pipeline, head), quality = 9), "audio/x-raw-float, rate=%d" % max(rates))

	#
	# whiten
	#

	if psd is not None:
		#
		# use fixed PSD
		#

		head = pipeparts.mkwhiten(pipeline, head, fft_length = psd_fft_length, psd_mode = 1)

		#
		# install signal handler to retrieve \Delta f when it is
		# known, resample the user-supplied PSD, and install it
		# into the whitener.  n is the number of frequency bins
		#

		def delta_f_changed(elem, delta_f, psd):
			n = int(round(elem.get_property("f-nyquist") / delta_f) + 1)
			psd = cbc_template_fir.interpolate_psd(psd, delta_f)
			elem.set_property("psd", psd.data[:n])

		head.connect_after("delta-f-changed", delta_f_changed, psd)
	else:
		#
		# use running average PSD
		#

		head = pipeparts.mkwhiten(pipeline, head, fft_length = psd_fft_length)
	head = pipeparts.mknofakedisconts(pipeline, head)	# FIXME:  remove after basetransform behaviour fixed

	#
	# down-sample whitened time series to remaining target sample rates
	# while applying an amplitude correction to adjust for low-pass
	# filter roll-off
	#

	# FIXME:  re-measure these, we've changed the quality of the
	# resampler since these were measured.
	correction = {
		128: 1.03 / 0.93,
		256: 1.03 / 0.93,
		512: 1.03 / 0.93,
		2048: 1.03 / 1.03
	}

	head = {max(rates): pipeparts.mktee(pipeline, head)}
	for rate in sorted(rates, reverse = True)[1:]:	# all but the highest rate
		source_rate = max(rates)
		head[rate] = pipeparts.mktee(pipeline, pipeparts.mkcapsfilter(pipeline, pipeparts.mkresample(pipeline, pipeparts.mkaudioamplify(pipeline, head[source_rate], math.sqrt(correction[rate] * float(source_rate) / rate)), quality = 9), "audio/x-raw-float, rate=%d" % rate))

	#
	# done.  return value is a dictionary of tee elements indexed by
	# sample rate
	#

	#for rate, elem in head.items():
	#	pipeparts.mknxydumpsink(pipeline, pipeparts.mkqueue(pipeline, elem), "src_%d.dump" % rate)
	return head


#
# one instrument, one template bank
#


def mkLLOIDbranch(pipeline, src, bank, bank_fragment, control_snk, control_src):
	#
	# FIR filter bank
	#

	# FIXME:  why the -1?  without it the pieces don't match but I
	# don't understand where this offset comes from.  it might really
	# need to be here, or it might be a symptom of a bug elsewhere.
	# figure this out.
	src = pipeparts.mktee(pipeline, pipeparts.mkfirbank(pipeline, src, latency = -int(round(bank_fragment.start * bank_fragment.rate)) - 1, fir_matrix = bank_fragment.orthogonal_template_bank))

	#
	# compute weighted sum-of-squares, feed to sum-of-squares
	# aggregator
	#

	pipeparts.mkchecktimestamps(pipeline, pipeparts.mkresample(pipeline, pipeparts.mkqueue(pipeline, pipeparts.mksumsquares(pipeline, src, weights = bank_fragment.sum_of_squares_weights)), quality = 9), name = "timestamps_%s_%d_%d_after_sumsquare_resampler" % (bank.logname, bank_fragment.start, bank_fragment.end)).link(control_snk)

	#
	# use sum-of-squares aggregate as gate control for orthogonal SNRs
	#

	src = pipeparts.mkgate(pipeline, pipeparts.mkqueue(pipeline, src), control = pipeparts.mkqueue(pipeline, control_src), threshold = bank.gate_threshold)
	#src = pipeparts.mkchecktimestamps(pipeline, src, name = "timestamps_%s_%d_%d_after_gate" % (bank.logname, bank_fragment.start, bank_fragment.end))

	#
	# buffer orthogonal SNRs
	#
	# FIXME:  teach the collectpads object not to wait for buffers on
	# pads whose segments have not yet been reached by the input on the
	# other pads.  then this large queue buffer will not be required
	# because streaming can begin through the downstream adders without
	# waiting for input from all upstream elements.
	src = pipeparts.mkqueue(pipeline, src, max_size_buffers = 0, max_size_bytes = 0, max_size_time = 2 * int(math.ceil(bank.filter_length)) * gst.SECOND)

	#
	# reconstruct physical SNRs
	#

	src = pipeparts.mkmatrixmixer(pipeline, src, matrix = bank_fragment.mix_matrix)
	src = pipeparts.mkresample(pipeline, src, quality = 9)
	src = pipeparts.mknofakedisconts(pipeline, src)	# FIXME:  remove after basetransform behaviour fixed
	src = pipeparts.mkchecktimestamps(pipeline, src, name = "timestamps_%s_%d_%d_after_snr_resampler" % (bank.logname, bank_fragment.start, bank_fragment.end))

	#
	# done
	#

	# FIXME:  find a way to use less memory without this hack
	del bank_fragment.orthogonal_template_bank
	del bank_fragment.sum_of_squares_weights
	del bank_fragment.mix_matrix

	return src


def mkLLOIDsingle(pipeline, hoftdict, instrument, detector, bank, verbose = False, control_snksrc = None):
	logname = "%s%s" % (instrument, (bank.logname and "_%s" % bank.logname or ""))

	#
	# construct sum-of-squares aggregator if needed
	#

	if control_snksrc is None:
		control_snksrc = mkcontrolsnksrc(pipeline, max(bank.get_rates()), verbose = verbose, suffix = logname)
	#pipeparts.mknxydumpsink(pipeline, pipeparts.mkqueue(pipeline, control_snksrc[1]), "control_%s.dump" % bank.logname)

	#
	# snr aggregator
	#

	snr = gst.element_factory_make("lal_adder")
	snr.set_property("sync", True)
	pipeline.add(snr)

	#
	# loop over template bank slices
	#

	for bank_fragment in bank.bank_fragments:
		branch_snr = mkLLOIDbranch(
			pipeline,
			pipeparts.mkqueue(pipeline, hoftdict[bank_fragment.rate], max_size_bytes = 0, max_size_buffers = 0, max_size_time = 2 * int(math.ceil(bank.filter_length)) * gst.SECOND),
			bank,
			bank_fragment,
			control_snksrc[0],
			control_snksrc[1]
		)
		#branch_snr = pipeparts.mktee(pipeline, branch_snr)
		#pipeparts.mknxydumpsink(pipeline, pipeparts.mkqueue(pipeline, branch_snr), "snr_%s_%02d.dump" % (logname, bank_fragment.start))
		branch_snr.link(snr)

	#
	# snr and \chi^{2}
	#

	snr = pipeparts.mktee(pipeline, pipeparts.mktogglecomplex(pipeline, pipeparts.mkcapsfilter(pipeline, snr, "audio/x-raw-float, rate=%d" % max(bank.get_rates()))))
	chisq = pipeparts.mkautochisq(pipeline, pipeparts.mkqueue(pipeline, snr), autocorrelation_matrix = bank.autocorrelation_bank, latency = -(bank.autocorrelation_bank.shape[1] - 1) / 2)
	# FIXME:  find a way to use less memory without this hack
	del bank.autocorrelation_bank
	#pipeparts.mknxydumpsink(pipeline, pipeparts.mktogglecomplex(pipeline, pipeparts.mkqueue(pipeline, snr)), "snr_%s.dump" % logname)
	#pipeparts.mkogmvideosink(pipeline, pipeparts.mkcapsfilter(pipeline, pipeparts.mkchannelgram(pipeline, pipeparts.mkqueue(pipeline, snr)), "video/x-raw-rgb, width=640, height=480, framerate=4/1"), "channelgram.avi", verbose = True)

	#
	# trigger generator and progress report
	#

	head = pipeparts.mktriggergen(pipeline, pipeparts.mkqueue(pipeline, snr), chisq, bank.template_bank_filename, bank.snr_threshold)
	if verbose:
		head = pipeparts.mkprogressreport(pipeline, head, "progress_xml_%s" % logname)

	#
	# done
	#

	return head


#
# many instruments, many template banks
#


def mkLLOIDmulti(pipeline, detectors, banks, psd, output_filename, psd_fft_length = 8, fake_data = False, injection_filename = None, verbose = False):
	if len(detectors) > 1:
		raise ValueError, "only 1 instrument supported yet"

	#
	# xml stream aggregator
	#

	#nto1 = gst.element_factory_make("input-selector")
	#nto1.set_property("select-all", True)
	#pipeline.add(nto1)
	#pipeparts.mktriggerxmlwritersink(pipeline, nto1, output_filename)

	#
	# loop over instruments and template banks
	#

	for instrument in detectors:
		rates = set(rate for bank in banks for rate in bank.get_rates())
		hoftdict = mkLLOIDsrc(pipeline, instrument, detectors[instrument], rates, psd = psd, psd_fft_length = psd_fft_length, fake_data = fake_data, injection_filename = injection_filename, verbose = verbose)
		for bank in banks:
			control_snksrc = mkcontrolsnksrc(pipeline, max(bank.get_rates()), verbose = verbose, suffix = "%s%s" % (instrument, (bank.logname and "_%s" % bank.logname or "")))
			head = mkLLOIDsingle(
				pipeline,
				hoftdict,
				instrument,
				detectors[instrument],
				bank,
				verbose = verbose,
				control_snksrc = control_snksrc
			)
			#pipeparts.mkqueue(pipeline, head).link(nto1)
			pipeparts.mktriggerxmlwritersink(pipeline, head, output_filename.replace(".xml", "_%s.xml" % bank.logname))

	#
	# done
	#


#
# LLOID Pipeline handler
#


class LLOIDHandler(object):
	def __init__(self, mainloop, pipeline, verbose = False):
		self.mainloop = mainloop
		self.pipeline = pipeline
		self.verbose = verbose

		bus = pipeline.get_bus()
		bus.add_signal_watch()
		bus.connect("message", self.on_message)

	def on_message(self, bus, message):
		if message.type == gst.MESSAGE_EOS:
			self.pipeline.set_state(gst.STATE_NULL)
			self.mainloop.quit()
		elif message.type == gst.MESSAGE_ERROR:
			gerr, dbgmsg = message.parse_error()
			print >>sys.stderr, "error (%s:%d '%s'): %s" % (gerr.domain, gerr.code, gerr.message, dbgmsg)
			self.pipeline.set_state(gst.STATE_NULL)
			self.mainloop.quit()
		elif message.type == gst.MESSAGE_ELEMENT:
			if message.structure.get_name() == "spectrum":
				psd = pipeio.parse_spectrum_message(message)


#
# =============================================================================
#
#                                 Command Line
#
# =============================================================================
#


def parse_command_line():
	parser = OptionParser(
		version = "%prog ??",
		usage = "%prog [options]",
		description = "Stream-based inspiral analysis tool"
	)
	parser.add_option("--frame-cache", metavar = "filename", help = "Set the name of the LAL cache listing the LIGO-Virgo .gwf frame files (optional).  This is required unless --fake-data is used in which case it must not be set.")
	parser.add_option("--gps-start-time", metavar = "seconds", help = "Set the start time of the segment to analyze in GPS seconds (required).  Can be specified to nanosecond precision.")
	parser.add_option("--gps-stop-time", metavar = "seconds", help = "Set the stop time of the segment to analyze in GPS seconds (required).  Can be specified to nanosecond precision.")
	parser.add_option("--injections", metavar = "filename", help = "Set the name of the LIGO light-weight XML file from which to load injections (optional).")
	parser.add_option("--instrument", metavar = "name", help = "Set the name of the instrument to analyze, e.g. \"H1\" (required).")
	parser.add_option("--channel-name", metavar = "name", default = "LSC-STRAIN", help = "Set the name of the channel to process (optional).  The default is \"LSC-STRAIN\".")
	parser.add_option("--output", metavar = "filename", help = "Set the name of the LIGO light-weight XML output file (required).")
	parser.add_option("--reference-psd", metavar = "filename", help = "Instead of measuring the noise spectrum, load the spectrum from this LIGO light-weight XML file (optional).")
	parser.add_option("--template-bank", metavar = "filename", action = "append", help = "Set the name of the LIGO light-weight XML file from which to load the template bank (required).  This option can be given multiple times to process multiple template banks in parallel.")
	parser.add_option("--write-pipeline", metavar = "filename", help = "Write a GStreamer XML description of the as-built pipeline to this file (optional).")
	parser.add_option("--write-psd", metavar = "filename", help = "Write measured noise spectrum to this LIGO light-weight XML file (optional).  This option has no effect if --reference-psd is used.")
	parser.add_option("--fake-data", action = "store_true", help = "Instead of reading data from .gwf files, generate and process coloured Gaussian noise modelling the Initial LIGO design spectrum (optional).")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose (optional).")

	options, filenames = parser.parse_args()

	required_options = ["gps_start_time", "gps_stop_time", "output", "template_bank"]
	if not options.fake_data:
		required_options += ["frame_cache"]

	missing_options = [option for option in required_options if getattr(options, option) is None]
	if missing_options:
		raise ValueError, "missing required option(s) %s" % ", ".join("--%s" % option.replace("_", "-") for option in sorted(missing_options))
	if options.fake_data and options.frame_cache is not None:
		raise ValueError, "cannot set --frame-cache with --fake-data"

	options.gps_start_time = LIGOTimeGPS(options.gps_start_time)
	options.gps_stop_time = LIGOTimeGPS(options.gps_stop_time)

	return options, filenames


#
# =============================================================================
#
#                               PSD Measurement
#
# =============================================================================
#


def measure_psd(instrument, detector, gps_start_time, gps_stop_time, rate, fake_data = False, injection_filename = None, psd_fft_length = 8, verbose = False):
	#
	# pipeline handler for PSD measurement
	#

	class PSDHandler(object):
		def __init__(self, mainloop, pipeline, verbose = False):
			self.mainloop = mainloop
			self.pipeline = pipeline
			self.verbose = verbose

			bus = pipeline.get_bus()
			bus.add_signal_watch()
			bus.connect("message", self.on_message)

			self.psd = None

		def on_message(self, bus, message):
			if message.type == gst.MESSAGE_EOS:
				self.pipeline.set_state(gst.STATE_NULL)
				self.mainloop.quit()
			elif message.type == gst.MESSAGE_ERROR:
				gerr, dbgmsg = message.parse_error()
				print >>sys.stderr, "error (%s:%d '%s'): %s" % (gerr.domain, gerr.code, gerr.message, dbgmsg)
				self.pipeline.set_state(gst.STATE_NULL)
				self.mainloop.quit()
			elif message.type == gst.MESSAGE_ELEMENT:
				if message.structure.get_name() == "spectrum":
					self.psd = pipeio.parse_spectrum_message(message)

	#
	# 8 FFT-lengths is just a ball-parky estimate of how much data is
	# needed for a good PSD, this isn't a requirement of the code (the
	# code requires a minimum of 1)
	#

	if float(gps_stop_time - gps_start_time) < 8 * psd_fft_length:
		raise ValueError, "segment [%s s, %s s) too short" % (gps_start_time, gps_stop_time)

	#
	# build pipeline
	#

	mainloop = gobject.MainLoop()

	pipeline = gst.Pipeline("psd")
	if fake_data:
		head = pipeparts.mkfakeLIGOsrc(pipeline, instrument = instrument, blocksize = detector.block_size, location = detector.frame_cache, channel_name = detector.channel)
	else:
		head = pipeparts.mkframesrc(pipeline, instrument = instrument, blocksize = detector.block_size, location = detector.frame_cache, channel_name = detector.channel)
	if verbose:
		head = pipeparts.mkprogressreport(pipeline, head, "Measuring PSD in %s" % instrument)
	if injection_filename is not None:
		head = pipeparts.mkinjections(pipeline, head, injection_filename)
	head = pipeparts.mkcapsfilter(pipeline, pipeparts.mkresample(pipeline, head, quality = 9), "audio/x-raw-float, rate=%d" % rate)
	head = pipeparts.mkqueue(pipeline, head, max_size_buffers = 3)
	head = pipeparts.mkwhiten(pipeline, head, fft_length = psd_fft_length, average_samples = int(round(float(gps_stop_time - gps_start_time) / (psd_fft_length / 2) - 1)))
	pipeparts.mkfakesink(pipeline, head)

	handler = PSDHandler(mainloop, pipeline, verbose = verbose)

	#
	# process segment
	#

	pipeline.set_state(gst.STATE_PAUSED)
	pipeline.seek(1.0, gst.Format(gst.FORMAT_TIME), gst.SEEK_FLAG_FLUSH, gst.SEEK_TYPE_SET, gps_start_time.ns(), gst.SEEK_TYPE_SET, gps_stop_time.ns())
	pipeline.set_state(gst.STATE_PLAYING)
	mainloop.run()

	#
	# done
	#

	return handler.psd


def write_psd(filename, psd, verbose = False):
	xmldoc = ligolw.Document()
	xmldoc.appendChild(ligolw.LIGO_LW()).appendChild(lalseries.build_REAL8FrequencySeries(psd))
	utils.write_filename(xmldoc, filename, gz = (filename or "stdout").endswith(".gz"), verbose = verbose)


#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#


#
# parse command line
#


options, filenames = parse_command_line()


#
# construct pipeline metadata and measure the PSD
#


psd_fft_length = 8


detectors = {
	options.instrument: DetectorData(options.frame_cache, options.channel_name)
}


if options.reference_psd is not None:
	psd = lalseries.parse_REAL8FrequencySeries(utils.load_filename(options.reference_psd, gz = (options.reference_psd or "stdin").endswith(".gz"), verbose = options.verbose))
else:
	psd = measure_psd(
		options.instrument,
		detectors[options.instrument],
		options.gps_start_time,
		options.gps_stop_time,
		2 * 2048,	# Hz;  must not be less than highest bank fragment sample rate (see below)
		psd_fft_length = psd_fft_length,
		fake_data = options.fake_data,
		injection_filename = options.injections,
		verbose = options.verbose
	)
	if options.write_psd is not None:
		write_psd(options.write_psd, psd, verbose = options.verbose)


banks = [
	Bank(
		template_bank_filename,
		psd,
		[
			Bank.BankFragment(2048, 0.0, 1.0),
			Bank.BankFragment(512, 1.0, 5.0),
			Bank.BankFragment(256, 5.0, 13.0),
			Bank.BankFragment(128, 13.0, 29.0)
		],
		gate_threshold = 2.0,
		snr_threshold = 5.5,
		tolerance = 0.9995,
		flow = 40.0,	# Hz
		autocorrelation_length = 201,	# samples
		logname = "bank%d" % n,
		verbose = options.verbose
	) for n, template_bank_filename in enumerate(options.template_bank)
]


#
# build pipeline
#


pipeline = gst.Pipeline("lloid")
mainloop = gobject.MainLoop()

mkLLOIDmulti(
	pipeline,
	detectors = detectors,
	banks = banks,
	psd = psd,
	output_filename = options.output,
	psd_fft_length = psd_fft_length,
	fake_data = options.fake_data,
	injection_filename = options.injections,
	verbose = options.verbose
)

if options.write_pipeline is not None:
	gst.xml_write_file(pipeline, file(options.write_pipeline, "w"))

handler = LLOIDHandler(mainloop, pipeline, verbose = options.verbose)


#
# process requested segment
#


pipeline.set_state(gst.STATE_PAUSED)
pipeline.seek(1.0, gst.Format(gst.FORMAT_TIME), gst.SEEK_FLAG_FLUSH, gst.SEEK_TYPE_SET, options.gps_start_time.ns(), gst.SEEK_TYPE_SET, options.gps_stop_time.ns())
pipeline.set_state(gst.STATE_PLAYING)
mainloop.run()


#
# done
#
