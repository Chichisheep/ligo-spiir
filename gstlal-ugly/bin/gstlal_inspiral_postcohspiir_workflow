#!/usr/bin/env python
#
# Copyright (C) 2011-2014 Chad Hanna
# Copyright (C) 2017 Qi Chu
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

## @file gstlal_inspiral_postcohspiir_daggen
# The offline gstlal inspiral workflow generator; Use to make HTCondor DAGs to run CBC workflows
#
# ### Usage:
# It is rare that you would invoke this program in a standalone mode. Usually the inputs are complicated and best automated via a Makefile, e.g., Makefile.triggers_example
#
# ### Command line options
#
# See datasource.append_options() for generic options
#
#	+ `--psd-fft-length` [int] (s): FFT length, default 16s.
#	+ `--reference-psd: Don't measure PSDs, use this one instead
#	+ `--overlap [int]: Set the factor that describes the overlap of the sub banks, must be even!
#	+ `--autocorrelation-length [int]: The minimum number of samples to use for auto-chisquared, default 201 should be odd
#	+ `--bank-cache [file names]: Set the bank cache files in format H1=H1.cache,H2=H2.cache, etc..
#	+ `--tolerance [float]: set the SPIIR tolerance, default 0.9999
#	+ `--flow [float]: set the low frequency cutoff, default 40 (Hz)
#	+ `--identity-transform: Use identity transform, i.e. no SPIIR
#	+ `--vetoes [filename]: Set the veto xml file.
#	+ `--time-slide-file [filename]: Set the time slide table xml file
#	+ `--web-dir [filename]: Set the web directory like /home/USER/public_html
#	+ `--fir-stride [int] (s): Set the duration of the fft output blocks, default 8
#	+ `--control-peak-time [int] (s): Set the peak finding time for the control signal, default 8
#	+ `--coincidence-threshold [int] (s): Set the coincidence window in seconds (default = 0.005).  The light-travel time between instruments will be added automatically in the coincidence test.
#	+ `--num-banks [str]: The number of parallel subbanks per gstlal_inspiral job. can be given as a list like 1,2,3,4 then it will split up the bank cache into N groups with M banks each.
#	+ `--max-inspiral-jobs [int]: Set the maximum number of gstlal_inspiral jobs to run simultaneously, default no constraint.
#	+ `--ht-gate-threshold [float]: set a threshold on whitened h(t) to veto glitches
#	+ `--inspiral-executable [str]: Options gstlal_inspiral | gstlal_iir_inspiral, default gstlal_inspiral
#	+ `--blind-injections [filename]: Set the name of an injection file that will be added to the data without saving the sim_inspiral table or otherwise processing the data differently.  Has the effect of having hidden signals in the input data. Separate injection runs using the --injections option will still occur.
#	+ `--far-injections` [filename]: Injection files with injections too far away to be seen that are not filtered. Must be 1:1 with --injections if given at all. See https://www.lsc-group.phys.uwm.edu/ligovirgo/cbcnote/NSBH/MdcInjections/MDC1 for example.
#	+ `--condor-command`: Set condor commands of the form command=value; can be given multiple times.
#	+ `--verbose: Be verbose
#
# ### Diagram of the HTCondor workfow produced
# @dotfile trigger_pipe.dot
# 

"""
This program makes a dag to run gstlal_inspiral_postcohspiir_online in offline mode
"""

__author__ = 'Chad Hanna <chad.hanna@ligo.org> Qi Chu <qi.chu@ligo.org>'

##############################################################################
# import standard modules and append the lalapps name to the python path
import sys, os, copy, math, stat, re
import subprocess, socket, tempfile
import itertools

##############################################################################
# import the modules we need to build the pipeline
from glue import iterutils
from glue import pipeline
from glue import lal
from glue import segments
from glue.ligolw import ligolw
from glue.ligolw import array as ligolw_array
from glue.ligolw import lsctables
from glue.ligolw import param as ligolw_param
import glue.ligolw.utils as ligolw_utils
import glue.ligolw.utils.segments as ligolw_segments
from optparse import OptionParser
from gstlal import inspiral, inspiral_pipe
from gstlal import dagparts as gstlaldagparts
from pylal import series as lalseries
import numpy
from pylal.datatypes import LIGOTimeGPS
from gstlal import datasource
import pdb

class LIGOLWContentHandler(ligolw.LIGOLWContentHandler):
	pass
ligolw_array.use_in(LIGOLWContentHandler)
lsctables.use_in(LIGOLWContentHandler)
ligolw_param.use_in(LIGOLWContentHandler)


#
# Utility functions
#


# deprecated please use inspiral_pipe.T050017_filename instead
# adapted from inspiral_pipe.py same function
def T050017_filename(instruments, description, start = None, end = None, extension = None, path = None):
	"""!
	A function to generate a T050017 filename.
	"""
	if type(instruments) != type(str()):
		instruments = "".join(sorted(instruments))
	if start is not None:
	  duration = end - start
	  extension = extension.strip('.')
	  if path is not None:
	    return '%s/%s-%s-%d-%d.%s' % (path, instruments, description, start, duration, extension)
	  else:
	    return '%s-%s-%d-%d.%s' % (instruments, description, start, duration, extension)
	else:
	  if path is not None:
		return '%s/%s-%s' % (path, instruments, description)
	  else:
		return '%s-%s' % (instruments, description)


def sim_tag_from_inj_file(injections):
	if injections is None:
		return None
	return injections.replace('.xml', '').replace('.gz', '').replace('-','_')

def get_max_length_from_bank(bank_cache, verbose = False):
	max_time = 0
	cache = lal.Cache.fromfilenames([bank_cache.values()[0]])
	for f in cache.pfnlist():
		xmldoc = ligolw_utils.load_filename(f, verbose = verbose, contenthandler = LIGOLWContentHandler)
		sngls = lsctables.SnglInspiralTable.get_table(xmldoc)
		max_time = max(max_time, max(sngls.get_column('template_duration')))
		xmldoc.unlink()

	return max_time

def chunks(l, n):
	for i in xrange(0, len(l), n):
		yield l[i:i+n]

def flatten(lst):
    "Flatten one level of nesting"
    return list(itertools.chain.from_iterable(lst))

#
# get a dictionary of all the disjoint 2+ detector combination segments
#

def gen_analysis_segments(analyzable_instruments_set, allsegs, boundary_seg, max_template_length):
	segsdict = segments.segmentlistdict()
	# 512 seconds for the whitener to settle + the maximum template_length
	start_pad = 512 + max_template_length
	# Chosen so that the overlap is only a ~5% hit in run time for long segments...
	segment_length = int(20 * start_pad)
	for n in range(2, 1 + len(analyzable_instruments_set)):
		for ifo_combos in iterutils.choices(list(analyzable_instruments_set), n):
			# never analyze H1H2 or H2L1 times
			if set(ifo_combos) == set(('H1', 'H2')) or set(ifo_combos) == set(('L1', 'H2')):
				print >> sys.stderr, "not analyzing: ", ifo_combos, " only time"
				continue
			segsdict[frozenset(ifo_combos)] = allsegs.intersection(ifo_combos) - allsegs.union(analyzable_instruments_set - set(ifo_combos))
			segsdict[frozenset(ifo_combos)] &= segments.segmentlist([boundary_seg])
			segsdict[frozenset(ifo_combos)] = segsdict[frozenset(ifo_combos)].protract(start_pad) #FIXME don't hard code
			segsdict[frozenset(ifo_combos)] = gstlaldagparts.breakupsegs(segsdict[frozenset(ifo_combos)], segment_length, start_pad) #FIXME don't hardcode
			if not segsdict[frozenset(ifo_combos)]:
				del segsdict[frozenset(ifo_combos)]
	return segsdict

def gen_psdJob_options(seg, channeldict, ifos, daggen_options):
	opts = {"gps-start-time":seg[0].seconds,
		"gps-end-time":seg[1].seconds,
		"data-source":"frames",
		"channel-name":datasource.pipeline_channel_list_from_channel_dict(channeldict, ifos = ifos),
		"psd-fft-length":daggen_options.psd_fft_length,
		"frame-segments-name": daggen_options.frame_segments_name}
	return opts

def gen_psd_node(refPSDJob, dag, parent_nodes, segsdict, channel_dict, options):
	psd_nodes = {}
	for ifos in segsdict:
		this_channel_dict = dict((k, channel_dict[k]) for k in ifos if k in channel_dict)
		for seg in segsdict[ifos]:
			psd_nodes[(ifos, seg)] = \
				inspiral_pipe.generic_node(
						refPSDJob, 
						dag, 
						parent_nodes = parent_nodes,
						opts = gen_psdJob_options(seg, this_channel_dict, ifos, options),
					input_files = {	"frame-cache":options.frame_cache,
							"frame-segments-file":options.frame_segments_file},
					output_files = {"write-psd":inspiral_pipe.T050017_filename(ifos, "REFERENCE_PSD", seg[0].seconds, seg[1].seconds, '.xml.gz', path = refPSDJob.output_path)}
				)
	return psd_nodes

def gen_bankJob_options(ifo, daggen_options):
	opts = {
		"instrument": ifo,
		"flow": daggen_options.flow,
		"epsilon": daggen_options.epsilon,
		"sampleRate": daggen_options.sampleRate,
		"req-min-match": daggen_options.req_min_match,
		"autocorrelation-length": daggen_options.autocorrelation_length,
		"padding": daggen_options.padding,
		"downsample": ""
		}
	return opts

def gen_bank_node(bankJob, dag, parent_nodes, psd, bank_groups, options, seg):
	bank_nodes = {}
	"""
	 bank_groups: [
	 {'H1':('bank1, bank2'), L1:('bank1', 'bank2')},
	 {'H1':('bank3, bank4'), L1:('bank3', 'bank4')}
	 ]
	"""
	for i, bank_group in enumerate(bank_groups):
		for ifo, template_files in bank_group.items():
			for template_bank in template_files:
				# FIXME:bank_id is searched from the template_bank name
				bank_id = re.search(r'\d+', template_bank.split("/")[-1].split("_")[-1]).group()
				spiir_bank_name = inspiral_pipe.T050017_filename(ifo, '%s_SPIIR' % (bank_id,), seg[0].seconds, seg[1].seconds, '.xml.gz', path = bankJob.output_path)

				bank_nodes.setdefault(ifo, []).append(
					inspiral_pipe.generic_node(
						bankJob, 
						dag,
						parent_nodes = parent_nodes, 
						opts = gen_bankJob_options(ifo, options),
					input_files = {
						"reference-psd":psd,
						"template-bank":template_bank},
					output_files = {"output":spiir_bank_name}
					)
				)
	return bank_nodes

def gen_spiir_bank_strings_from_banknode(bank_nodes, instruments = None):
	"""
	outstrings: ["H1:bankname1,L1:bankname1", "H1:bankname2,L1:bankname2", ...]
	"""
	# FIXME assume that the number of spiir nodes is the same per ifo, a good assumption though
	outstrings = []
	for i in range(len(bank_nodes.values()[0])):
		spiir_bank_string = ""
		for ifo in bank_nodes:
			if instruments is not None and ifo not in instruments:
				continue
			spiir_bank_string += "%s:%s," % (ifo, bank_nodes[ifo][i].output_files["output"])
		spiir_bank_string = spiir_bank_string.strip(",")
		outstrings.append(spiir_bank_string)
	return outstrings


def gen_spiir_bank_cache(spiir_bank_strings, counter, injection = False):
	if injection:
		dir_name = "gstlal_inspiral_postcohspiir_inj"
	else:
		dir_name = "gstlal_inspiral_postcohspiir"
	spiir_cache_entries = []
	parsed_spiir_bank_strings = [inspiral.parse_iirbank_string(one_spiir_bank_string) for one_spiir_bank_string in spiir_bank_strings]
	for spiir_bank_parsed_dict in parsed_spiir_bank_strings:
		for url in spiir_bank_parsed_dict.itervalues():
			#spiir_cache_entries.append(lal.CacheEntry.from_T050017(url))
			spiir_cache_entries.append(url)

	return [spiir_cache_entry.url for spiir_cache_entry in spiir_cache_entries] 

# set the histogram trials to be 1 for shift jobs
def gen_inspiralshiftJob_options(job_tag, spiir_bank_strings, seg, channeldict, daggen_options):
	input_spiir_bank_string = ""
	for one_string in spiir_bank_strings:
		input_spiir_bank_string += "%s --iir-bank " % one_string

	input_spiir_bank_string = input_spiir_bank_string.rstrip(" --iir-bank ")

	#output_stats_string = ""
	#for one_string in stats_name_list:
	#	output_stats_string += "%s --cohfar-accumbackground-output-name " % one_string
	#output_stats_string = output_stats_string.rstrip(" --cohfar-accumbackground-output-name ")
	detrsp_map_name = "%s_detrsp_map_start_param.xml" % ifo_string
	opts = {
		"job-tag": job_tag,
		"iir-bank": input_spiir_bank_string,
		"psd-fft-length":daggen_options.psd_fft_length,
		"ht-gate-threshold":daggen_options.ht_gate_threshold,
		"frame-segments-name":daggen_options.frame_segments_name,
		"gps-start-time":seg[0].seconds,
		"gps-end-time":seg[1].seconds,
		"channel-name":datasource.pipeline_channel_list_from_channel_dict(channeldict),
		"tmp-space":inspiral_pipe.condor_scratch_space(),
		"track-psd":"",
		"data-source":"frames",
		"cuda-postcoh-detrsp-fname": detrsp_map_name,
		"cuda-postcoh-hist-trials":"1",
		"cuda-postcoh-output-skymap":daggen_options.cuda_postcoh_output_skymap
		}
	return opts
	
def gen_inspiralJob_options(job_tag, spiir_bank_strings, seg, channeldict, daggen_options, ifos):
	input_spiir_bank_string = ""
	for one_string in spiir_bank_strings:
		input_spiir_bank_string += "%s --iir-bank " % one_string

	input_spiir_bank_string = input_spiir_bank_string.rstrip(" --iir-bank ")

	#output_stats_string = ""
	#for one_string in stats_name_list:
	#	output_stats_string += "%s --cohfar-accumbackground-output-name " % one_string
	#output_stats_string = output_stats_string.rstrip(" --cohfar-accumbackground-output-name ")
	ifos_string = "" 
	for ifo in ifos:
		ifos_string += "%s" % ifo

	detrsp_map_name = "%s_detrsp_map_start_param.xml" % ifo_string
	opts = {
		"job-tag": job_tag,
		"iir-bank": input_spiir_bank_string,
		"psd-fft-length":daggen_options.psd_fft_length,
		"ht-gate-threshold":daggen_options.ht_gate_threshold,
		"frame-segments-name":daggen_options.frame_segments_name,
		"gps-start-time":seg[0].seconds,
		"gps-end-time":seg[1].seconds,
		"channel-name":datasource.pipeline_channel_list_from_channel_dict(channeldict),
		"tmp-space":inspiral_pipe.condor_scratch_space(),
		"track-psd":"",
		"data-source":"frames",
		"cuda-postcoh-detrsp-fname": detrsp_map_name,
		"cuda-postcoh-hist-trials":daggen_options.cuda_postcoh_hist_trials,
		"cuda-postcoh-output-skymap":daggen_options.cuda_postcoh_output_skymap
		}
	return opts
	

def gen_inspiral_node(inspiralPostcohspiirJob, inspiralPostcohspiirShiftJob, inspiralPostcohspiirInjJob, dag, bank_nodes, segsdict, options, channel_dict):

	inspiral_nodes = {}
	# setup the dict to index the output by bankbins
	spiir_zlag = {}
	spiir_zlag[None] = {}
	spiir_zlag["shift"] = {}
	for injections in options.injections:
		spiir_zlag[sim_tag_from_inj_file(injections)] = {}

	spiir_stats = {}
	spiir_stats[None] = {}
	spiir_stats["shift"] = {}
	for injections in options.injections:
		spiir_stats[sim_tag_from_inj_file(injections)] = {}

	for ifos in segsdict:

		# setup dictionaries to hold the inspiral nodes
		inspiral_nodes[(ifos, None)] = {}	
		inspiral_nodes[(ifos, "shift")] = {}	
		for injections in options.injections:
			inspiral_nodes[(ifos, sim_tag_from_inj_file(injections))] = {}

		
		numbanks = max(options.num_banks)

		seg_counter = 0
		for seg in segsdict[ifos]:
			
			# only use a channel dict with the relevant channels
			this_channel_dict = dict((k, channel_dict[k]) for k in ifos if k in channel_dict)

			# get the svd bank strings
			spiir_bank_strings_full = gen_spiir_bank_strings_from_banknode(bank_nodes, instruments = this_channel_dict.keys())
			""" 
			output zerolags format example: 
			gstlal_inspiral_postcohspiir_online/bankbin1/H1L1-bankbin1_SPIIR_zerolag-1180xx-14800.xml.gz
			output stats format:
			gstlal_inspiral_postcohspiir_online/bankbin1/H1L1-0001_SPIIR_stats-1180xx-14800.xml.gz
			"""
			for chunk_counter, spiir_bank_strings in enumerate(chunks(spiir_bank_strings_full, numbanks)):

				# results for one bank chunk will be stored in the same place
				job_tag = "%s/bankbin%d" % (inspiralPostcohspiirJob.tag_base, chunk_counter)
				# setup output names
				output_name = inspiral_pipe.T050017_filename(ifos, "bankbin%d_SPIIR_zerolag" % chunk_counter, seg[0].seconds, seg[1].seconds, "xml.gz", path = job_tag)
				# to be consistent with the bank name
				stats_name_list = [T050017_filename(ifos, "%04d_SPIIR_stats" % (i + numbanks * chunk_counter, ), seg[0].seconds, seg[1].seconds, "xml.gz", path = job_tag) for i,s in enumerate(spiir_bank_strings)]
				# non injection node
				# FIXME: single detector postcoh ?
				noninjnode = inspiral_pipe.generic_node(
						inspiralPostcohspiirJob, 
						dag, 
						parent_nodes = sum(bank_nodes.values(),[]),
						opts = gen_inspiralJob_options(job_tag, spiir_bank_strings, seg, this_channel_dict, options, ifos),
						input_files = {
							"frame-cache":options.frame_cache,
							"frame-segments-file":options.frame_segments_file,
							"reference-psd":psd_nodes[(ifos, seg)].output_files["write-psd"],
							"blind-injections":options.blind_injections,
							"veto-segments-file":options.vetoes,
							},
						output_files = {
							"finalsink-output-name": output_name,
							"cohfar-accumbackground-output-name": stats_name_list,
							},

						)
				# Set a post script to check for file integrity
				noninjnode.set_post_script("gzip_test.sh")
				check_files = ["%s" % output_name] + ["%s" % one_stats_name for one_stats_name in stats_name_list]
				noninjnode.add_post_script_arg(" ".join(check_files))
				#noninjnode.add_post_script_arg(" ".join(output_names + dist_stats_names))
				# impose a priority to help with depth first submission
				noninjnode.set_priority(chunk_counter)
				inspiral_nodes[(ifos, None)].setdefault(seg, []).append(noninjnode)
				spiir_zlag[None].setdefault("bankbin%d" % chunk_counter, []).append(noninjnode)
				spiir_stats[None].setdefault("bankbin%d" % chunk_counter, []).append(noninjnode)

			""" 
			output zerolags format example: 
			gstlal_inspiral_postcohspiir_shift/bankbin1/H1L1-bankbin1_SPIIR_zerolag-1180xx-14800.xml.gz
			output stats format:
			gstlal_inspiral_postcohspiir_shift/bankbin1/H1L1-0001_SPIIR_stats-1180xx-14800.xml.gz
			"""
			# for time-shifted jobs:
			for chunk_counter, spiir_bank_strings in enumerate(chunks(spiir_bank_strings_full, numbanks)):

				# results for one bank chunk will be stored in the same place
				job_tag = "%s/bankbin%d" % (inspiralPostcohspiirShiftJob.tag_base, chunk_counter)
				# setup output names
				output_name = inspiral_pipe.T050017_filename(ifos, "bankbin%d_SPIIR_zerolag" % chunk_counter, seg[0].seconds, seg[1].seconds, "xml.gz", path = job_tag)
				stats_name_list = [T050017_filename(ifos, "%04d_SPIIR_stats" % (i + numbanks * chunk_counter, ), seg[0].seconds, seg[1].seconds, "xml.gz", path = job_tag) for i,s in enumerate(spiir_bank_strings)]
	
				# non injection node
				shiftnode = inspiral_pipe.generic_node(
						inspiralPostcohspiirShiftJob, 
						dag, 
						parent_nodes = sum(bank_nodes.values(),[]),
						opts = gen_inspiralshiftJob_options(job_tag, spiir_bank_strings, seg, this_channel_dict, options, ifos),
						# FIXME: hard-coded to be 8pi
						input_files = {
							"frame-cache":options.frame_cache,
							"frame-segments-file":options.frame_segments_file,
							"reference-psd":psd_nodes[(ifos, seg)].output_files["write-psd"],
							"blind-injections":options.blind_injections,
							"veto-segments-file":options.vetoes,
							"control-time-shift-string":"H1:25.13272",
							},
						output_files = {
							"finalsink-output-name": output_name,
							"cohfar-accumbackground-output-name": stats_name_list,
							},


						)
				# Set a post script to check for file integrity
				shiftnode.set_post_script("gzip_test.sh")
				check_files = ["%s" % output_name] + ["%s" % one_stats_name for one_stats_name in stats_name_list]
				shiftnode.add_post_script_arg(" ".join(check_files))
				#shiftnode.add_post_script_arg(" ".join(output_names + dist_stats_names))
				# impose a priority to help with depth first submission
				shiftnode.set_priority(chunk_counter)
				inspiral_nodes[(ifos, "shift")].setdefault(seg, []).append(shiftnode)
				spiir_zlag["shift"].setdefault("bankbin%d" % chunk_counter, []).append(noninjnode)
				spiir_stats["shift"].setdefault("bankbin%d" % chunk_counter, []).append(noninjnode)



			""" 
			output zerolags format example: 
			gstlal_inspiral_postcohspiir_shift/bankbin1/H1L1-bankbin1_SPIIR_zerolag-1180xx-14800.xml.gz
			output stats format:
			gstlal_inspiral_postcohspiir_shift/bankbin1/H1L1-0001_SPIIR_stats-1180xx-14800.xml.gz
			"""
			for chunk_counter, spiir_bank_strings in enumerate(chunks(spiir_bank_strings_full, numbanks)):
				# process injections
				inj_counter = 0
				for injections in options.injections:
					# results for one bank chunk will be stored in the same place
					job_tag = "%s_inj%d/bankbin%d" % (inspiralPostcohspiirJob.tag_base, inj_counter, chunk_counter)
					# setup output names
					output_name = inspiral_pipe.T050017_filename(ifos, "bankbin%d_SPIIR_zerolag" % chunk_counter, seg[0].seconds, seg[1].seconds, "xml.gz", path = job_tag)
					stats_name_list = [T050017_filename(ifos, "%04d_SPIIR_stats" % (i + numbanks * chunk_counter, ), seg[0].seconds, seg[1].seconds, "xml.gz", path = job_tag) for i,s in enumerate(spiir_bank_strings)]
	
					sim_name = sim_tag_from_inj_file(injections)
	
					# setup injection node
					injnode = inspiral_pipe.generic_node(
						inspiralPostcohspiirInjJob, 
						dag, 
						parent_nodes = sum(bank_nodes.values(),[]),
						opts = gen_inspiralJob_options(job_tag, spiir_bank_strings, seg, this_channel_dict, options, ifos),
						input_files = {
							"frame-cache":options.frame_cache,
							"frame-segments-file":options.frame_segments_file,
							"reference-psd":psd_nodes[(ifos, seg)].output_files["write-psd"],
							"injections":injections,
							"veto-segments-file":options.vetoes,
							},
						output_files = {
							"finalsink-output-name": output_name,
							"cohfar-accumbackground-output-name": stats_name_list,
							},

					)
					# Set a post script to check for file integrity
					injnode.set_post_script("gzip_test.sh")
					check_files = ["%s" % output_name] + ["%s" % one_stats_name for one_stats_name in stats_name_list]
					injnode.add_post_script_arg(" ".join(check_files))
					# impose a priority to help with depth first submission
					injnode.set_priority(chunk_counter)
					inspiral_nodes[(ifos, sim_name)].setdefault(seg, []).append(injnode)
					spiir_zlag[sim_name].setdefault("bankbin%d" % chunk_counter, []).append(noninjnode)
					spiir_stats[sim_name].setdefault("bankbin%d" % chunk_counter, []).append(noninjnode)
					inj_counter += 1
			seg_counter += 1

	return inspiral_nodes, spiir_zlag, spiir_stats

""" 
output zerolags format example: 
gstlal_inspiral_postcohspiir_online/bankbin1/H1L1-bankbin1_SPIIR_zerolag-1180xx-14800.xml.gz
output stats format:
gstlal_inspiral_postcohspiir_online/bankbin1/H1L1-0001_SPIIR_stats-1180xx-14800.xml.gz
"""


"""
step 1: merge the stats of the same bankbin from whatever ifo combo files.
The output name is gstlal_cohfar_merge_stats/bankbin1_SPIIR_stats-boundarystart-boundaryend.xml.gz, ..
with single_ifo to single_ifo stats, double_ifo to double_ifo stats, and tri_ifo to tri_ifo stats
step 2: merge bankbin1_xx, bankbin2_xx, .. to gstlal_cohfar_merge_stats/bankbox1_SPIIR_stats-boundarystart-boundaryend.xml.gz to assign fap for zerolags
"""

def merge_stats(dag, mergestatsJob, spiir_stats, instrument_set, boundary_seg, bankbox_list):

	mergestats_nodes = []
	mergestats_bin_nodes = []
	mergestats_box_nodes = []
	tmp_nodes_round1 = []
	tmp_nodes_round2 = []
	# FIXME: hard-coded split stats files to be processed
	nstats_split = 10
	nstats_remain = 0
	ifos = "".join(instrument_set)
	# non-injections
	for ibin, binname in enumerate(spiir_stats[None].keys()):
		tmp_nodes_round1 = []
		nstats_remain = 0
		# merge stats every nstats_split files to tmpID_SPIIR_stats-start-end.xml.gz
		for tmp_counter, binnodes in enumerate(chunks(spiir_stats[None][binname], nstats_split)):
			stats_name = ",".join([one_stats_name for i in xrange(0, len(binnodes)) for one_stats_name in binnodes[i].output_files["cohfar-accumbackground-output-name"]])
			output_path = "%s/%s" % (mergestatsJob.tag_base, binname)
			if not os.path.exists(output_path):
				os.mkdir(output_path)
			one_tmp_node = inspiral_pipe.generic_node(mergestatsJob, dag,
				parent_nodes = binnodes,
				opts = {"input-format": "stats",
					"duration": 0,
					"ifos": ifos},
				input_files = {"input":stats_name},
				output_files = {"output":inspiral_pipe.T050017_filename("tmp%d" % tmp_counter,"%s_SPIIR_stats" % binname, boundary_seg[0].seconds, boundary_seg[1].seconds, '.xml.gz', path = output_path)}
			)
			tmp_nodes_round1.append(one_tmp_node)
			nstats_remain += 1

		mergestats_nodes.append(tmp_nodes_round1)
		
		tmp_nodes_round2 = []
		# if there are still many tmp stats left, such like tmp1, tmp2,.. tmp50,..
		# get a second round of merging, so remaining tmp1, tmp2, ... tmp5,
		if nstats_remain > nstats_split:
			for tmp_counter, binnodes in enumerate(chunks(tmp_nodes_round1, nstats_split)):
				stats_name = ",".join([binnodes[i].output_files["output"] for i in xrange(0, len(binnodes))])
				output_path = "%s/%s" % (mergestatsJob.tag_base, binname)
				one_tmp_node = inspiral_pipe.generic_node(mergestatsJob, dag,
					parent_nodes = binnodes,
					opts = {"input-format": "stats",
						"duration": 0,
						"ifos": ifos},
					input_files = {"input":stats_name},
					output_files = {"output":inspiral_pipe.T050017_filename("tmp%d" % tmp_counter,"%s_SPIIR_stats" % binname, boundary_seg[0].seconds, boundary_seg[1].seconds, '.xml.gz', path = output_path)}
			)
				tmp_nodes_round2.append(one_tmp_node)
			mergestats_nodes.append(tmp_nodes_round2)

		if len(tmp_nodes_round2) > 0:
			stats_name = ",".join([tmp_nodes_round2[i].output_files["output"] for i in xrange(0, len(tmp_nodes_round2))])
			parent_nodes = tmp_nodes_round2
		else:
			stats_name = ",".join([tmp_nodes_round1[i].output_files["output"] for i in xrange(0, len(tmp_nodes_round1))])
			parent_nodes = tmp_nodes_round1

		output_path = "%s/%s" % (mergestatsJob.tag_base, binname)
		one_bin_merge_node = inspiral_pipe.generic_node(mergestatsJob, dag,
			parent_nodes = parent_nodes,
			opts = {"input-format": "stats",
				"duration": 0,
				"ifos": ifos},
			input_files = {"input":stats_name},
			output_files = {"output":inspiral_pipe.T050017_filename("%s" % binname,"SPIIR_stats", boundary_seg[0].seconds, boundary_seg[1].seconds, '.xml.gz', path = output_path)}
			)
	
		mergestats_nodes.append(one_bin_merge_node)
		mergestats_bin_nodes.append(one_bin_merge_node)

	# merge a set of bins to a box
	ibin_start = 0
	for ibox, nbin in enumerate(bankbox_list):
		# sort the bankbin outputs to be from small mass to large mass
		sort_bin_nodes = sorted(mergestats_bin_nodes, key = lambda x: int((re.search("\d+", x.output_files["output"])).group(0)))
		stats_name = ",".join([sort_bin_nodes[i].output_files["output"] for i in xrange(ibin_start, ibin_start+nbin)])
		ibin_start += nbin
		output_path = "%s/bankbox%d" % (mergestatsJob.tag_base, ibox)
		if not os.path.exists(output_path):
			os.mkdir(output_path)
		one_box_merge_node = inspiral_pipe.generic_node(mergestatsJob, dag,
			parent_nodes = binnodes,
			opts = {"input-format": "stats",
				"duration": 0,
				"ifos": ifos},
			input_files = {"input":stats_name},
			output_files = {"output":inspiral_pipe.T050017_filename("bankbox%d" % ibox,"SPIIR_stats", boundary_seg[0].seconds, boundary_seg[1].seconds, '.xml.gz', path = output_path)}
		)
		mergestats_box_nodes.append(one_box_merge_node)

		#FIXME: remove the tmp stats



def cluster_zerolags():
	"""
	cluster zerolags in 4s window with zerolag SNRs
	step 1: cluster zerolags over boundary start-end
	output gstlal_cluster_zerolags_online/bankbin1_SPIIR_zerolag-boundarystart-boundaryend.xml.gz
	step 2: cluster bankbins to bankboxs
	output gstlal_cluster_zerolags_online/bankbox1_SPIIR_zerolag-boundarystart-boundaryend.xml.gz
	"""
	# Break up the likelihood jobs into chunks to process fewer files, e.g, 25
	likelihood_nodes.setdefault(None,[]).append(
		[inspiral_pipe.generic_node(calcLikelihoodJob, dag,
			parent_nodes = [priornode, snrpdfnode] + parents, # add parents here in case a gstlal inpsiral job's trigger file is corrupted - then we can just mark that job as not done and this job will rerun. 
			opts = {"tmp-space":inspiral_pipe.condor_scratch_space()},
			input_files = {"":chunked_inputs},
			input_cache_files = {"likelihood-cache":diststats +
			[priornode.output_files["write-likelihood"],
				snrpdfnode.output_files["write-likelihood"]]
				}
			) for chunked_inputs in chunks(inputs, 25)]
		)

	# then injections
	for inj in options.injections:
		for n, (outputs, diststats) in enumerate((spiir_output[sim_tag_from_inj_file(inj)][key], spiir_diststats[key]) for key in sorted(spiir_output[None].keys())):
			inputs = [o[0] for o in outputs]
			parents = []
			[parents.extend(o[1]) for o in outputs]
			# Break up the likelihood jobs into chunks to process fewer files, e.g., 25
			likelihood_nodes.setdefault(sim_tag_from_inj_file(inj),[]).append(
				[inspiral_pipe.generic_node(calcLikelihoodJobInj, dag,
					parent_nodes = parents + [priornodes[n], snrpdfnode],
					opts = {"tmp-space":inspiral_pipe.condor_scratch_space()},
					input_files = {"":chunked_inputs},
					input_cache_files = {"likelihood-cache":diststats + 
						[priornodes[n].output_files["write-likelihood"], 
						snrpdfnode.output_files["write-likelihood"]]
						}
					) for chunked_inputs in chunks(inputs, 25)]
				)

	
	# after assigning the likelihoods cluster and merge by sub bank and whether or not it was an injection run
	files_to_group = 10
	for subbank, (inj, nodes) in enumerate(likelihood_nodes.items()):
		# Flatten the nodes for this sub bank
		nodes = flatten(nodes)
		merge_nodes = []
		# Flatten the input/output files from calc_likelihood
		inputs = flatten([node.input_files[""] for node in nodes])
		if inj is None:
			# 10 at a time irrespective of the sub bank they came from so the jobs take a bit longer to run
			for n in range(0, len(inputs), files_to_group):
				merge_nodes.append(inspiral_pipe.generic_node(lalappsRunSqliteJob, dag, parent_nodes = nodes,
					opts = {"sql-file":options.cluster_sql_file, "tmp-space":inspiral_pipe.condor_scratch_space()},
					input_files = {"":inputs[n:n+files_to_group]}
					)
				)
				if options.copy_raw_results:
					merge_nodes[-1].set_pre_script("store_raw.sh")
					merge_nodes[-1].add_pre_script_arg(" ".join(inputs[n:n+files_to_group]))

			# Merging all the dbs from the same sub bank
			for subbank, inputs in enumerate([node.input_files[""] for node in nodes]):
				db = inspiral_pipe.T050017_filename(instruments, '%04d_LLOID' % (subbank,), int(boundary_seg[0]), int(boundary_seg[1]), '.sqlite')
				sqlitenode = inspiral_pipe.generic_node(toSqliteJob, dag, parent_nodes = merge_nodes,
					opts = {"replace":"", "tmp-space":inspiral_pipe.condor_scratch_space()},
					input_files = {"":inputs},
					output_files = {"database":db}
				)
				sqlitenode = inspiral_pipe.generic_node(lalappsRunSqliteJob, dag, parent_nodes = [sqlitenode],
					opts = {"sql-file":options.cluster_sql_file, "tmp-space":inspiral_pipe.condor_scratch_space()},
					input_files = {"":db}
				)
				outnodes.setdefault(None, []).append(sqlitenode)
		else:
			# 10 at a time irrespective of the sub bank they came from so the jobs take a bit longer to run
			for n in range(0, len(inputs), files_to_group):
				merge_nodes.append(inspiral_pipe.generic_node(lalappsRunSqliteJob, dag, parent_nodes = nodes,
					opts = {"sql-file":options.injection_sql_file, "tmp-space":inspiral_pipe.condor_scratch_space()},
					input_files = {"":inputs[n:n+files_to_group]}
					)
				)
				if options.copy_raw_results:
					merge_nodes[-1].set_pre_script("store_raw.sh")
					merge_nodes[-1].add_pre_script_arg(" ".join(inputs[n:n+files_to_group]))

			# Merging all the dbs from the same sub bank and injection run
			for subbank, inputs in enumerate([node.input_files[""] for node in nodes]):
				injdb = inspiral_pipe.T050017_filename(instruments, '%04d_LLOID_%s' % (subbank, sim_tag_from_inj_file(inj)), int(boundary_seg[0]), int(boundary_seg[1]), '.sqlite')
				sqlitenode = inspiral_pipe.generic_node(toSqliteJob, dag, parent_nodes = merge_nodes,
					opts = {"replace":"", "tmp-space":inspiral_pipe.condor_scratch_space()},
					input_files = {"":inputs},
					output_files = {"database":injdb}
				)
				sqlitenode = inspiral_pipe.generic_node(lalappsRunSqliteJob, dag, parent_nodes = [sqlitenode],
					opts = {"sql-file":options.injection_sql_file, "tmp-space":inspiral_pipe.condor_scratch_space()},
					input_files = {"":injdb}
				)
				outnodes.setdefault(sim_tag_from_inj_file(inj), []).append(sqlitenode)

	return rankpdf_nodes, outnodes


def finalize_runs(dag, lalappsRunSqliteJob, toXMLJob, ligolwInspinjFindJob, toSqliteJob, innodes, options, instruments):
	"""
	assign FARs, use gstlal_cohfar_merge_stats/bankbox1_SPIIR_stats-start-end.xml.gz for gstlal_cluster_zerolags_online/bankbox1_SPIIR_zerolag-start-end.xml.gz
	apply injFind to gstlal_cluster_zerolags_inj/bankbox1_SPIIR_zerolags-start-end.xml.gz
	"""

	if options.vetoes is None:
		vetoes = []
	else:
		vetoes = [options.vetoes]

	chunk_nodes = []
	# Process the chirp mass bins in chunks to paralellize the merging process
	for chunk, dbs in enumerate(chunks([node.input_files[""] for node in innodes[None]], 20)):
		# Merge the final non injection database into chunks
		noninjdb = inspiral_pipe.T050017_filename(instruments, 'ALL_LLOID_CHUNK_%d' % chunk, int(boundary_seg[0]), int(boundary_seg[1]), '.sqlite')
		sqlitenode = inspiral_pipe.generic_node(toSqliteJob, dag, parent_nodes = innodes[None],
			opts = {"replace":"", "tmp-space":inspiral_pipe.condor_scratch_space()},
			input_files = {"": dbs},
			output_files = {"database":noninjdb}
		)

		# cluster the final non injection database
		noninjsqlitenode = inspiral_pipe.generic_node(lalappsRunSqliteJob, dag, parent_nodes = [sqlitenode],
			opts = {"sql-file":options.cluster_sql_file, "tmp-space":inspiral_pipe.condor_scratch_space()},
			input_files = {"":noninjdb}
		)
		chunk_nodes.append(noninjsqlitenode)

	# Merge the final non injection database
	noninjdb = inspiral_pipe.T050017_filename(instruments, 'ALL_LLOID', int(boundary_seg[0]), int(boundary_seg[1]), '.sqlite')
	sqlitenode = inspiral_pipe.generic_node(toSqliteJob, dag, parent_nodes = chunk_nodes,
		opts = {"replace":"", "tmp-space":inspiral_pipe.condor_scratch_space()},
		input_files = {"": ([node.input_files[""] for node in chunk_nodes] + vetoes + [options.frame_segments_file])},
		output_files = {"database":noninjdb}
	)

	# cluster the final non injection database
	noninjsqlitenode = inspiral_pipe.generic_node(lalappsRunSqliteJob, dag, parent_nodes = [sqlitenode],
		opts = {"sql-file":options.cluster_sql_file, "tmp-space":inspiral_pipe.condor_scratch_space()},
		input_files = {"":noninjdb}
	)

	injdbs = []
	outnodes = [noninjsqlitenode]

	for injections, far_injections in zip(options.injections, options.far_injections):

		# extract only the nodes that were used for injections
		thisinjnodes = innodes[sim_tag_from_inj_file(injections)]
		chunk_nodes = []

		for chunk, dbs in enumerate(chunks([node.input_files[""] for node in thisinjnodes], 20)):

			# Setup the final output names, etc.
			injdb = inspiral_pipe.T050017_filename(instruments, 'ALL_LLOID_CHUNK_%d_%s' % (chunk, sim_tag_from_inj_file(injections)), int(boundary_seg[0]), int(boundary_seg[1]), '.sqlite')


			# merge
			sqlitenode = inspiral_pipe.generic_node(toSqliteJob, dag, parent_nodes = thisinjnodes,
				opts = {"replace":"", "tmp-space":inspiral_pipe.condor_scratch_space()},
				input_files = {"":dbs},
				output_files = {"database":injdb}
			)

			# cluster
			clusternode = inspiral_pipe.generic_node(lalappsRunSqliteJob, dag, parent_nodes = [sqlitenode],
				opts = {"sql-file":options.cluster_sql_file, "tmp-space":inspiral_pipe.condor_scratch_space()},
				input_files = {"":injdb}
			)
			chunk_nodes.append(clusternode)


		# Setup the final output names, etc.
		injdb = inspiral_pipe.T050017_filename(instruments, 'ALL_LLOID_%s' % sim_tag_from_inj_file(injections), int(boundary_seg[0]), int(boundary_seg[1]), '.sqlite')
		injdbs.append(injdb)
		injxml = os.path.splitext(injdb)[0] + ".xml.gz"

		# If there are injections that are too far away to be seen in a separate file, add them now. 
		if far_injections is not None:
			xml_input = [injxml] + [far_injections]
		else:
			xml_input = injxml

		# merge
		sqlitenode = inspiral_pipe.generic_node(toSqliteJob, dag, parent_nodes = chunk_nodes,
			opts = {"replace":"", "tmp-space":inspiral_pipe.condor_scratch_space()},
			input_files = {"": ([node.input_files[""] for node in chunk_nodes] + vetoes + [options.frame_segments_file, injections])},
			output_files = {"database":injdb}
		)

		# cluster
		clusternode = inspiral_pipe.generic_node(lalappsRunSqliteJob, dag, parent_nodes = [sqlitenode],
			opts = {"sql-file":options.cluster_sql_file, "tmp-space":inspiral_pipe.condor_scratch_space()},
			input_files = {"":injdb}
		)


		clusternode = inspiral_pipe.generic_node(toXMLJob, dag, parent_nodes = [clusternode],
			opts = {"tmp-space":inspiral_pipe.condor_scratch_space()},
			output_files = {"extract":injxml},
			input_files = {"database":injdb}
		)

		inspinjnode = inspiral_pipe.generic_node(ligolwInspinjFindJob, dag, parent_nodes = [clusternode],
			opts = {"time-window":0.9},
			input_files = {"":injxml}
		)

		sqlitenode = inspiral_pipe.generic_node(toSqliteJob, dag, parent_nodes = [inspinjnode],
			opts = {"replace":"", "tmp-space":inspiral_pipe.condor_scratch_space()},
			output_files = {"database":injdb},
			input_files = {"":xml_input}
		)
			
		outnodes.append(sqlitenode)

	return injdbs, noninjdb, outnodes

def parse_command_line():
	parser = OptionParser(description = __doc__)

	# generic data source options
	datasource.append_options(parser)
	parser.add_option("--psd-fft-length", metavar = "s", default = 16, type = "int", help = "FFT length, default 16s")

	# reference_psd
	parser.add_option("--reference-psd", help = "Don't measure PSDs, use this one instead")
	
	# SPIIR bank construction options
	parser.add_option("--flow", metavar = "Hz", type = "float", default = 40.0, help = "Set the template low-frequency cut-off (default = 40.0).")
	parser.add_option("--sampleRate", metavar = "Hz", type = "float", default = 4096.0, help = "Set the sample rate of the IIR template bank (optional).")
	parser.add_option("--padding", metavar = "pad", type = "float", default = 1.3, help = "Fractional amount to pad time slices.")
	parser.add_option("--autocorrelation-length", metavar = "len", type = "float", default = 201, help = "Autocorrelation length for chisq.")
	parser.add_option("--epsilon", metavar = "pad", type = "float", default = 0.02, help = "Second order correction factor.")
	parser.add_option("--req-min-match", metavar = "match", type = "float", default = 0.99, help = "Set the SPIIR approximation minimal match (default = 0.99).")
	#FIXME figure out how to encode instrument info

	parser.add_option("--downsample", action = "store_true", help = "Choose if you want to downsample IIR bank (recommended)")
	parser.add_option("--bank-cache", metavar = "filenames", help = "Set the bank cache files in format H1=H1.cache,H2=H2.cache, etc..")
	
	# trigger generation options
	parser.add_option("--vetoes", metavar = "filename", help = "Set the veto xml file.")
	parser.add_option("--web-dir", metavar = "directory", help = "Set the web directory like /home/USER/public_html")
	parser.add_option("--num-banks", metavar = "str", default = 8, help = "The number of parallel subbanks per gstlal_inspiral job. can be given as a list like 1,2,3,4 then it will split up the bank cache into N groups with M banks each.")
	parser.add_option("--max-inspiral-jobs", type="int", metavar = "jobs", help = "Set the maximum number of gstlal_inspiral jobs to run simultaneously, default no constraint.")
	parser.add_option("--ht-gate-threshold", type="float", help="set a threshold on whitened h(t) to veto glitches")
	parser.add_option("--inspiral-executable", default = "gstlal_inspiral_postcohspiir_online", help = "default gstlal_inspiral_postcohspiir_online")
	parser.add_option("--blind-injections", metavar = "filename", help = "Set the name of an injection file that will be added to the data without saving the sim_inspiral table or otherwise processing the data differently.  Has the effect of having hidden signals in the input data. Separate injection runs using the --injections option will still occur.")
	#parser.add_option("--far-injections", action = "append", help = "Injection files with injections too far away to be seen and are not filtered. Required. See https://www.lsc-group.phys.uwm.edu/ligovirgo/cbcnote/NSBH/MdcInjections/MDC1 for example.")
	parser.add_option("--verbose", action = "store_true", help = "Be verbose")

	parser.add_option("--cuda-postcoh-detrsp-fname", metavar = "filename", help = "detector response filename.")
	parser.add_option("--cuda-postcoh-hist-trials", metavar = "hist-trials", type = "int", default = 100, help = "histogram trials for background distribution.")
	parser.add_option("--cuda-postcoh-output-skymap", metavar = "output-skymap", type = "int", default = 0, help = "if output skymap, 1: yes, 0: no")


	# Override the datasource injection option
	parser.remove_option("--injections")
	parser.add_option("--injections", action = "append", default = [], help = "append injection files to analyze")

	# Condor commands
	parser.add_option("--condor-command", action = "append", default = [], metavar = "command=value", help = "set condor commands of the form command=value; can be given multiple times")

	# bankbox commands
	parser.add_option("--bankbox-list", default = None, metavar = "nbin1:nbin2xxx", help = "set the banks to be merged")

	
	options, filenames = parser.parse_args()
	options.num_banks = [int(v) for v in options.num_banks.split(",")]
	

	fail = ""
	for option in ("bank_cache",):
		if getattr(options, option) is None:
			fail += "must provide option %s\n" % (option)
	if fail: raise ValueError, fail

	return options, filenames


#
# Useful variables
#

options, filenames = parse_command_line()
bank_cache = inspiral_pipe.parse_cache_str(options.bank_cache)
detectors = datasource.GWDataSourceInfo(options)
channel_dict = detectors.channel_dict
instruments = "".join(sorted(bank_cache.keys()))
instrument_set = bank_cache.keys()
boundary_seg = detectors.seg

output_dir = "plots"

#
# Setup the dag
#

try:
	os.mkdir("logs")
except:
	pass
dag = inspiral_pipe.DAG("trigger_pipe")

if options.max_inspiral_jobs is not None:
	dag.add_maxjobs_category("INSPIRAL", options.max_inspiral_jobs)

#
# Make an xml integrity checker
#

f = open("gzip_test.sh", "w")
f.write("#!/bin/bash\nsleep 60\ngzip --test $@")
f.close()
os.chmod("gzip_test.sh", stat.S_IRUSR | stat.S_IXUSR | stat.S_IRGRP | stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH | stat.S_IWUSR)

#
# A pre script to backup data before feeding to lossy programs
# (e.g. clustering routines)
#

f = open("store_raw.sh", "w")
f.write("""#!/bin/bash
for f in $@;do mkdir -p $(dirname $f)/raw;cp $f $(dirname $f)/raw/$(basename $f);done""")
f.close()
os.chmod("store_raw.sh", stat.S_IRUSR | stat.S_IXUSR | stat.S_IRGRP | stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH | stat.S_IWUSR)

#
# setup the job classes
#

# for PSD generation
refPSDJob = inspiral_pipe.generic_job("gstlal_reference_psd", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":"2GB", "request_cpus":"2"}))

medianPSDJob = inspiral_pipe.generic_job("gstlal_median_of_psds", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":"2GB"}))

# for spiir bank generation
bankJob = inspiral_pipe.generic_job("gstlal_iir_bank", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":"16GB"}))

# horizon information
horizonJob = inspiral_pipe.generic_job("gstlal_plot_psd_horizon", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":"2GB"}))

# filtering and coherent search
# non-injection run
# FIXME: hard-code the request cpus and memory, so each node can only process 1 job
inspiralPostcohspiirJob = inspiral_pipe.generic_job(options.inspiral_executable, condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_cpus":"TARGET.cpus", "request_memory":"TARGET.memory", "Requirements":"(TARGET.Online_CBC_IIR_GPU=?=True)", "+Online_CBC_IIR_GPU":"True", "environment":"CUDA_VISIBLE_DEVICES=1,2,3,4"}))

# shifted-data run
# FIXME: hard-code the request cpus and memory, so each node can only process 1 job
inspiralPostcohspiirShiftJob = inspiral_pipe.generic_job(options.inspiral_executable, tag_base="gstlal_inspiral_postcohspiir_shift", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_cpus":"TARGET.cpus", "request_memory":"TARGET.memory", "Requirements":"(TARGET.Online_CBC_IIR_GPU=?=True)", "+Online_CBC_IIR_GPU":"True", "environment":"CUDA_VISIBLE_DEVICES=1,2,3,4"}))


#injection run, 
# FIXME: hard-code the request cpus and memory, so each node can only process 1 job
inspiralPostcohspiirInjJob = inspiral_pipe.generic_job(options.inspiral_executable, tag_base="gstlal_inspiral_postcohspiir_inj", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_cpus":"TARGET.cpus", "request_memory":"TARGET.memory", "Requirements":"(TARGET.Online_CBC_IIR_GPU=?=True)", "+Online_CBC_IIR_GPU":"True", "environment":"CUDA_VISIBLE_DEVICES=1,2,3,4"}))

# calculate the PDF of the rankings statistic
mergestatsJob = inspiral_pipe.generic_job("gstlal_cohfar_calc_fap", tag_base="gstlal_cohfar_merge_stats", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":"2GB", "request_cpus":"1"}))

# find the injections
#ligolwInspinjFindJob = inspiral_pipe.generic_job("ligolw_inspinjfind", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":"2GB"}))

# from xml to sqlite
#toSqliteJob = inspiral_pipe.generic_job("ligolw_sqlite", tag_base = "ligolw_sqlite_from_xml", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":"2GB"}))

# to xml
#toXMLJob = inspiral_pipe.generic_job("ligolw_sqlite", tag_base = "ligolw_sqlite_to_xml", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":"2GB"}))

# execute sql script on a sql database
#lalappsRunSqliteJob = inspiral_pipe.generic_job("lalapps_run_sqlite", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":"2GB"}))

# plot summary
#plotSummaryJob = inspiral_pipe.generic_job("gstlal_inspiral_plotsummary", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":"2GB"}))

# plot individual injection summary
#plotIndividualInjectionsSummaryJob = inspiral_pipe.generic_job("gstlal_inspiral_plotsummary", tag_base = "gstlal_inspiral_plotsummary_inj", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":"2GB"}))

# plot sensitivity
#plotSensitivityJob = inspiral_pipe.generic_job("gstlal_inspiral_plot_sensitivity", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":"2GB"}))

# openpage
#openpageJob = inspiral_pipe.generic_job("gstlal_inspiral_summary_page", tag_base = 'gstlal_inspiral_summary_page_open', condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":"2GB"}))
#pageJob = inspiral_pipe.generic_job("gstlal_inspiral_summary_page", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":"2GB"}))
# closed page
#marginalizeJob = inspiral_pipe.generic_job("gstlal_inspiral_marginalize_likelihood", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":"2GB"}))
# plot background
#plotbackgroundJob = inspiral_pipe.generic_job("gstlal_inspiral_plot_background", condor_commands = inspiral_pipe.condor_command_dict_from_opts(options.condor_command, {"request_memory":"2GB"}))

#
#
# Get the analysis segments
#

segsdict = gen_analysis_segments(set(bank_cache.keys()), detectors.frame_segments, boundary_seg, get_max_length_from_bank(bank_cache))

if options.reference_psd is None:

	#
	# Compute the PSDs for each segment
	#


	psd_nodes = gen_psd_node(refPSDJob, dag, [], segsdict, channel_dict, options)

	#
	# plot the horizon distance
	#

	inspiral_pipe.generic_node(horizonJob, dag,
		parent_nodes = psd_nodes.values(),
		input_files = {"":[node.output_files["write-psd"] for node in psd_nodes.values()]},
		output_files = {"":inspiral_pipe.T050017_filename(instruments, "HORIZON", boundary_seg[0].seconds, boundary_seg[1].seconds, '.png', path = output_dir)}
	)

	#
	# compute the median PSD
	#

	median_psd_node = \
		inspiral_pipe.generic_node(medianPSDJob, dag,
			parent_nodes = psd_nodes.values(),
			input_files = {"":[node.output_files["write-psd"] for node in psd_nodes.values()]},
			output_files = {"output-name": inspiral_pipe.T050017_filename(instruments, "REFERENCE_PSD", boundary_seg[0].seconds, boundary_seg[1].seconds, '.xml.gz', path = medianPSDJob.output_path)}
		)

	ref_psd = median_psd_node.output_files["output-name"]
	ref_psd_parent_nodes = [median_psd_node]

else:
	ref_psd = lalseries.read_psd_xmldoc(ligolw_utils.load_filename(options.reference_psd, verbose = options.verbose, contenthandler = LIGOLWContentHandler)) 

	# NOTE: compute just the SNR pdf cache here, set other features to 0
	# NOTE: This will likely result in downstream codes needing to compute
	# more SNR PDFS, since in this codepath only an average spectrum is
	# used.
	ref_psd_parent_nodes = []

#
# Compute SPIIR banks
#

bank_nodes = gen_bank_node(bankJob, dag, ref_psd_parent_nodes, ref_psd, inspiral_pipe.build_bank_groups(bank_cache, options.num_banks), options, boundary_seg)


#	
# Inspiral jobs by segment
#

"""
inspiral_nodes[(ifos, None)] = {"seg0": [noninjnode1, noninjnode2,...]}
inspiral_nodes[(ifos, "shift")] = {"seg0": [shifnode1, shiftnode2,...]}
inspiral_nodes[(ifos, "inj_name")] = {"seg0": [injnode1, injnode2,...]}
spiir_zlag[None] = {"bankbin0": [noninjnode1, noninjnode2,...]}
spiir_zlag["shift"] = {"bankbin0": [shifnode1, shiftnode2,...]}
spiir_zlag["inj_name"] = {"banbin0": [injnode1, injnode2,...]}
"""
inspiral_nodes, spiir_zlag, spiir_stats = gen_inspiral_node(inspiralPostcohspiirJob, inspiralPostcohspiirShiftJob, inspiralPostcohspiirInjJob, dag, bank_nodes, segsdict, options, channel_dict)

#
# Adapt the output of the gstlal_inspiral jobs to be suitable for the remainder of this analysis
if options.bankbox_list is None:
	bankbox_list = [len(spiir_zlag[None].keys())]
else:
	bankbox_list = options.bankbox_list.split(":")

merge_stats(dag, mergestatsJob, spiir_stats, instrument_set, boundary_seg, bankbox_list)


# make summary plots
#plotnodes = []

#plotnodes.append(inspiral_pipe.generic_node(plotSummaryJob, dag, parent_nodes=[farnode],
#	opts = {"segments-name": options.frame_segments_name, "tmp-space": inspiral_pipe.condor_scratch_space(), "user-tag": "ALL_LLOID_COMBINED", "output-dir": output_dir},
#	input_files = {"":[noninjdb] + injdbs}
#))


#
#
# all done
#

dag.write_sub_files()
dag.write_dag()
dag.write_script()
dag.write_cache()
