#!/usr/bin/env python
#
# Copyright (C) 2011 Chris Pankow
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
"""Stream-based burst analysis tool"""

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


import os
import sys
import time
import signal
import glob
import tempfile
import threading

from optparse import OptionParser
import ConfigParser
from ConfigParser import SafeConfigParser

import numpy

import pygtk
pygtk.require("2.0")
import gobject
gobject.threads_init()
import pygst
pygst.require("0.10")

# This mess is to make gstreamer stop eating our help messages.
if "--help" in sys.argv or "-h" in sys.argv:
	try:
		del sys.argv[ sys.argv.index( "--help" ) ]
	except ValueError:	
		pass
	try:
		del sys.argv[ sys.argv.index( "-h" ) ]
	except ValueError:	
		pass
	
	import gst
	sys.argv.append( "--help" )
else:
	import gst

from gstlal import pipeparts
from gstlal.reference_psd import write_psd, read_psd_xmldoc

import gstlal.excesspower as ep
from gstlal.epparts import EPHandler
from gstlal import datasource

from glue import GWDataFindClient

from glue.ligolw import ligolw
from glue.ligolw import array
from glue.ligolw import param
from glue.ligolw import lsctables
from glue.ligolw import table
array.use_in(ligolw.LIGOLWContentHandler)
param.use_in(ligolw.LIGOLWContentHandler)
lsctables.use_in(ligolw.LIGOLWContentHandler)
from glue.ligolw import utils

from glue.segments import segment, segmentlist, segmentlistdict, PosInfinity
from glue import segmentsUtils
from glue import gpstime
from glue.lal import LIGOTimeGPS, Cache, CacheEntry

from pylal.xlal.datatypes.snglburst import from_buffer as sngl_bursts_from_buffer
from pylal.xlal.datatypes.real8frequencyseries import REAL8FrequencySeries
from pylal.xlal.lalburst import XLALlnOneMinusChisqCdf

#
# =============================================================================
#
#                        Message Handler Methods
#
# =============================================================================
#

# These are linked later in the pipeline to do the appropriate actions when signals are sent up.

def on_psd_change( elem, pspec, hand ):
	"""
	Get the PSD object and signal the handler to rebuild everything.
	"""
	if options.verbose:
		print >> sys.stderr, "Intercepted spectrum signal."

	hand.psd = REAL8FrequencySeries(
		name = "PSD",
		#epoch = laltypes.LIGOTimeGPS(0, message.structure["timestamp"]),
		f0 = 0.0,
		deltaF = elem.get_property( "delta-f" ),
		#sampleUnits = laltypes.LALUnit(message.structure["sample-units"].strip()),
		data = numpy.array( elem.get_property( "mean-psd" ) )
	)


	# Determine if the PSD has changed enough to warrant rebuilding the filter
	# bank.
	psd_power = sum(hand.psd.data)*hand.psd.deltaF
	#change = abs((hand.psd_power - psd_power) / (hand.psd_power + psd_power) )
	hand.psd_change = (hand.psd_power - psd_power) / (hand.psd_power + psd_power) 
	if abs(hand.psd_change) > hand.psd_change_thresh:
		if options.verbose:
			print >> sys.stderr, "Processed signal. PSD change %d, regenerating filters" % int(hand.psd_change*100)
		hand.psd_power = psd_power
		hand.rebuild_everything()


def on_spec_corr_change( elem, pspec, hand ):
	"""
	Get the 2-point spectral correlation object and signal the handler to rebuild everything.
	"""
	if options.verbose:
		print >> sys.stderr, "Intercepted correlation signal."
	hand.spec_corr = elem.get_property( "spectral-correlation" )

	if hand.cache_spec_corr:
		f = open( "spec_corr.dat", "w" )
		k_end = len( hand.spec_corr ) / 2
		for k, sp in enumerate( hand.spec_corr ):
			f.write( "%d %g\n" % (k, sp) )
	
	# If the spectrum correlation changes, rebuild everything
	if hand.psd is not None:
		hand.rebuild_everything()

def get_triggers(elem, handler, ndof):

	buffer = elem.emit("pull-buffer")

	if not handler.output:
		return # We don't want event information

	# TODO: Can I set units here on the buffer fields, avoid changing the 
	# triggers themslves and *not* screw up the underlying framework?

	# NOTE: This is here and not in the handler, since self.start may not be 
	# well defined before the pipeline starts, and thus shouldn't be checked 
	# until after we start running. We must know handler.start by the time we 
	# start drawing triggers, though
	whiten_seg = segment( 
		LIGOTimeGPS(handler.start), 
		LIGOTimeGPS(handler.start + handler.whitener_offset)
	)

	trigs = lsctables.New(lsctables.SnglBurstTable,
			["ifo", "peak_time", "peak_time_ns", "start_time", "start_time_ns",
			"duration",  "search", "event_id", "process_id",
			"central_freq", "channel", "amplitude", "snr", "confidence",
			"chisq", "chisq_dof", "bandwidth"])
	for row in sngl_bursts_from_buffer(buffer):
		if row.peak_time in whiten_seg:
			continue
		row.duration *= ndof
		row.chisq_dof *= ndof
		# TODO Move this into the trigger generator
		# FIXME: Determine "magic number" or remove it
		row.confidence = -XLALlnOneMinusChisqCdf(row.snr * 0.62, ndof * 0.62)
		if options.compat:
			row.snr = row.snr / ndof - 1

		# This is done here so that the current PSD is used rather than
		# what might be there when the triggers are actually output
		ep.compute_amplitude( row, handler.psd )

		trigs.append( row )
	
	#buf_ts = (buffer.timestamp*1e-9 - handler.start) / handler.units + handler.start
	# TODO: Why does the buf_dur need unit conversion, but not the timestamp
	buf_ts = buffer.timestamp*1e-9 
	buf_dur = buffer.duration*1e-9 / handler.units
	handler.stop = (buf_ts + buf_dur)

	# Check if clustering reduces the amount of events
	if len(handler.triggers) >= handler.max_events and handler.clustering:
		handler.process_triggers( trigs, cluster_passes=1 )
	else:
		handler.process_triggers( trigs )

	if handler.stop - handler.time_since_dump > handler.dump_frequency or len(handler.triggers) >= handler.max_events:

		fname = ep.make_cache_parseable_name(
			inst = handler.inst,
			tag = handler.channel,
			start = handler.time_since_dump,
			stop = handler.stop,
			ext = "xml",
			dir = handler.outdir
		)

		# It's possible that triggers come out so fast that the filename would
		# be identical and overwrite those triggers. This appends an integer
		# identified to prevent thtat.
		uniq = 1
		while os.path.exists( fname ):
			fname = ep.make_cache_parseable_name(
				inst = handler.inst,
				tag = "%s_%d" % (handler.channel, uniq),
				start = handler.time_since_dump,
				stop = handler.stop,
				ext = "xml",
				dir = handler.outdir
			)
			uniq += 1

		handler.process_triggers( [], cluster_passes = True )
		handler.write_triggers( filename = fname, flush = True )
		handler.time_since_dump = handler.stop 

#
# =============================================================================
#
#                             Options Handling
#
# =============================================================================
#

parser = OptionParser()

datasource.append_options( parser )

parser.add_option("-f", "--initialization-file", dest="infile", help="Options to be pased to the pipeline handler. Strongly recommended.", default=None)
parser.add_option("-d", "--diagnostics", dest="diagnostics", action="store_true", help="Turn on multiple diagnostic dumps. Use with caution, as it will dump gigabytes of data (potentially) in a matter of minutes. Useful in nongraphical environemnts to monitor data throughput.", default=False)
parser.add_option("-v", "--verbose", dest="verbose", action="store_true", help="Be verbose.", default=False)
parser.add_option("-r", "--sample-rate", dest="sample_rate", action="store", type="int", help="Sample rate of the incoming data.")
parser.add_option("-S", "--stream-tfmap", dest="stream_tfmap", action="store", help="Encode the time frequency map to video as it is analyzed. If the argument to this option is \"video\" then the pipeline will attempt to stream to a video source. If the option is instead a filename, the video will be sent to that name. Prepending \"keyframe=\" to the filename will start a new video every time a keyframe is hit.")
parser.add_option("-t", "--disable-triggers", dest="disable_triggers", action="store_true", help="Don't record triggers.", default=False)
parser.add_option("-T", "--file-cache", dest="file_cache", action="store_true", help="Create file caches of output files. If no corresponding --file-cache-name option is provided, an approrpriate name is constructed by default, and will only be created at the successful conclusion of the pipeline run.", default=False)
parser.add_option("-F", "--file-cache-name", dest="file_cache_name", action="store", help="Name of the trigger file cache to be written to. If this option is specified, then when each trigger file is written, this file will be written to with the corresponding cache entry. See --file-cache for other details.", default=None)
parser.add_option("-c", "--lalapps-power-compatibility", dest="compat", action="store_true", default=False, help="Output trigger information which conforms to the lalapps_power conventions.")
parser.add_option("-C", "--clustering", dest="clustering", action="store_true", default=False, help="Employ trigger tile clustering before output stage. Default or if not specificed is off." )
parser.add_option("-u", "--enable-db-uploads", dest="db_uploads", action="store_true", default=False, help="Upload uploads to trigger archiving services (e.g. gracedb). The threshold for upload should be in the initialization file. Default or if not specificed is off." )
parser.add_option("-m", "--enable-channel-monitoring", dest="channel_monitoring", action="store_true", default=False, help="Emable monitoring of channel statistics like even rate/signifiance and PSD power" )

(options, args) = parser.parse_args()

if options.data_source == "frames" and options.frame_cache is None:
	if options.gps_start_time is None or options.gps_end_time is None:
		sys.exit( "No frame cache present, and no GPS times set. Cannot query for data without an interval to query in." )

	# Shamelessly stolen from gw_data_find
	print "Querying LDR server for data location." 
	try:
		server, port = os.environ["LIGO_DATAFIND_SERVER"].split(":")
	except ValueError:
		sys.exit( "Invalid LIGO_DATAFIND_SERVER environment variable set" )
	print "Server is %s:%s" % (server, port)

	try:
		frame_type = cfg.get( "instrument", "frame_type" )
	except ConfigParser.NoOptionError:
		sys.exit( "Invalid cache location, and no frame type set, so I can't query LDR for the file locations." )
	if frame_type == "":
		sys.exit( "No frame type set, aborting." )

	print "Frame type is %s" % frame_type
	connection =  \
		GWDataFindClient.GWDataFindHTTPConnection(host=server, port=port)
	print "Equivalent command line is "
	print "gw_data_find -o %s -s %d -e %d -u file -t %s" %(handler.inst[0], options.gps_start_time, options.gps_end_time, frame_type)
	cache = connection.find_frame_urls( handler.inst[0], 
		frame_type,
		options.gps_start_time,
		options.gps_end_time,
		urltype="file",
		on_gaps="error"
	)

	tmpfile, tmpname = tempfile.mkstemp()
	print "Writing cache of %d files to %s" % (len(cache), tmpname)
	cache.tofile( open(tmpname, "w") )
	connection.close()
	options.frame_cache = tmpname

gw_data_source_opts = datasource.GWDataSourceInfo( options )

# Verbosity and diagnostics
verbose = options.verbose
diagnostics = options.diagnostics

#
# =============================================================================
#
#                           Handler / Pipeline options
#
# =============================================================================
#

# We need a pipeline and pipeline handler instance to configure
pipeline = gst.Pipeline( "gstlal_excesspower" )
mainloop = gobject.MainLoop()
handler = EPHandler(mainloop, pipeline)

if options.channel_monitoring:
	handler.channel_monitoring = True

# If a sample rate other than the native rate is requested, we'll need to keep
# track of it
if options.sample_rate is not None:
	handler.rate = options.sample_rate

# Does the user want a cache file to track the trigger files we spit out?
if not options.file_cache:
	handler.output_cache = None

# And if so, if you give us a name, we'll update it every time we output, else
# only at the end of the run
if options.file_cache and options.file_cache_name is not None:
	handler.output_cache_name = options.file_cache_name

# Clustering on/off
handler.clustering = options.clustering
# Be verbose?
handler.verbose = options.verbose

if not options.infile:
	print >>sys.stderr, "Initialization file required."
elif not os.path.exists( options.infile ):
	print >>sys.stderr, "Initialization file path is invalid."
	sys.exit(-1)

cfg = SafeConfigParser()
cfg.read( options.infile )

# Instruments and channels
if len(gw_data_source_opts.channel_dict.keys()) == 1:
	handler.inst = gw_data_source_opts.channel_dict.keys()[0]
elif cfg.has_option( "instrument", "detector" ):
	handler.inst = cfg.get( "instrument", "detector" )
else:
	sys.exit( "Unable to determine instrument." )

if gw_data_source_opts.channel_dict[handler.inst] is not None:
	handler.channel = gw_data_source_opts.channel_dict[handler.inst]
elif cfg.has_option( "instrument", "channel" ):
	handler.channel = cfg.get( "instrument", "channel" )
else:
	# TODO: In the future, we may request multiple channels for the same 
	# instrument -- e.g. from a single raw frame
	sys.exit( "Unable to determine channel." )
print "Channel name(s): " + handler.channel 

# TODO: Is this still needed?
if cfg.has_option( "instrument", "site" ):
	site = cfg.get( "instrument", "site" )
else:
	print >>sys.stderr, "No site requested, using detector as default cache sieve."
	site = None

# FFT and time-frequency parameters
handler.flow = cfg.getfloat( "tf_parameters", "min-frequency" )
handler.fhigh = cfg.getfloat( "tf_parameters", "max-frequency" )
handler.base_band = cfg.getfloat( "tf_parameters", "base-resolution" )
handler.max_level = cfg.getint( "tf_parameters", "max-resolution-level" )
handler.max_duration = cfg.getfloat( "tf_parameters", "max-time-resolution" )

#base_band = handler.base_band

if cfg.has_option( "tf_parameters", "fft-length" ):
	handler.fft_length = cfg.getfloat( "tf_parameters", "fft-length" )

if cfg.has_option( "cache", "cache-psd-every" ):
	handler.cache_psd = cfg.getint( "cache", "cache-psd-every" )
	print "PSD caching enabled. PSD will be recorded every %d seconds" % handler.cache_psd
else:
	handler.cache_psd = None

if cfg.has_option( "cache", "cache-psd-dir" ):
	handler.cache_psd_dir = cfg.get( "cache", "cache-psd-dir" )
	print "Caching PSD to %s" % handler.cache_psd_dir
	
# Used to keep track if we need to lock the PSD into the whitener
psdfile = None
if cfg.has_option( "cache", "reference-psd" ):
	psdfile = cfg.get( "cache", "reference-psd" )
	try:
		handler.psd = read_psd_xmldoc( utils.load_filename( psdfile, contenthandler = ligolw.LIGOLWContentHandler ) )[ handler.inst ]
		print "Reference PSD for instrument %s from file %s loaded" % ( handler.inst, psdfile )
		# Reference PSD disables caching (since we already have it
		handler.cache_psd = None
	except KeyError: # Make sure we have a PSD for this instrument
		sys.exit( "PSD for instrument %s requested, but not found in file %s. Available instruments are %s" % (handler.inst, psdfile, str(handler.psd.keys())) )

# TODO: Fix this up too
if cfg.has_option( "cache", "cache-spectral-correlation" ):
	handler.cache_spec_corr = cfg.getboolean( "cache", "cache-spectral-correlation" )

# Triggering options
if cfg.has_option( "triggering", "output-file" ):
	handler.outfile = cfg.get( "triggering", "output-file" )
if cfg.has_option( "triggering", "output-directory" ):
	handler.outdir = cfg.get( "triggering", "output-directory" )

handler.snr_thresh = cfg.getfloat( "triggering", "snr-thresh" )
if options.db_uploads:

	handler.db_thresh = cfg.getfloat( "triggering", "db-thresh" )
	if handler.db_thresh is None:
		print >>sys.stderr, "Warning, DB upload requested, but no threshold provided. Disabling."

	handler.db_client = cfg.get( "triggering", "db-client" )
	if handler.db_thresh is None:
		print >>sys.stderr, "Warning, DB upload requested, but no DB path provided. Disablinhg"
		handler.db_thresh = None

if cfg.has_option( "triggering", "events_per_file" ):
	handler.max_events = cfg.get_int( "triggering", "events_per_file" )

# If a specific (trigger) time is of interest, specify its GPS here
# TODO: Read from sngl_inspirals and sngl_bursts
# TODO: Make this useful
trigger_begin, trigger_end = None, None
if cfg.has_option( "triggering", "trig_time_start" ):
	trigger_begin = cfg.getfloat( "triggering", "trig_time_start" )
if cfg.has_option( "triggering", "trig_time_end" ):
	trigger_end = cfg.getfloat( "triggering", "trig_time_end" )
if trigger_begin and trigger_end:
	handler.set_trigger_time_and_action( segment( trigger_begin, trigger_end ) )

# This is invoked here, or else the default rate is used, which will cause funny behavior for the defaults with some cases
df = 1.0 / handler.fft_length
handler.filter_len = 2*int(2*handler.base_band / df )
handler.build_default_psd( handler.rate, df, handler.fhigh )
handler.rebuild_filter()
handler.rebuild_chan_mix_matrix()

# Set process params in handler for use with the output xmldocs
handler.make_process_tables( options, None )

# Max trigger duration (s)
# TODO: Uhh... what was going on here?
#handler.max_duration

#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#

if options.verbose:
	print >>sys.stderr, "Assembling pipeline...\n",

# TODO: He looks lonely... move him somewhere useful
duration = -1

head = datasource.mkbasicsrc( pipeline, 
	gw_data_source_opts, 
	handler.inst, 
	verbose 
)

# Drop first PSD estimates until it settles
if psdfile is not None:
	head = pipeparts.mkdrop( pipeline, head, handler.fft_length*8*handler.rate )

if gw_data_source_opts.data_source == "lvshm":
	# FIXME: This is a guess. Not a terrible one, but still inaccurate
	handler.start = gpstime.GpsSecondsFromPyUTC( time.time() )
	handler.time_since_dump = handler.start

	# FIXME: Use data source callbacks
	gate = pipeline.get_by_name( "%s_state_vector_gate" % handler.inst )
	gate.set_property( "emit-signals", True )
	gate.connect("start", handler.handle_segment, "on" )
	gate.connect("stop", handler.handle_segment, "off" )

"""
# Diagnostic plot
if( diagnostics ):
	head = postdatatee = pipeparts.mktee( pipeline, head )
	pipeparts.mknxydumpsink( pipeline, 
		pipeparts.mkqueue( pipeline, postdatatee ), 
		cfg.get( "diagnostics", "strain-data-output" )
	)
"""

# Convert to 64 bit
#head = pipeparts.mkcapsfilter( pipeline, pipeparts.mkaudioconvert( pipeline, head ), "audio/x-raw-float,width=64" )
# Data conditioning
head = pipeparts.mkcapsfilter( pipeline, pipeparts.mkresample( pipeline, head ), "audio/x-raw-float,rate=%d" % handler.rate )

head = whitener = pipeparts.mkwhiten( pipeline, head )
whitener.set_property( "fft-length", handler.fft_length ) 
whitener.set_property( "expand-gaps", True ) 
if psdfile is not None: # In other words, we have a reference PSD
	whitener.set_property( "mean-psd", handler.psd.data )
	whitener.set_property( "psd-mode", 1 ) # GSTLAL_PSDMODE_FIXED

head = pipeparts.mkqueue( pipeline, head )

# Diagnostic plot
if diagnostics:
	head = postresamptee = pipeparts.mktee( pipeline, head )
	pipeparts.mknxydumpsink( pipeline, 
		pipeparts.mkqueue( pipeline, head ), 
		cfg.get( "diagnostics", "whitened-data-output" )
	)

if verbose:
	head = pipeparts.mkprogressreport( pipeline, head, "whitened stream" )

# excess power channel firbank
# TODO: Do we gain anything by changing the block stride?
head = pipeparts.mkfirbank( pipeline, head, 
	time_domain=False, 
	block_stride=handler.rate 
)

handler.add_firbank( head )
nchannels = handler.filter_bank.shape[0]
print "FIR bank constructed with %d %f Hz channels" % (nchannels, handler.base_band)

if verbose:
	head = pipeparts.mkprogressreport( pipeline, head, "FIR bank stream" )

# TODO: Uncomment here
#head = postfirtee = pipeparts.mkqueue( pipeline, pipeparts.mktee( pipeline, head ) )
#####

if diagnostics:
	pipeparts.mknxydumpsink( pipeline, 
		postfirtee, 
		cfg.get( "diagnostics", "fir-output" )
	)

# First branch -- send fully sampled data to wider channels for processing
nlevels = int(numpy.ceil( numpy.log2( nchannels ) )) 
for res_level in range(0, min(handler.max_level, nlevels)):
	# TODO: Uncomment here
	#head = postfirtee
	#######

	band = handler.base_band * 2**res_level
	# TODO: Check this
	chan = numpy.ceil( nchannels / 2.0**res_level )

	# The undersample_rate for band = R/2 is => sample_rate (passthrough)
	undersamp_rate = 2 * band


	# If the rate which would be set by the undersampler falls below one, we 
	# have to take steps to prevent this, as gstreamer can't handle this. The 
	# solution is to change the "units" of the rate. Ideally, this should be 
	# done much earlier in the pipeline (e.g. as the data comes out of the 
	# source), however, to avoid things like figuring out what that means for 
	# the FIR bank we change units here, and readjust appropriately in the 
	# trigger output.
	if undersamp_rate < 1:
		print "Automatically adjusting units to compensate for undersample rate falling below unity."
		# No, it's not factors of ten, but rates which aren't factors
		# of two are often tricky, thus if the rate is a factor of two, the 
		# units conversion won't change that.
		if undersamp_rate > ep.EXCESSPOWER_UNIT_SCALE['mHz']:
			unit = 'mHz'
		elif undersamp_rate > ep.EXCESSPOWER_UNIT_SCALE['uHz']:
			unit = 'uHz'
		elif undersamp_rate > ep.EXCESSPOWER_UNIT_SCALE['nHz']:
			unit = 'nHz'
		else:
			sys.exit( "Requested undersampling rate would fall below 1 nHz." )
		# FIXME: No love for positive power of 10 units?

		handler.units = ep.EXCESSPOWER_UNIT_SCALE[unit]
		undersamp_rate /= handler.units
		print "Undersampling rate for level %d: %f %s" % (res_level, undersamp_rate, unit)
		head = pipeparts.mkcapssetter( pipeline, head, "audio/x-raw-float,rate=%d" % (handler.rate/handler.units), replace=False )
		head = pipeparts.mkgeneric( pipeline, head, "lal_audioundersample" )
		head = pipeparts.mkcapssetter( pipeline, head, "audio/x-raw-float,rate=%d" % undersamp_rate, replace=False )
	else:
		print "Undersampling rate for level %d: %f Hz" % (res_level, undersamp_rate)
		head = pipeparts.mkgeneric( pipeline, head, "lal_audioundersample" )
		head = pipeparts.mkcapsfilter( pipeline, head, "audio/x-raw-float,rate=%d" % undersamp_rate )

	if diagnostics:
		head = postustee = pipeparts.mktee( pipeline, pipeparts.mkqueue( pipeline, head ) )
		pipeparts.mknxydumpsink( pipeline, postustee, "postundersamp_res_%d.txt" % res_level )

	if verbose:
		head = pipeparts.mkprogressreport( pipeline, head, 
			"Undersampled stream level %d" % res_level
		)

	head = matmixer = pipeparts.mkmatrixmixer( pipeline, head )
	handler.add_matmixer( matmixer, res_level )

	if verbose:
		head = pipeparts.mkprogressreport( pipeline, head,
			"post matrix mixer %d" % res_level 
		)

	if diagnostics:
		head = postmmtee = pipeparts.mktee( pipeline, pipeparts.mkqueue( pipeline, head ) )
		pipeparts.mknxydumpsink( pipeline, postmmtee, "postmatmix_res_%d.txt" % res_level )

	head = pipeparts.mkgeneric( pipeline, head, "pow" )
	head.set_property( "exponent", 2 )

	if verbose:
		head = pipeparts.mkprogressreport( pipeline, head, 
			"Energy stream level %d" % res_level
		)

	# TODO: Uncomment here
	#head = pipeparts.mkqueue( pipeline, head )
	#####

	ndof = 2 # samples -- min number
	# Second branch -- duration
	# max_samp = int(handler.max_duration*rate)
	#while duration <= max_samp:
		#duration = duration << 1

	# Multi channel FIR filter -- used to add together frequency bands into 
	# tiles
	# FIXME: This is a workaround around until the audiofirfilter is fixed.
	# Basically, if the firfilter gets a small enough kernel, it invokes 
	# time-domain convolution. Something about the way it fills the buffers is
	# wrong in regards to the way it keeps history and thus the output buffers
	# partially desynched from the input -- even with an identity transform.

	# Workaround: Zero pad the kernel past the length (32 samples) in which the 
	# element switches to FFT convolution which doesn't exhibit bad behavior.

	#head = pipeparts.mkchecktimestamps( pipeline, head, "before audiofir" )
	head = pipeparts.mkgeneric( pipeline, head, "audiofirfilter" )
	head.set_property( "kernel", 
		ep.build_fir_sq_adder( ndof, padding=max(0, 33-ndof) )
	)
	head = pipeparts.mknofakedisconts( pipeline, head )
	#head = pipeparts.mkchecktimestamps( pipeline, head, "after audiofir" )

	if diagnostics:
		head = postdurtee = pipeparts.mktee( pipeline, pipeparts.mkqueue( pipeline, head ) )
		pipeparts.mknxydumpsink( pipeline, postdurtee, "postdur_res_%d_%d.txt" % (res_level, ndof) )

	if verbose:
		head = pipeparts.mkprogressreport( pipeline, head, 
			"After energy summation resolution level %d, %d DOF" % 
				(res_level, ndof) 
		)

	# TODO: Audio
	if options.stream_tfmap:
		if len(options.stream_tfmap.split("=")) == 2:
			split_opt, filename = options.stream_tfmap.split("=")
		else:
			filename = options.stream_tfmap
			# TODO: Make this more elegant
			if filename == "video": 
				filename = None
			split_opt = None

		head = ep.stream_tfmap_video( pipeline, head, 
			handler, 
			filename,
			split_opt
		)
		head = pipeparts.mkqueue( pipeline, head )

	if options.disable_triggers:
		pipeparts.mkfakesink( pipeline, head )
		continue

	# Trigger generator
	head = pipeparts.mkbursttriggergen( pipeline, head, ndof, 
		bank = handler.build_filter_xml( res_level )
	)

	if handler.fap is not None:
		# Still needs magic number... or use the EP version
		# ndof_eff = ndof * 0.62
		snr_thresh = ep_utils.determine_thresh_from_fap(fap, ndof)
	else:
		# TODO: Make clear in the ini file that the thresh is power SNR, not amplitude and then remove the square here
		snr_thresh = handler.snr_thresh**2
	head.set_property( "snr-thresh", snr_thresh )

	if verbose:
		head = pipeparts.mkprogressreport( pipeline, head, 
			"Trigger generator resolution level %d, %d DOF" % (res_level, ndof) 
		)

	# TODO: combine trigger streams from various levels

	# TODO: This will have to be linked to multiple outgoing streams
	appsink = pipeparts.mkappsink(pipeline, pipeparts.mkqueue(pipeline, head))
	appsink.connect_after("new-buffer", get_triggers, handler, ndof)

### END OF PIPELINE

# Spectrum notification processing
whitener.connect_after( "notify::mean-psd", on_psd_change, handler )
# Handle spectral correlation changes
# TODO: Make sure this doesn't have to be in the mm loop
whitener.connect_after( "notify::spectral-correlation", on_spec_corr_change, handler )

# Handle shutdowns
signal.signal( signal.SIGINT, handler.shutdown )
signal.signal( signal.SIGTERM, handler.shutdown )

print >>sys.stderr, "Startin' up."
pipeline.set_state( gst.STATE_PLAYING )
#if diagnostics:
def write_pipeline_graph():
	pipeparts.write_dump_dot(pipeline, "gstlal_excesspower", verbose = True)
	gst.DEBUG_BIN_TO_DOT_FILE( pipeline,
		gst.DEBUG_GRAPH_SHOW_ALL,
		"gstlal_excesspower"
	)
mainloop.run()
