#!/usr/bin/env python
#
# Copyright (C) 2011 Chris Pankow
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
"""Stream-based burst analysis tool"""

#
# =============================================================================
#
#                                   Preamble
#
# =============================================================================
#


import os
import sys
import signal
import glob
import tempfile
import threading

from optparse import OptionParser
import ConfigParser
from ConfigParser import SafeConfigParser

import numpy

import pygtk
pygtk.require("2.0")
import gobject
gobject.threads_init()
import pygst
pygst.require("0.10")
import gst

from gstlal.pipeutil import gst
from gstlal import pipeparts
from gstlal.lloidparts import DetectorData, mkLLOIDbasicsrc, get_gate_state
from gstlal.reference_psd import write_psd, read_psd_xmldoc

import gstlal.excesspower as ep
from gstlal.epparts import EPHandler

from glue import GWDataFindClient

from glue.ligolw import ligolw
from glue.ligolw import array
from glue.ligolw import param
from glue.ligolw import lsctables
from glue.ligolw import table
array.use_in(ligolw.LIGOLWContentHandler)
param.use_in(ligolw.LIGOLWContentHandler)
lsctables.use_in(ligolw.LIGOLWContentHandler)
from glue.ligolw import utils

from glue.segments import segment, segmentlist, segmentlistdict, PosInfinity
from glue import segmentsUtils
from glue import gpstime
from glue.lal import LIGOTimeGPS, Cache, CacheEntry

from pylal.xlal.datatypes.snglburst import from_buffer as sngl_bursts_from_buffer
from pylal.xlal.datatypes.real8frequencyseries import REAL8FrequencySeries
from pylal.xlal.lalburst import XLALlnOneMinusChisqCdf

#
# =============================================================================
#
#                        Message Handler Methods
#
# =============================================================================
#

# These are linked later in the pipeline to do the appropriate actions when signals are sent up.

def on_psd_change( elem, pspec, hand ):
	"""
	Get the PSD object and signal the handler to rebuild everything.
	"""
	if options.verbose:
		print >> sys.stderr, "Intercepted spectrum signal."

	hand.psd = REAL8FrequencySeries(
		name = "PSD",
		#epoch = laltypes.LIGOTimeGPS(0, message.structure["timestamp"]),
		f0 = 0.0,
		deltaF = elem.get_property( "delta-f" ),
		#sampleUnits = laltypes.LALUnit(message.structure["sample-units"].strip()),
		data = numpy.array( elem.get_property( "mean-psd" ) )
	)


	# Determine if the PSD has changed enough to warrant rebuilding the filter
	# bank.
	psd_power = sum(hand.psd.data)
	change = abs((hand.psd_power - psd_power) / (hand.psd_power + psd_power) )
	if change > hand.psd_change_thresh:
		if options.verbose:
			print >> sys.stderr, "Processed signal. PSD change %d, regenerating filters" % int(change*100)
		hand.psd_power = psd_power
		hand.rebuild_everything()


def on_spec_corr_change( elem, pspec, hand ):
	"""
	Get the 2-point spectral correlation object and signal the handler to rebuild everything.
	"""
	if options.verbose:
		print >> sys.stderr, "Intercepted correlation signal."
	hand.spec_corr = elem.get_property( "spectral-correlation" )

	if hand.cache_spec_corr:
		f = open( "spec_corr.dat", "w" )
		k_end = len( hand.spec_corr ) / 2
		for k, sp in enumerate( hand.spec_corr ):
			f.write( "%d %g\n" % (k, sp) )
	
	# If the spectrum correlation changes, rebuild everything
	if hand.psd is not None:
		hand.rebuild_everything()

def get_triggers(elem, handler, ndof):

	buffer = elem.emit("pull-buffer")

	if not handler.output:
		return # We don't want event information

	# TODO: Can I set units here on the buffer fields, avoid changing the 
	# triggers themslves and *not* screw up the underlying framework?

	# NOTE: This is here and not in the handler, since self.start may not be 
	# well defined before the pipeline starts, and thus shouldn't be checked 
	# until after we start running. We must know handler.start by the time we 
	# start drawing triggers, though
	whiten_seg = segment( 
		LIGOTimeGPS(handler.start), 
		LIGOTimeGPS(handler.start + handler.whitener_offset)
	)

	for row in sngl_bursts_from_buffer(buffer):
		if row.peak_time in whiten_seg:
			continue
		row.duration *= ndof
		# TODO Move this into the trigger generator
		# FIXME: Determine "magic number" or remove it
		row.confidence = -XLALlnOneMinusChisqCdf(row.snr * 0.62, ndof * 0.62)
		if options.compat:
			row.snr = row.snr / ndof - 1
		handler.triggers.append( row )
	
	#buf_ts = (buffer.timestamp*1e-9 - handler.start) / handler.units + handler.start
	# TODO: Why does the buf_dur need unit conversion, but not the timestamp
	buf_ts = buffer.timestamp*1e-9 
	buf_dur = buffer.duration*1e-9 / handler.units
	handler.stop = (buf_ts + buf_dur)
	if handler.stop - handler.time_since_dump > handler.dump_frequency or len(handler.triggers) >= handler.max_events:

		fname = ep.make_cache_parseable_name(
			inst = handler.inst,
			tag = handler.channel,
			start = handler.time_since_dump,
			stop = handler.stop,
			ext = "xml",
			dir = handler.outdir
		)

		# It's possible that triggers come out so fast that the filename would
		# be identical and overwrite those triggers. This appends an integer
		# identified to prevent thtat.
		uniq = 1
		while os.path.exists( fname ):
			fname = ep.make_cache_parseable_name(
				inst = handler.inst,
				tag = "%d_%s" % (uniq. handler.channel),
				start = handler.time_since_dump,
				stop = handler.stop,
				ext = "xml",
				dir = handler.outdir
			)
			uniq += 1

		handler.write_triggers( filename = fname, flush = True )
		handler.time_since_dump = handler.stop 

	# TODO: Update a single file every couple of seconds.
	"""
	dur = 0
	if( len(handler.triggers) > 0 ):
		dur = handler.triggers[-1].peak_time - handler.triggers[0].peak_time

	if( len(handler.triggers) > 1000 or dur > 16 ):
		handler.write_triggers( flush=True )
	"""

#
# =============================================================================
#
#                             Options Handling
#
# =============================================================================
#

parser = OptionParser()
parser.add_option("-f", "--initialization-file", dest="infile", help="Options to be pased to the pipeline handler. Strongly recommended.", default=None)
parser.add_option("-d", "--diagnostics", dest="diagnostics", action="store_true", help="Turn on multiple diagnostic dumps. Use with caution, as it will dump gigabytes of data (potentially) in a matter of minutes. Useful in nongraphical environemnts to monitor data throughput.", default=False)
parser.add_option("-v", "--verbose", dest="verbose", action="store_true", help="Be verbose.", default=False)
parser.add_option("-n", "--channel-name", dest="channame", action="store", help="Specify the channel name. Note that this will override the specification in the ini file (if any). This is useful if you want to run the same analysis on channels with the same characterisitics.")
parser.add_option("-w", "--channel-width", dest="nbyte", action="store", type=int, help="Specify the channel width in bits (default = 64)", default=64)
parser.add_option("-r", "--sample-rate", dest="sample_rate", action="store", type="int", help="Sample rate of the incoming data.")
parser.add_option("-D", "--data-source", dest="data_source", action="store", help="Data source to read from. Valid options are gwffile,lldata,lldata_sv,whitedata,fakeLIGO,fakeadvLIGO. One is required. If gwffile is selected, then a cache file should also be provided. If whitedata, fakeLIGO, or fakeadvLIGO is selected, then a sample rate must also be provided.")
parser.add_option("-S", "--stream-tfmap", dest="stream_tfmap", action="store", help="Encode the time frequency map to video as it is analyzed. If the argument to this option is \"video\" then the pipeline will attempt to stream to a video source. If the option is instead a filename, the video will be sent to that name. Prepending \"keyframe=\" to the filename will start a new video every time a keyframe is hit.")
parser.add_option("-s", "--gps-start", dest="gps_start", action="store", type="float", help="Seek to gps time before beginning analysis.", default=None)
parser.add_option("-e", "--gps-end", dest="gps_end", action="store", type="float", help="End the analysis at this gps time.", default=None)
parser.add_option("-t", "--disable-triggers", dest="disable_triggers", action="store_true", help="Don't record triggers.", default=False)
parser.add_option("-T", "--file-cache", dest="file_cache", action="store_true", help="Create file caches of output files. If no corresponding --file-cache-name option is provided, an approrpriate name is constructed by default, and will only be created at the successful conclusion of the pipeline run.", default=False)
parser.add_option("-F", "--file-cache-name", dest="file_cache_name", action="store", help="Name of the trigger file cache to be written to. If this option is specified, then when each trigger file is written, this file will be written to with the corresponding cache entry. See --file-cache for other details.", default=None)
parser.add_option("-c", "--lalapps-power-compatibility", dest="compat", action="store_true", default=False, help="Output trigger information which conforms to the lalapps_power conventions.")
parser.add_option("-C", "--clustering", dest="clustering", action="store_true", default=False, help="Employ trigger tile clustering before output stage. Default or if not specificed is off." )
parser.add_option("-u", "--enable-db-uploads", dest="db_uploads", action="store_true", default=False, help="Upload uploads to trigger archiving services (e.g. gracedb). The threshold for upload should be in the initialization file. Default or if not specificed is off." )

(options, args) = parser.parse_args()

# The data rate at which we wish to do analysis
# Assumed lower than the input data
data_source = options.data_source

# TODO: Stop reinventing the wheel and replace this with mkLLOIDsrc
valid_data_sources = [ "gwffile", 
	  "nds",
	  "lldata", 
	  "lldata_sv", 
	  "whitedata", 
	  "fakeLIGO", 
	  "fakeadvLIGO" ]
if not data_source in valid_data_sources:
	print >>sys.stderr, "Either no data soruce was selected, or an invalid one was requested."
	sys.exit(-1)

if not options.sample_rate and ( data_source in ["whitedata", "fakeLIGO", "fakeadvLIGO"] ):
	print >>sys.stderr, "Sample rate not specified and fake data requested."
	sys.exit(-1)
else:
	sample_rate = options.sample_rate

# Verbosity and diagnostics
verbose = options.verbose
diagnostics = options.diagnostics

#
# =============================================================================
#
#                           Handler / Pipeline options
#
# =============================================================================
#

# We need a pipeline and pipeline handler instance to configure
pipeline = gst.Pipeline( "gstlal_excesspower" )
mainloop = gobject.MainLoop()
handler = EPHandler(mainloop, pipeline)

if not options.file_cache:
	handler.output_cache = None

if options.file_cache and options.file_cache_name is not None:
	handler.output_cache_name = options.file_cache_name

handler.rate = options.sample_rate
handler.clustering = options.clustering
handler.verbose = options.verbose

# Set process params in handler for use with the output xmldocs
handler.make_process_tables( options, None )

# Used to keep track if we need to lock the PSD into the whitener
if not options.infile:
	print >>sys.stderr, "Initialization file required."
elif not os.path.exists( options.infile ):
	print >>sys.stderr, "Initialization file path is invalid."
	sys.exit(-1)

cfg = SafeConfigParser()
cfg.read( options.infile )

# Instruments and channels
handler.inst = cfg.get( "instrument", "detector" )
try:
	handler.channel = cfg.get( "instrument", "channel" )
except ConfigParser.NoOptionError:
	handler.channel = None
if options.channame is not None:
	handler.channel = options.channame

# We weren't provided a channel -- we're not mind readers, dammit!
if handler.channel is None:
	exit("No channel specified in the configuration file or on the command line.")
print "Channel name: " + handler.channel

try:
	site = cfg.get( "instrument", "site" )
except ConfigParser.NoOptionError:
	print >>sys.stderr, "No site requested, using detector as default cache sieve."
	site = None

# Handler options
try:
	handler.fft_length = cfg.getfloat( "tf_parameters", "fft-length" )
except ConfigParser.NoOptionError:
	pass

handler.flow = cfg.getfloat( "tf_parameters", "min-frequency" )
handler.fhigh = cfg.getfloat( "tf_parameters", "max-frequency" )
handler.base_band = cfg.getfloat( "tf_parameters", "base-resolution" )
handler.max_level = cfg.getint( "tf_parameters", "max-resolution-level" )
handler.max_duration = cfg.getfloat( "tf_parameters", "max-time-resolution" )

if cfg.has_option( "cache", "cache-psd-every" ):
	handler.cache_psd = cfg.getint( "cache", "cache-psd-every" )
	print "PSD caching enabled."
else:
	handler.cache_psd = None
	
psdfile = None
if cfg.has_option( "cache", "reference-psd" ):
	psdfile = cfg.get( "cache", "reference-psd" )
	try:
		handler.psd = read_psd_xmldoc( utils.load_filename( psdfile, contenthandler = ligolw.LIGOLWContentHandler ) )[ handler.inst ]
		print "Reference PSD for instrument %s from file %s loaded" % ( handler.inst, psdfile )
		# Reference PSD disables caching (since we already have it
		handler.cache_psd = None
	except KeyError: # Make sure we have a PSD for this instrument
		sys.exit( "PSD for instrument %s requested, but not found in file %s. Available instruments are %s" % (handler.inst, psdfile, str(handler.psd.keys())) )

# TODO: Fix this up too
if cfg.has_option( "cache", "cache-spectral-correlation" ):
	handler.cache_spec_corr = cfg.getboolean( "cache", "cache-spectral-correlation" )

if cfg.has_option( "triggering", "output-file" ):
	handler.outfile = cfg.get( "triggering", "output-file" )
if cfg.has_option( "triggering", "output-directory" ):
	handler.outdir = cfg.get( "triggering", "output-directory" )

handler.snr_thresh = cfg.getfloat( "triggering", "snr-thresh" )
if options.db_uploads:

	handler.db_thresh = cfg.getfloat( "triggering", "db-thresh" )
	if handler.db_thresh is None:
		print >>sys.stderr, "Warning, DB upload requested, but no threshold provided. Disabling."

	handler.db_client = cfg.get( "triggering", "db-client" )
	if handler.db_thresh is None:
		print >>sys.stderr, "Warning, DB upload requested, but no DB path provided. Disablinhg"
		handler.db_thresh = None

if cfg.has_option( "triggering", "events_per_file" ):
	handler.max_events = cfg.get_int( "triggering", "events_per_file" )

# If a specific (trigger) time is of interest, specify its GPS here
# TODO: Read from sngl_inspirals and sngl_bursts
# TODO: Make this useful
trigger_begin, trigger_end = None, None
if cfg.has_option( "triggering", "trig_time_start" ):
	trigger_begin = cfg.getfloat( "triggering", "trig_time_start" )
if cfg.has_option( "triggering", "trig_time_end" ):
	trigger_end = cfg.getfloat( "triggering", "trig_time_end" )
if trigger_begin and trigger_end:
	handler.set_trigger_time_and_action( segment( trigger_begin, trigger_end ) )

gwflocation = cfg.get( "instrument", "location" )
if gwflocation == "":
	gwflocation = None
if options.data_source == "gwffile" and gwflocation is None:

	if options.gps_start is None or options.gps_end is None:
		sys.exit( "No frame cache present, and no GPS times set. Cannot query for data without an interval to query in." )

	# Shamelessly stolen from gw_data_find
	print "Querying LDR server for data location." 
	try:
		server, port = os.environ["LIGO_DATAFIND_SERVER"].split(":")
	except ValueError:
		sys.exit( "Invalid LIGO_DATAFIND_SERVER environment variable set" )
	print "Server is %s:%s" % (server, port)

	try:
		frame_type = cfg.get( "instrument", "frame_type" )
	except ConfigParser.NoOptionError:
		sys.exit( "Invalid cache location, and no frame type set, so I can't query LDR for the file locations." )
	if frame_type == "":
		sys.exit( "No frame type set, aborting." )

	print "Frame type is %s" % frame_type
	connection =  \
		GWDataFindClient.GWDataFindHTTPConnection(host=server, port=port)
	print "Equivalent command line is "
	print "gw_data_find -o %s -s %d -e %d -u file -t %s" %(handler.inst[0], options.gps_start, options.gps_end, frame_type)
	cache = connection.find_frame_urls( handler.inst[0], 
		frame_type,
		options.gps_start,
		options.gps_end,
		urltype="file",
		on_gaps="error"
	)

	tmpfile, tmpname = tempfile.mkstemp()
	print "Writing cache of %d files to %s" % (len(cache), tmpname)
	cache.tofile( open(tmpname, "w") )
	connection.close()
	gwflocation = tmpname

inj_loc = cfg.get( "injections", "xml-location" )
if not os.path.isfile( inj_loc ):
	print >>sys.stderr, "Injection file not found, disabling option."
	inj_loc = None

base_band = handler.base_band

# This is invoked here, or else the default rate is used, which will cause funny behavior for the defaults with some cases
# TODO: Less hardcodish -- update this when the rate or base_band is updated
handler.filter_len = 2*int(2*handler.rate/handler.base_band)
handler.build_default_psd( handler.rate, handler.filter_len )
handler.rebuild_filter()
handler.rebuild_chan_mix_matrix()

# Max trigger duration (s)
# TODO: Uhh... what was going on here?
#handler.max_duration

#
# =============================================================================
#
#                                     Main
#
# =============================================================================
#

if options.verbose:
	print >>sys.stderr, "Assembling pipeline...\n",

duration = -1

# Data source
# TODO: Use new datasource library
if data_source == "nds":
	head = pipeparts.mkndssrc( pipeline,
		host = cfg.get( "instrument", "ndshost" ),
		instrument = handler.inst,
		channel_name = handler.channel
	)
	try:
		head.set_property( "port", cfg.getint( "instrument", "ndsport" ) )
	except ConfigParser.NoOptionError:
		print >>sys.stderr, "Warning, no NDS host port specified. Using default."

	# FIXME: Don't assume real-time
	head.set_property( "channel-type", "online" )

elif data_source == "fakeLIGO":
	head = pipeparts.mkfakeLIGOsrc( pipeline, 
		instrument = handler.inst,
		channel_name = handler.channel
	)

elif data_source == "fakeadvLIGO":
	head = pipeparts.mkfakeadvLIGOsrc( pipeline, 
		instrument = handler.inst,
		channel_name = handler.channel
	)

elif data_source == "whitedata":
	# TODO: use mkaudiotestsrc
	head = gst.element_factory_make( "audiotestsrc" )
	pipeline.add( head )
	head.set_property( "wave", 9 ) # unity variance zero mean gaussian noise

elif data_source == "lldata":
	# FIXME: Unhardcode this.
	# TODO: Possibly make lldata have a parameter like:
	# --data-source lldata=shmname
	shmmap = { "L1": "LLO_Data",
	           "H1": "LHO_Data",
	           "V1": "VIRGO_Data"}
	head = pipeparts.mklvshmsrc( pipeline, shm_name = shmmap[handler.inst] )
	src = head = pipeparts.mkframecppchanneldemux( pipeline, head )
	sink = pipeparts.mkaudiorate( pipeline, None, skip_to_first = True, silent = False )
	src_deferred_link( src, "%s:%s" % (handler.inst, handler.channel), sink.get_pad("sink") )
	head = sink

	# FIXME: This is a guess. Not a terrible one, but still inaccurate
	handler.start = gpstime.GpsSecondsFromPyUTC( time.time() )
	handler.time_since_dump = gpstime.GpsSecondsFromPyUTC( time.time() )

elif data_source == "lldata_sv":
	head = mkLLOIDbasicsrc( pipeline,
		seekevent = None,
		instrument = handler.inst,
		detector = DetectorData(None, handler.channel),
		data_source = "online",
		state_vector_on_off_dict = { handler.inst: (ep.DEFAULT_DQ_VECTOR_ON, ep.DEFAULT_DQ_VECTOR_OFF) }
	)
	# FIXME: Need to get the gate by something other than name
	gate = pipeline.get_by_name( "lal_gate0" )
	gate.connect("start", handler.handle_segment, "on" )
	gate.connect("stop", handler.handle_segment, "off" )

	# FIXME: This is a guess. Not a terrible one, but still inaccurate
	handler.time_since_dump = gpstime.GpsSecondsFromPyUTC( time.time() )

elif data_source == "gwffile":
	framesrc = head = pipeparts.mkframesrc( pipeline, 
		gwflocation, 
		handler.inst, 
		handler.channel
	)

	if options.gps_start is None and options.gps_end is None:
		print >>sys.stderr, "Warning, inferring analysis duration from cache."

	start, duration = ep.duration_from_cache( gwflocation )
	if options.gps_start:
		handler.start = start = options.gps_start 
	else:
		options.gps_start = handler.start = start
		options.gps_start = float(options.gps_start)
	if options.gps_end:
		duration = options.gps_end - start
	else:
		options.gps_end = start + duration
		options.gps_end = float(options.gps_end)
	handler.time_since_dump = start

	# TODO: Must unhardcode this -- but requires knowledge of native rate
	if options.nbyte == 64:
		bsize=16384*8 # 64 bit stream
	elif options.nbyte == 32:
		bsize=16384*4 # 32 bit stream
	if options.verbose:
		print "Blocksize %d" % bsize
	#duration = int(duration*handler.rate/16384.0)
	head.set_property( "blocksize", bsize )

	# FIXME: Make this an option
	if handler.inst == "V1": 
		sieve = "V*"
		head.set_property( "cache-dsc-regex", sieve )
	if site:
		sieve = site[0] + "*"
	else: 
		sieve = handler.inst + "*"
	head.set_property( "cache-dsc-regex", sieve )
	
else:
	print >>sys.stderr, "Data source %s not recognized. Check the valid options in the help message."
	sys.exit(-1)

# Seeking
seekevent = None
if options.gps_start is not None and options.gps_end is not None:

	# TODO: Use LIGOTimeGPS
	print "Duration: " + str(duration)
	print >>sys.stderr, "Seeking to GPS %d, segment duration %d" % (start, duration)
	print >>sys.stderr, "Will stop at GPS %d" % (start + duration)

	seek, dur = long(start*1e9), long(duration*1e9)
	seekevent = gst.event_new_seek( 1.0, 
		gst.Format(gst.FORMAT_TIME),
		gst.SEEK_FLAG_KEY_UNIT | gst.SEEK_FLAG_FLUSH,
		gst.SEEK_TYPE_SET, seek,
		gst.SEEK_TYPE_SET, seek + dur
	)

elif options.gps_start is not None:

	print >>sys.stderr, "Seeking to GPS %d (ns)" % start
	seek = long(options.gps_start*1e9)
	seekevent = gst.event_new_seek( 1.0, 
		gst.Format(gst.FORMAT_TIME),
		gst.SEEK_FLAG_KEY_UNIT | gst.SEEK_FLAG_FLUSH,
		gst.SEEK_TYPE_SET, seek,
		gst.SEEK_TYPE_NONE, 0
	)

if( seekevent is not None ):
	if head.set_state(gst.STATE_READY) != gst.STATE_CHANGE_SUCCESS:
		exit("Unable to ready pipeline to accept seek.")
	if not head.send_event( seekevent ):
		exit("Unable to send seek event to " + str(head))

# Diagnostic plot
if( diagnostics ):
	head = postdatatee = pipeparts.mktee( pipeline, head )
	pipeparts.mknxydumpsink( pipeline, 
		pipeparts.mkqueue( pipeline, postdatatee ), 
		cfg.get( "diagnostics", "strain-data-output" )
	)

# Convert to 64 bit
head = pipeparts.mkcapsfilter( pipeline, pipeparts.mkaudioconvert( pipeline, head ), "audio/x-raw-float,width=64" )
# Data conditioning
head = pipeparts.mkcapsfilter( pipeline, pipeparts.mkresample( pipeline, head ), "audio/x-raw-float,rate=%d" % handler.rate )

if inj_loc:
	head = pipeparts.mkinjections( pipeline, head, inj_loc )

if inj_loc and verbose:
	head = pipeparts.mkprogressreport( pipeline, head, "injection stream" )

head = whitener = pipeparts.mkwhiten( pipeline, head )
whitener.set_property( "fft-length", handler.fft_length ) 
if psdfile is not None: # In other words, we have a reference PSD
	whitener.set_property( "mean-psd", handler.psd.data )
	whitener.set_property( "psd-mode", 1 ) # GSTLAL_PSDMODE_FIXED

head = pipeparts.mkqueue( pipeline, head )

# Diagnostic plot
if diagnostics:
	head = postresamptee = pipeparts.mktee( pipeline, head )
	pipeparts.mknxydumpsink( pipeline, 
		pipeparts.mkqueue( pipeline, head ), 
		cfg.get( "diagnostics", "whitened-data-output" )
	)

if verbose:
	head = pipeparts.mkprogressreport( pipeline, head, "whitened stream" )

# excess power channel firbank
# TODO: Do we gain anything by changing the block stride?
head = pipeparts.mkfirbank( pipeline, head, 
	time_domain=False, 
	block_stride=handler.rate 
)

handler.add_firbank( head )
nchannels = handler.filter_bank.shape[0]
print "FIR bank constructed with %d %f Hz channels" % (nchannels, base_band)

if verbose:
	head = pipeparts.mkprogressreport( pipeline, head, "FIR bank stream" )

# TODO: Uncomment here
#head = postfirtee = pipeparts.mkqueue( pipeline, pipeparts.mktee( pipeline, head ) )
#####

if diagnostics:
	pipeparts.mknxydumpsink( pipeline, 
		postfirtee, 
		cfg.get( "diagnostics", "fir-output" )
	)

# First branch -- send fully sampled data to wider channels for processing
nlevels = int(numpy.ceil( numpy.log2( nchannels ) )) 
for res_level in range(0, min(handler.max_level, nlevels)):
	# TODO: Uncomment here
	#head = postfirtee
	#######

	band = base_band * 2**res_level
	# TODO: Check this
	chan = numpy.ceil( nchannels / 2.0**res_level )

	# The undersample_rate for band = R/2 is => sample_rate (passthrough)
	undersamp_rate = 2 * band


	# If the rate which would be set by the undersampler falls below one, we 
	# have to take steps to prevent this, as gstreamer can't handle this. The 
	# solution is to change the "units" of the rate. Ideally, this should be 
	# done much earlier in the pipeline (e.g. as the data comes out of the 
	# source), however, to avoid things like figuring out what that means for 
	# the FIR bank we change units here, and readjust appropriately in the 
	# trigger output.
	if undersamp_rate < 1:
		print "Automatically adjusting units to compensate for undersample rate falling below unity."
		# No, it's not factors of ten, but rates which aren't factors
		# of two are often tricky, thus if the rate is a factor of two, the 
		# units conversion won't change that.
		if undersamp_rate > ep.EXCESSPOWER_UNIT_SCALE['mHz']:
			unit = 'mHz'
		elif undersamp_rate > ep.EXCESSPOWER_UNIT_SCALE['uHz']:
			unit = 'uHz'
		elif undersamp_rate > ep.EXCESSPOWER_UNIT_SCALE['nHz']:
			unit = 'nHz'
		else:
			sys.exit( "Requested undersampling rate would fall below 1 nHz." )
		# FIXME: No love for positive power of 10 units?

		handler.units = ep.EXCESSPOWER_UNIT_SCALE[unit]
		undersamp_rate /= handler.units
		print "Undersampling rate for level %d: %f %s" % (res_level, undersamp_rate, unit)
		head = pipeparts.mkcapssetter( pipeline, head, "audio/x-raw-float,rate=%d" % (handler.rate/handler.units), replace=False )
		head = pipeparts.mkgeneric( pipeline, head, "lal_audioundersample" )
		head = pipeparts.mkcapssetter( pipeline, head, "audio/x-raw-float,rate=%d" % undersamp_rate, replace=False )
	else:
		print "Undersampling rate for level %d: %f Hz" % (res_level, undersamp_rate)
		head = pipeparts.mkgeneric( pipeline, head, "lal_audioundersample" )
		head = pipeparts.mkcapsfilter( pipeline, head, "audio/x-raw-float,rate=%d" % undersamp_rate )

	if diagnostics:
		head = postustee = pipeparts.mktee( pipeline, pipeparts.mkqueue( pipeline, head ) )
		pipeparts.mknxydumpsink( pipeline, postustee, "postundersamp_res_%d.txt" % res_level )

	if verbose:
		head = pipeparts.mkprogressreport( pipeline, head, 
			"Undersampled stream level %d" % res_level
		)

	head = matmixer = pipeparts.mkmatrixmixer( pipeline, head )
	handler.add_matmixer( matmixer, res_level )

	if verbose:
		head = pipeparts.mkprogressreport( pipeline, head,
			"post matrix mixer %d" % res_level 
		)

	if diagnostics:
		head = postmmtee = pipeparts.mktee( pipeline, pipeparts.mkqueue( pipeline, head ) )
		pipeparts.mknxydumpsink( pipeline, postmmtee, "postmatmix_res_%d.txt" % res_level )

	head = pipeparts.mkgeneric( pipeline, head, "pow" )
	head.set_property( "exponent", 2 )

	if verbose:
		head = pipeparts.mkprogressreport( pipeline, head, 
			"Energy stream level %d" % res_level
		)

	# TODO: Uncomment here
	#head = pipeparts.mkqueue( pipeline, head )
	#####

	ndof = 2 # samples -- min number
	# Second branch -- duration
	# max_samp = int(handler.max_duration*rate)
	#while duration <= max_samp:
		#duration = duration << 1

	# Multi channel FIR filter -- used to add together frequency bands into 
	# tiles
	# FIXME: This is a workaround around until the audiofirfilter is fixed.
	# Basically, if the firfilter gets a small enough kernel, it invokes 
	# time-domain convolution. Something about the way it fills the buffers is
	# wrong in regards to the way it keeps history and thus the output buffers
	# partially desynched from the input -- even with an identity transform.

	# Workaround: Zero pad the kernel past the length (32 samples) in which the 
	# element switches to FFT convolution which doesn't exhibit bad behavior.

	#head = pipeparts.mkchecktimestamps( pipeline, head, "before audiofir" )
	head = pipeparts.mkgeneric( pipeline, head, "audiofirfilter" )
	head.set_property( "kernel", 
		ep.build_fir_sq_adder( ndof, padding=max(0, 33-ndof) )
	)
	head = pipeparts.mknofakedisconts( pipeline, head )
	#head = pipeparts.mkchecktimestamps( pipeline, head, "after audiofir" )

	if diagnostics:
		head = postdurtee = pipeparts.mktee( pipeline, pipeparts.mkqueue( pipeline, head ) )
		pipeparts.mknxydumpsink( pipeline, postdurtee, "postdur_res_%d_%d.txt" % (res_level, ndof) )

	if verbose:
		head = pipeparts.mkprogressreport( pipeline, head, 
			"After energy summation resolution level %d, %d DOF" % 
				(res_level, ndof) 
		)

	# TODO: Audio
	if options.stream_tfmap:
		if len(options.stream_tfmap.split("=")) == 2:
			split_opt, filename = options.stream_tfmap.split("=")
		else:
			filename = options.stream_tfmap
			# TODO: Make this more elegant
			if filename == "video": 
				filename = None
			split_opt = None

		head = ep.stream_tfmap_video( pipeline, head, 
			handler, 
			filename,
			split_opt
		)

	if options.disable_triggers:
		pipeparts.mkfakesink( pipeline, head )
		continue

	# Trigger generator
	head = pipeparts.mkbursttriggergen( pipeline, head, ndof, 
		bank = handler.build_filter_xml( res_level )
	)

	if handler.fap is not None:
		# Still needs magic number... or use the EP version
		# ndof_eff = ndof * 0.62
		snr_thresh = ep_utils.determine_thresh_from_fap(fap, ndof)
	else:
		# TODO: Make clear in the ini file that the thresh is power SNR, not amplitude and then remove the square here
		snr_thresh = handler.snr_thresh**2
	head.set_property( "snr-thresh", snr_thresh )

	if verbose:
		head = pipeparts.mkprogressreport( pipeline, head, 
			"Trigger generator resolution level %d, %d DOF" % (res_level, ndof) 
		)

	# TODO: combine trigger streams from various levels

	# TODO: This will have to be linked to multiple outgoing streams
	appsink = pipeparts.mkappsink(pipeline, pipeparts.mkqueue(pipeline, head))
	appsink.connect_after("new-buffer", get_triggers, handler, ndof)

### END OF PIPELINE

# Spectrum notification processing
whitener.connect_after( "notify::mean-psd", on_psd_change, handler )
# Handle spectral correlation changes
# TODO: Make sure this doesn't have to be in the mm loop
whitener.connect_after( "notify::spectral-correlation", on_spec_corr_change, handler )

# Handle shutdowns
signal.signal( signal.SIGINT, handler.shutdown )
signal.signal( signal.SIGTERM, handler.shutdown )

print >>sys.stderr, "Startin' up."
pipeline.set_state( gst.STATE_PLAYING )
if diagnostics:
	write_dump_dot(pipeline, "test", verbose = True)
	#gst.DEBUG_BIN_TO_DOT_FILE( pipeline,
		#gst.DEBUG_GRAPH_SHOW_ALL,
		#cfg.get( "diagnostics", "dot-file-location" )
	#)
mainloop.run()
