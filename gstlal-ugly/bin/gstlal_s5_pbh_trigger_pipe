#!/usr/bin/python
"""
This program makes a dag to run gstlal_inspiral offline
"""

__author__ = 'Chad Hanna <channa@caltech.edu>'

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, copy, math
import subprocess, socket, tempfile

##############################################################################
# import the modules we need to build the pipeline
from glue import iterutils
from glue import pipeline
from glue import lal
from glue.ligolw import lsctables
from glue import segments
from glue.ligolw import array
from glue.ligolw import param
import glue.ligolw.utils as utils
import glue.ligolw.utils.segments as ligolw_segments
from optparse import OptionParser
from gstlal.svd_bank import read_bank
from gstlal import inspiral, inspiral_pipe
import numpy


#
# Generic job classes
#


class InspiralJob(pipeline.CondorDAGJob):
	"""
	A generic job class for gstlal inspiral stuff
	"""
	def __init__(self, executable, tag_base):
		self.__prog__ = tag_base
		self.__executable = executable
		self.__universe = 'vanilla'
		pipeline.CondorDAGJob.__init__(self, self.__universe, self.__executable)
		self.add_condor_cmd('getenv','True')
		self.tag_base = tag_base
		self.set_sub_file(tag_base+'.sub')
		self.set_stdout_file('logs/'+tag_base+'-$(macroid)-$(macronodename)-$(cluster)-$(process).out')
		self.set_stderr_file('logs/'+tag_base+'-$(macroid)-$(macronodename)-$(cluster)-$(process).err')
		self.number = 1


class InspiralNode(pipeline.CondorDAGNode):
	"""
	A generic node class for gstlal inspiral stuff
	"""
	def __init__(self, job, dag, p_node=[]):
		pipeline.CondorDAGNode.__init__(self, job)
		for p in p_node:
			self.add_parent(p)
		dag.add_node(self)


#
# gstlal_s5_pbh_summary_page
#


class gstlal_s5_pbh_summary_page_job(InspiralJob):
	"""
	A gstlal_s5_pbh_summary_page job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_s5_pbh_summary_page'), tag_base='gstlal_s5_pbh_summary_page'):
		InspiralJob.__init__(self, executable, tag_base)


class gstlal_s5_pbh_summary_page_node(InspiralNode):
	"""
	A gstlal_s5_pbh_summary_page_node
	"""
	def __init__(self, job, dag, name_tag, web_dir, title, open_box=True, p_node=[]):
		InspiralNode.__init__(self, job, dag, p_node)
		self.add_var_opt("output-name-tag", name_tag)
		self.add_var_opt("webserver-dir", web_dir)
		self.add_var_opt("title", title)
		if open_box: self.add_var_opt("open-box", "")


#
# gstlal_inspiral_plotsummary
#


class gstlal_inspiral_plotsummary_job(InspiralJob):
	"""
	A gstlal_inspiral_plotsummary_job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_inspiral_plotsummary'), tag_base='gstlal_inspiral_plotsummary'):
		InspiralJob.__init__(self, executable, tag_base)


class gstlal_inspiral_plotsummary_node(InspiralNode):
	"""
	A gstlal_inspiral_plotsummary_node
	"""
	def __init__(self, job, dag, base, input=[], tmp_space=inspiral_pipe.log_path(), p_node=[]):
		InspiralNode.__init__(self, job, dag, p_node)
		self.add_var_opt("base", base)
		self.add_var_opt("tmp-space", tmp_space)
		for f in input:
			self.add_file_arg(f)


#
# lalapps_run_sqlite
#


class lalapps_run_sqlite_job(InspiralJob):
	"""
	A lalapps_run_sqlite
	"""
	def __init__(self, executable=inspiral_pipe.which('lalapps_run_sqlite'), tag_base='lalapps_run_sqlite'):
		InspiralJob.__init__(self, executable, tag_base)


class lalapps_run_sqlite_node(InspiralNode):
	"""
	A lalapps_run_sqlite node
	"""
	def __init__(self, job, dag, sql_file, input=[], tmp_space=inspiral_pipe.log_path(), p_node=[]):
		InspiralNode.__init__(self, job, dag, p_node)
		self.add_var_opt("sql-file", sql_file)
		self.add_var_opt("tmp-space", tmp_space)
		if len(input) == 1:
			self.output_name = input[0]
		for f in input:
			self.add_file_arg(f)


#
# ligolw_sqlite
#


class ligolw_sqlite_from_xml_job(InspiralJob):
	"""
	A ligolw_sqlite_job
	"""
	def __init__(self, executable=inspiral_pipe.which('ligolw_sqlite'), tag_base='ligolw_sqlite_from_xml'):
		InspiralJob.__init__(self, executable, tag_base)


class ligolw_sqlite_to_xml_job(InspiralJob):
	"""
	A ligolw_sqlite_job
	"""
	def __init__(self, executable=inspiral_pipe.which('ligolw_sqlite'), tag_base='ligolw_sqlite_to_xml'):
		InspiralJob.__init__(self, executable, tag_base)


class ligolw_sqlite_node(InspiralNode):
	"""
	A ligolw_sqlite node
	"""
	def __init__(self, job, dag, database, input=[], replace=True, tmp_space=inspiral_pipe.log_path(), extract=None, p_node=[]):
		InspiralNode.__init__(self, job, dag, p_node)
		if extract is not None:
			self.add_var_opt("extract", extract)
		self.add_var_opt("database", database)
		if replace:
			self.add_var_opt("replace", "")
		self.add_var_opt("tmp-space", tmp_space)
		for f in input:
			if f is not None:
				self.add_file_arg(f)
		self.output_name = database


#
# ligolw_inspinjfind
#


class ligolw_inspinjfind_job(InspiralJob):
	"""
	A ligolw_inspinjfind_job
	"""
	def __init__(self, executable=inspiral_pipe.which('ligolw_inspinjfind'), tag_base='ligolw_inspinjfind'):
		InspiralJob.__init__(self, executable, tag_base)


class ligolw_inspinjfind_node(InspiralNode):
	"""
	A ligolw_inspinjfind node
	"""
	def __init__(self, job, dag, xml, p_node=[]):
		InspiralNode.__init__(self, job, dag, p_node)
		self.add_var_arg(xml)
		self.output_name = xml


#
# gstlal_inspiral
#


class gstlal_inspiral_job(InspiralJob):
	"""
	A gstlal_inspiral job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_inspiral'), tag_base='gstlal_inspiral'):
		InspiralJob.__init__(self, executable, tag_base)
		self.add_condor_cmd('requirements', '( CAN_RUN_MULTICORE )')
		self.add_condor_cmd('request_cpus', '6')
		self.add_condor_cmd('+RequiresMultipleCores', 'True')

class gstlal_inspiral_inj_job(InspiralJob):
	"""
	A gstlal_inspiral job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_inspiral'), tag_base='gstlal_inspiral_inj'):
		InspiralJob.__init__(self, executable, tag_base)
		self.add_condor_cmd('requirements', '( CAN_RUN_MULTICORE )')
		self.add_condor_cmd('request_cpus', '6')
		self.add_condor_cmd('+RequiresMultipleCores', 'True')


class gstlal_inspiral_node(InspiralNode):
	"""
	A gstlal_inspiral node
	"""
	#FIXME add frame segments, name and veto segments name
	def __init__(self, job, dag, frame_cache, frame_segments_file, frame_segments_name, gps_start_time, gps_end_time, channel_dict, reference_psd, svd_bank, tmp_space=inspiral_pipe.log_path(), ht_gate_thresh=20.0, injections=None, control_peak_time = 8, vetoes=None, time_slide_file=None, fir_stride = 8, instruments = "H1H2L1", p_node=[]):
		InspiralNode.__init__(self, job, dag, p_node)

		if time_slide_file is not None:
			self.add_var_opt("time-slide-file", time_slide_file)
		self.add_var_opt("frame-cache", frame_cache)
		self.add_var_opt("frame-segments-file", frame_segments_file)
		self.add_var_opt("frame-segments-name", frame_segments_name)
		self.add_var_opt("gps-start-time",gps_start_time)
		self.add_var_opt("gps-end-time",gps_end_time)
		self.add_var_opt("channel-name", inspiral.pipeline_channel_list_from_channel_dict(channel_dict))
		self.add_var_opt("reference-psd", reference_psd)
		self.add_var_opt("svd-bank", svd_bank)
		self.add_var_opt("tmp-space", tmp_space)
		self.add_var_opt("track-psd", "")
		self.add_var_opt("control-peak-time", control_peak_time)
		self.add_var_opt("fir-stride", fir_stride)
		self.injections = injections
		if self.injections is not None:
			self.add_var_opt("injections", injections)
		if vetoes is not None:
			self.add_var_opt("veto-segments-file", vetoes)
		path = os.getcwd()
		svd_bank = os.path.split(svd_bank)[1].replace('.xml','')
		if self.injections is not None:
			self.output_name = '%s/%s-%04d-%d-%d-LLOID-simulations.sqlite' % (path, instruments, job.number, gps_start_time, gps_end_time)
		else:
			self.output_name = '%s/%s-%04d-%d-%d-LLOID.sqlite' % (path, instruments, job.number, gps_start_time, gps_end_time)
			self.background_name = '%s/%s-%04d-%d-%d-LLOID_snr_chi.xml.gz' % (path, instruments, job.number, gps_start_time, gps_end_time)
		job.number += 1
		self.add_var_opt("output",self.output_name)
		dag.output_cache.append(lal.CacheEntry(instruments, "-", segments.segment(gps_start_time, gps_end_time), "file://localhost/%s" % (self.output_name,)))


#
# gstlal_compute_far_from_snr_chisq_histograms
#


class gstlal_inspiral_calc_likelihood_job(InspiralJob):
	"""
	A gstlal_inspiral_calc_likelihood job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_inspiral_calc_likelihood'), tag_base='gstlal_inspiral_calc_likelihood'):
		InspiralJob.__init__(self, executable, tag_base)


class gstlal_inspiral_calc_likelihood_node(InspiralNode):
	"""
	A gstlal_inspiral_calc_likelihood node
	"""
	def __init__(self, job, dag, likelihood_file, synthesize_injections = 1000000, input = [], likelihood_output_name = "post_calc_likelihood_", background_prior = 1.0, p_node=[]):
		InspiralNode.__init__(self, job, dag, p_node)
		self.add_var_opt("likelihood-file", likelihood_file)
		self.add_var_opt("tmp-space", inspiral_pipe.log_path())
		for f in input:
			self.add_file_arg(f)
		fpath = os.path.split(likelihood_file)
		self.background_name = os.path.join(fpath[0], likelihood_output_name + fpath[1])
		self.add_var_opt("write-likelihood", self.background_name)
		self.add_var_opt("background-prior", background_prior)
		self.add_var_opt("synthesize-injections", synthesize_injections)
		self.output_names = input


#
# gstlal_compute_far_from_snr_chisq_histograms
#


class gstlal_compute_far_from_snr_chisq_histograms_jobFAP(InspiralJob):
	"""
	A gstlal_compute_far_from_snr_chisq_histograms job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_compute_far_from_snr_chisq_histograms'), tag_base='gstlal_compute_far_from_snr_chisq_histogramsFAP'):
		InspiralJob.__init__(self, executable, tag_base)


class gstlal_compute_far_from_snr_chisq_histograms_jobFAR(InspiralJob):
	"""
	A gstlal_compute_far_from_snr_chisq_histograms job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_compute_far_from_snr_chisq_histograms'), tag_base='gstlal_compute_far_from_snr_chisq_histogramsFAR'):
		InspiralJob.__init__(self, executable, tag_base)


class gstlal_compute_far_from_snr_chisq_histograms_node(InspiralNode):
	"""
	A gstlal_compute_far_from_snr_chisq_histograms_job node
	"""
	def __init__(self, job, dag, background_bins_file, additional_trials_factor, FAR = False, FAP = False, noninj_input = [], inj_input = [], p_node=[]):
		InspiralNode.__init__(self, job, dag, p_node)
		if background_bins_file is not None:
			self.add_var_opt("background-bins-file", background_bins_file)
		self.add_var_opt("additional-trials-factor", additional_trials_factor)
		self.add_var_opt("tmp-space", inspiral_pipe.log_path())
		if FAP:
			self.add_var_opt("compute-fap", "")
		if FAR:
			self.add_var_opt("compute-far", "")
		if len(inj_input) > 0:
			self.add_var_opt("injection-dbs", pipeline_dot_py_append_opts_hack("injection-dbs", inj_input))
		self.add_var_opt("non-injection-db", pipeline_dot_py_append_opts_hack("non-injection-db", noninj_input))
		self.output_names = inj_input + noninj_input


#
# Utility functions
#


# FIXME surely this is in glue
def parse_cache_str(instr):
	dictcache = {}
	if instr is None: return dictcache
	for c in instr.split(','):
		ifo = c.split("=")[0]
		cache = c.replace(ifo+"=","")
		dictcache[ifo] = cache
	return dictcache

def pipeline_dot_py_append_opts_hack(opt, vals):
	out = vals[0]
	for v in vals[1:]:
		out += " --%s %s" % (opt, v)
	return out

def parse_command_line():
	parser = OptionParser(description = __doc__)
	parser.add_option("--injections", metavar = "filename", help = "Set the name of the xml file for injections")
	parser.add_option("--frame-cache", metavar = "filename", help = "Set the frame cache file")
	parser.add_option("--frame-segments-file", metavar = "filename", help = "Set the frame segments file")
	parser.add_option("--frame-segments-name", metavar = "name", help = "Set the frame segments name")
	parser.add_option("--reference-psd", metavar = "filename", help = "Set the reference psd file.")
	parser.add_option("--bank-cache", metavar = "filenames", help = "Set the bank cache files in format H1=H1.cache,H2=H2.cache, etc..")
	parser.add_option("--vetoes", metavar = "filename", help = "Set the veto xml file.")
	parser.add_option("--gps-start-time", metavar = "GPS", help = "Set the gps start time in seconds", type="int")
	parser.add_option("--gps-stop-time", metavar = "GPS", help = "Set the gps stop time in seconds", type="int")
	#FIXME get this from the cache?
	parser.add_option("--channel", metavar = "name", default = [], action = "append", help = "Set the name of the channel to process (optional).  The default is \"LSC-STRAIN\" for all detectors. Override with IFO=CHANNEL-NAME can be given multiple times")
	parser.add_option("--time-slide-file", metavar = "filename", help = "Set the time slide table xml file")
	parser.add_option("--cluster-sql-file", metavar = "filename", help = "Set the sql file to apply to noninjection databases")
	parser.add_option("--injection-sql-file", metavar = "filename", help = "Set the sql file to apply to injection databases")
	parser.add_option("--web-dir", metavar = "directory", help = "Set the web directory like /home/USER/public_html")
	parser.add_option("--fir-stride", type="int", metavar = "secs", default = 8, help = "Set the duration of the fft output blocks, default 8")
	parser.add_option("--control-peak-time", type="int", default = 8, metavar = "secs", help = "Set the peak finding time for the control signal, default 8")
	parser.add_option("--do-iir-pipeline", action="store_true", help = "run the iir pipeline instead of lloid")
	parser.add_option("--num-banks", type="int", metavar = "banks", default = 1, help = "Set the number of template banks per job, default 1")

	options, filenames = parser.parse_args()

	fail = ""
	for option in ("frame_cache", "reference_psd", "bank_cache", "gps_start_time", "gps_stop_time"):
		if getattr(options, option) is None:
			fail += "must provide option %s\n" % (option)
	if fail: raise ValueError, fail

	#FIXME add consistency check?
	bankcache = parse_cache_str(options.bank_cache)
	channel_dict = inspiral.channel_dict_from_channel_list(options.channel)

	return options, filenames, bankcache, channel_dict


#
# MAIN
#

options, filenames, bank_cache, channel_dict = parse_command_line()
# FIXME use glue function
instruments = "".join(sorted(bank_cache.keys()))

try: os.mkdir("logs")
except: pass
dag = inspiral_pipe.DAG("trigger_pipe")

#
# setup the job classes
#

if options.do_iir_pipeline is not None:
	gstlalInspiralJob = gstlal_inspiral_job(executable=inspiral_pipe.which('gstlal_iir_inspiral'))
	gstlalInspiralInjJob = gstlal_inspiral_inj_job(executable=inspiral_pipe.which('gstlal_iir_inspiral'))
else:
	gstlalInspiralJob = gstlal_inspiral_job()
	gstlalInspiralInjJob = gstlal_inspiral_inj_job()
calcLikelihoodJob = gstlal_inspiral_calc_likelihood_job()
gstlalInspiralComputeFarFromSnrChisqHistogramsJobFAP = gstlal_compute_far_from_snr_chisq_histograms_jobFAP()
gstlalInspiralComputeFarFromSnrChisqHistogramsJobFAR = gstlal_compute_far_from_snr_chisq_histograms_jobFAR()
ligolwInspinjFindJob = ligolw_inspinjfind_job()
toSqliteJob = ligolw_sqlite_from_xml_job()
toXMLJob = ligolw_sqlite_to_xml_job()
lalappsRunSqliteJob = lalapps_run_sqlite_job()
plotJob = gstlal_inspiral_plotsummary_job()
openpageJob = gstlal_s5_pbh_summary_page_job(tag_base = 'gstlal_s5_pbh_summary_page_open')
pageJob = gstlal_s5_pbh_summary_page_job()

noninj_nodes = []
inj_nodes = []
cluster_nodes = []

###############################################################################
# loop over banks to run gstlal inspiral pre clustering and far computation
###############################################################################

for s, trials_factor in inspiral_pipe.build_bank_string(bank_cache, [options.num_banks]):

	#
	# non injections
	#


	#
	# FIXME this is approximately correct, can we do better?
	#
	# Trials factors:
	#
	# The false alarm probability depends on how many coincidences are
	# formed.  The more you form the more chance you have to see an
	# excursion.  But the coincidences formed in any one gstlal_inspiral
	# job are correlated. We use the SVD to estimate the independent
	# degrees of freedom.  This is expressed as a fraction of the total
	# number of templates to get the degree of independence.  We then make
	# the simplifying assumption that each gstlal_inspiral job produces
	# roughly the same number of events.  Then the trials factor is
	#
	# trials factor = # of events in job \times # of jobs \times independence fraction.
	#
	# The far code already uses the number of events in the job to rank the
	# events.  Thus we just need to provide the factor # of jobs \times
	# independence fraction as an additional argument
	#

	far_input_nodes = []

	noninjnode = gstlal_inspiral_node(gstlalInspiralJob, dag, options.frame_cache, options.frame_segments_file, options.frame_segments_name, int(options.gps_start_time), int(options.gps_stop_time), channel_dict, reference_psd=options.reference_psd, svd_bank=s, injections=None, vetoes=options.vetoes, time_slide_file=options.time_slide_file, control_peak_time = options.control_peak_time, fir_stride = options.fir_stride)
	
	noninjnode_likelihood = gstlal_inspiral_calc_likelihood_node(calcLikelihoodJob, dag, noninjnode.background_name, input = [noninjnode.output_name], p_node=[noninjnode])

	# book keeping
	far_input_nodes.append(noninjnode)
	noninj_nodes.append(noninjnode)
	
	#
	# injections, if you want them
	#

	if options.injections is not None:
		injnode = gstlal_inspiral_node(gstlalInspiralInjJob, dag, options.frame_cache, options.frame_segments_file, options.frame_segments_name, int(options.gps_start_time), int(options.gps_stop_time), channel_dict, reference_psd=options.reference_psd, svd_bank=s, injections=options.injections, vetoes=options.vetoes, control_peak_time = options.control_peak_time, fir_stride = options.fir_stride)
	
		# FIXME the previous compute likelihood function already populates the injections, we should just use those
		injnode_likelihood = gstlal_inspiral_calc_likelihood_node(calcLikelihoodJob, dag, noninjnode_likelihood.background_name, input = [injnode.output_name], likelihood_output_name = "post_calc_likelihood_simulation", p_node=[noninjnode_likelihood, injnode])
		
		# book keeping
		far_input_nodes.append(injnode)
		inj_nodes.append(injnode)
		#
		# FAR nodes
		#
	
		farnode = gstlal_compute_far_from_snr_chisq_histograms_node(gstlalInspiralComputeFarFromSnrChisqHistogramsJobFAP, dag, noninjnode_likelihood.background_name, trials_factor, FAP = True, noninj_input = [noninjnode.output_name], inj_input = [injnode.output_name], p_node = [noninjnode_likelihood, injnode_likelihood])
	
	else:
		farnode = gstlal_compute_far_from_snr_chisq_histograms_node(gstlalInspiralComputeFarFromSnrChisqHistogramsJobFAP, dag, noninjnode_likelihood.background_name, trials_factor, FAP = True, noninj_input = [noninjnode.output_name], inj_input = [], p_node = [noninjnode_likelihood])

#
	# Pre clustering
	#

	for f in far_input_nodes:
		if f.injections:
			# cluster injections and drop the sim inspiral table
			clusternode = lalapps_run_sqlite_node(lalappsRunSqliteJob, dag, options.injection_sql_file, input=[f.output_name], p_node=[farnode])
		else:
			# cluster non injection files
			clusternode = lalapps_run_sqlite_node(lalappsRunSqliteJob, dag, options.cluster_sql_file, input=[f.output_name], p_node=[farnode])
		clusternode = ligolw_sqlite_node(toXMLJob, dag, f.output_name, replace=False, extract=f.output_name.replace('.sqlite','.xml.gz'), p_node=[clusternode])
		cluster_nodes.append(clusternode)

###############################################################################
# after all of the FAR ranking is finished put everything into a database
###############################################################################

#
# non injection DB
#

noninjdb = "%s-ALL-%d-%d-LLOID.sqlite" % (instruments, options.gps_start_time, options.gps_stop_time)
# merge
sqlitenode = ligolw_sqlite_node(toSqliteJob, dag, noninjdb, input=[os.path.split(f.output_name)[1].replace('.sqlite','.xml.gz') for f in noninj_nodes] + [options.vetoes, options.frame_segments_file], p_node=cluster_nodes)
# cluster
noninjsqlitenode = lalapps_run_sqlite_node(lalappsRunSqliteJob, dag, options.cluster_sql_file, input=[noninjdb], p_node=[sqlitenode])

#
# injection DB
# FIXME someday support more than one injection run I guess
#

if inj_nodes:
	injdb = "%s-ALL-%d-%d-LLOID-simulations.sqlite" % (instruments, options.gps_start_time, options.gps_stop_time)
	injxml = injdb+".xml.gz"
	# merge
	sqlitenode = ligolw_sqlite_node(toSqliteJob, dag, injdb, input=[os.path.split(f.output_name)[1].replace('.sqlite','.xml.gz') for f in inj_nodes] + [options.vetoes, options.frame_segments_file, options.injections], p_node=cluster_nodes)
	# cluster
	clusternode = lalapps_run_sqlite_node(lalappsRunSqliteJob, dag, options.cluster_sql_file, input=[injdb], p_node=[sqlitenode])
	# convert to XML
	clusternode = ligolw_sqlite_node(toXMLJob, dag, injdb, replace=False, extract=injxml, p_node=[clusternode])
	# find injections
	inspinjnode = ligolw_inspinjfind_node(ligolwInspinjFindJob, dag, injxml, p_node=[clusternode])
	# convert back to sqlite
	sqlitenode = ligolw_sqlite_node(toSqliteJob, dag, injdb, input=[injxml], p_node=[inspinjnode])
	farnode = gstlal_compute_far_from_snr_chisq_histograms_node(gstlalInspiralComputeFarFromSnrChisqHistogramsJobFAR, dag, None, 1, FAR = True, noninj_input = [noninjdb], inj_input = [injdb], p_node = [sqlitenode, noninjsqlitenode])
	# compute FARs
	name_tag = "plots/sub-solar-mass-%d-%d_" % (options.gps_start_time, options.gps_stop_time)
	# make plots
	plotnode = gstlal_inspiral_plotsummary_node(plotJob, dag, name_tag, input=[noninjdb, injdb], p_node=[farnode])
	# make a web page
	gstlal_s5_pbh_summary_page_node(pageJob, dag, name_tag, options.web_dir, title="Sub-solar-mass-%d-%d-closed-box" % (options.gps_start_time, options.gps_stop_time), open_box = False, p_node=[plotnode])
	gstlal_s5_pbh_summary_page_node(openpageJob, dag, name_tag, options.web_dir, title="Sub-solar-mass-%d-%d-open-box" % (options.gps_start_time, options.gps_stop_time), p_node=[plotnode])
else:
	# compute FARs without injections
	farnode = gstlal_compute_far_from_snr_chisq_histograms_node(gstlalInspiralComputeFarFromSnrChisqHistogramsJobFAR, dag, None, 1, FAR = True, noninj_input = [noninjdb], inj_input = [], p_node = [noninjsqlitenode])

#
# all done
#

dag.write_sub_files()
dag.write_dag()
dag.write_script()
dag.write_cache()
