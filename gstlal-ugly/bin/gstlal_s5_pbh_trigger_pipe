#!/usr/bin/python
"""
This program makes a dag to run gstlal_inspiral offline
"""

__author__ = 'Chad Hanna <channa@caltech.edu>'

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, copy, math
import subprocess, socket, tempfile

##############################################################################
# import the modules we need to build the pipeline
from glue import iterutils
from glue import pipeline
from glue import lal
from glue.ligolw import lsctables
from glue import segments
from glue.ligolw import array
from glue.ligolw import param
import glue.ligolw.utils as utils
import glue.ligolw.utils.segments as ligolw_segments
from optparse import OptionParser
from gstlal.svd_bank import read_bank
from gstlal import inspiral, inspiral_pipe
import numpy
from pylal.datatypes import LIGOTimeGPS

#
# Generic job classes
#


class InspiralJob(pipeline.CondorDAGJob):
	"""
	A generic job class for gstlal inspiral stuff
	"""
	def __init__(self, executable, tag_base):
		self.__prog__ = tag_base
		self.__executable = executable
		self.__universe = 'vanilla'
		pipeline.CondorDAGJob.__init__(self, self.__universe, self.__executable)
		self.add_condor_cmd('getenv','True')
		self.tag_base = tag_base
		self.set_sub_file(tag_base+'.sub')
		self.set_stdout_file('logs/'+tag_base+'-$(macroid)-$(macronodename)-$(cluster)-$(process).out')
		self.set_stderr_file('logs/'+tag_base+'-$(macroid)-$(macronodename)-$(cluster)-$(process).err')
		self.number = 1


class InspiralNode(pipeline.CondorDAGNode):
	"""
	A generic node class for gstlal inspiral stuff
	"""
	def __init__(self, job, dag, p_node=[]):
		pipeline.CondorDAGNode.__init__(self, job)
		for p in p_node:
			self.add_parent(p)
		dag.add_node(self)


#
# gstlal_s5_pbh_summary_page
#


class gstlal_s5_pbh_summary_page_job(InspiralJob):
	"""
	A gstlal_s5_pbh_summary_page job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_s5_pbh_summary_page'), tag_base='gstlal_s5_pbh_summary_page'):
		InspiralJob.__init__(self, executable, tag_base)


class gstlal_s5_pbh_summary_page_node(InspiralNode):
	"""
	A gstlal_s5_pbh_summary_page_node
	"""
	def __init__(self, job, dag, name_tag, web_dir, title, open_box=True, p_node=[]):
		InspiralNode.__init__(self, job, dag, p_node)
		self.add_var_opt("output-name-tag", name_tag)
		self.add_var_opt("webserver-dir", web_dir)
		self.add_var_opt("title", title)
		if open_box: self.add_var_opt("open-box", "")


#
# gstlal_inspiral_plotsummary
#


class gstlal_inspiral_plotsummary_job(InspiralJob):
	"""
	A gstlal_inspiral_plotsummary_job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_inspiral_plotsummary'), tag_base='gstlal_inspiral_plotsummary'):
		InspiralJob.__init__(self, executable, tag_base)


class gstlal_inspiral_plotsummary_node(InspiralNode):
	"""
	A gstlal_inspiral_plotsummary_node
	"""
	def __init__(self, job, dag, base, input=[], tmp_space=inspiral_pipe.log_path(), p_node=[]):
		InspiralNode.__init__(self, job, dag, p_node)
		self.add_var_opt("base", base)
		self.add_var_opt("tmp-space", tmp_space)
		for f in input:
			self.add_file_arg(f)


#
# lalapps_run_sqlite
#


class lalapps_run_sqlite_job(InspiralJob):
	"""
	A lalapps_run_sqlite
	"""
	def __init__(self, executable=inspiral_pipe.which('lalapps_run_sqlite'), tag_base='lalapps_run_sqlite'):
		InspiralJob.__init__(self, executable, tag_base)


class lalapps_run_sqlite_node(InspiralNode):
	"""
	A lalapps_run_sqlite node
	"""
	def __init__(self, job, dag, sql_file, input=[], tmp_space=inspiral_pipe.log_path(), p_node=[]):
		InspiralNode.__init__(self, job, dag, p_node)
		self.add_var_opt("sql-file", sql_file)
		self.add_var_opt("tmp-space", tmp_space)
		if len(input) == 1:
			self.output_name = input[0]
		for f in input:
			self.add_file_arg(f)


#
# ligolw_sqlite
#


class ligolw_sqlite_from_xml_job(InspiralJob):
	"""
	A ligolw_sqlite_job
	"""
	def __init__(self, executable=inspiral_pipe.which('ligolw_sqlite'), tag_base='ligolw_sqlite_from_xml'):
		InspiralJob.__init__(self, executable, tag_base)


class ligolw_sqlite_to_xml_job(InspiralJob):
	"""
	A ligolw_sqlite_job
	"""
	def __init__(self, executable=inspiral_pipe.which('ligolw_sqlite'), tag_base='ligolw_sqlite_to_xml'):
		InspiralJob.__init__(self, executable, tag_base)


class ligolw_sqlite_node(InspiralNode):
	"""
	A ligolw_sqlite node
	"""
	def __init__(self, job, dag, database, input=[], replace=True, tmp_space=inspiral_pipe.log_path(), extract=None, p_node=[]):
		InspiralNode.__init__(self, job, dag, p_node)
		if extract is not None:
			self.add_var_opt("extract", extract)
		self.add_var_opt("database", database)
		if replace:
			self.add_var_opt("replace", "")
		self.add_var_opt("tmp-space", tmp_space)
		for f in input:
			if f is not None:
				self.add_file_arg(f)
		self.output_db_name = database
		self.output_xml_name = extract


#
# ligolw_inspinjfind
#


class ligolw_inspinjfind_job(InspiralJob):
	"""
	A ligolw_inspinjfind_job
	"""
	def __init__(self, executable=inspiral_pipe.which('ligolw_inspinjfind'), tag_base='ligolw_inspinjfind'):
		InspiralJob.__init__(self, executable, tag_base)


class ligolw_inspinjfind_node(InspiralNode):
	"""
	A ligolw_inspinjfind node
	"""
	def __init__(self, job, dag, xml, p_node=[]):
		InspiralNode.__init__(self, job, dag, p_node)
		self.add_var_arg(xml)
		self.output_name = xml


#
# gstlal_inspiral
#


class gstlal_inspiral_job(InspiralJob):
	"""
	A gstlal_inspiral job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_inspiral'), tag_base='gstlal_inspiral'):
		InspiralJob.__init__(self, executable, tag_base)
		self.add_condor_cmd('requirements', '( CAN_RUN_MULTICORE )')
		self.add_condor_cmd('request_cpus', '6')
		self.add_condor_cmd('+RequiresMultipleCores', 'True')

class gstlal_inspiral_inj_job(InspiralJob):
	"""
	A gstlal_inspiral job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_inspiral'), tag_base='gstlal_inspiral_inj'):
		InspiralJob.__init__(self, executable, tag_base)
		self.add_condor_cmd('requirements', '( CAN_RUN_MULTICORE )')
		self.add_condor_cmd('request_cpus', '6')
		self.add_condor_cmd('+RequiresMultipleCores', 'True')


def sim_tag_from_inj_file(injections):
	return injections.replace('.xml', '').replace('.gz', '')


class gstlal_inspiral_node(InspiralNode):
	"""
	A gstlal_inspiral node
	"""
	#FIXME add veto segments name
	def __init__(self, job, dag, frame_cache, frame_segments_file, frame_segments_name, gps_start_time, gps_end_time, channel_dict, reference_psd, svd_bank, tmp_space=inspiral_pipe.log_path(), ht_gate_thresh=20.0, injections=None, control_peak_time = 8, vetoes=None, time_slide_file=None, fir_stride = 8, instruments = "H1H2L1", p_node=[]):
		InspiralNode.__init__(self, job, dag, p_node)

		if time_slide_file is not None:
			self.add_var_opt("time-slide-file", time_slide_file)
		self.add_var_opt("frame-cache", frame_cache)
		self.add_var_opt("frame-segments-file", frame_segments_file)
		self.add_var_opt("frame-segments-name", frame_segments_name)
		self.add_var_opt("gps-start-time",gps_start_time)
		self.add_var_opt("gps-end-time",gps_end_time)
		self.add_var_opt("channel-name", inspiral.pipeline_channel_list_from_channel_dict(channel_dict))
		self.add_var_opt("reference-psd", reference_psd)
		self.add_var_opt("svd-bank", svd_bank)
		self.add_var_opt("tmp-space", tmp_space)
		self.add_var_opt("track-psd", "")
		self.add_var_opt("control-peak-time", control_peak_time)
		self.add_var_opt("fir-stride", fir_stride)
		self.injections = injections
		if self.injections is not None:
			self.add_var_opt("injections", injections)
		if vetoes is not None:
			self.add_var_opt("veto-segments-file", vetoes)
		path = os.getcwd()
		svd_bank = os.path.split(svd_bank)[1].replace('.xml','')
		if self.injections is not None:
			sim_name = sim_tag_from_inj_file(self.injections)
			self.output_name = '%s/%s-%04d-%d-%d-LLOID-%s.sqlite' % (path, instruments, job.number, gps_start_time, gps_end_time, sim_name)
		else:
			self.output_name = '%s/%s-%04d-%d-%d-LLOID.sqlite' % (path, instruments, job.number, gps_start_time, gps_end_time)
			self.background_name = '%s/%s-%04d-%d-%d-LLOID_snr_chi.xml.gz' % (path, instruments, job.number, gps_start_time, gps_end_time)
		job.number += 1
		self.add_var_opt("output",self.output_name)
		dag.output_cache.append(lal.CacheEntry(instruments, "-", segments.segment(gps_start_time, gps_end_time), "file://localhost/%s" % (self.output_name,)))


#
# gstlal_compute_far_from_snr_chisq_histograms
#


class gstlal_inspiral_calc_likelihood_job(InspiralJob):
	"""
	A gstlal_inspiral_calc_likelihood job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_inspiral_calc_likelihood'), tag_base='gstlal_inspiral_calc_likelihood'):
		InspiralJob.__init__(self, executable, tag_base)


class gstlal_inspiral_calc_likelihood_node(InspiralNode):
	"""
	A gstlal_inspiral_calc_likelihood node
	"""
	def __init__(self, job, dag, likelihood_files = [], synthesize_injections = 1000000, input = [], likelihood_output_name = "post_calc_likelihood_", background_prior = 1.0, p_node=[]):
		InspiralNode.__init__(self, job, dag, p_node)
		self.add_var_opt("likelihood-file", pipeline_dot_py_append_opts_hack("likelihood-file", likelihood_files))
		self.add_var_opt("tmp-space", inspiral_pipe.log_path())
		for f in input:
			self.add_file_arg(f)
		fpath = os.path.split(likelihood_files[0])
		self.background_name = os.path.join(fpath[0], likelihood_output_name + fpath[1])
		self.add_var_opt("write-likelihood", self.background_name)
		self.add_var_opt("background-prior", background_prior)
		self.add_var_opt("synthesize-injections", synthesize_injections)
		self.output_names = input
		if len(input) == 1:
			self.output_name = input[0]


#
# gstlal_compute_far_from_snr_chisq_histograms
#


class gstlal_compute_far_from_snr_chisq_histograms_jobFAP(InspiralJob):
	"""
	A gstlal_compute_far_from_snr_chisq_histograms job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_compute_far_from_snr_chisq_histograms'), tag_base='gstlal_compute_far_from_snr_chisq_histogramsFAP'):
		InspiralJob.__init__(self, executable, tag_base)


class gstlal_compute_far_from_snr_chisq_histograms_jobFAR(InspiralJob):
	"""
	A gstlal_compute_far_from_snr_chisq_histograms job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_compute_far_from_snr_chisq_histograms'), tag_base='gstlal_compute_far_from_snr_chisq_histogramsFAR'):
		InspiralJob.__init__(self, executable, tag_base)


class gstlal_compute_far_from_snr_chisq_histograms_node(InspiralNode):
	"""
	A gstlal_compute_far_from_snr_chisq_histograms_job node
	"""
	def __init__(self, job, dag, background_bins_files = [], additional_trials_factor = 1, FAR = False, FAP = False, noninj_input = [], inj_input = [], p_node=[]):
		InspiralNode.__init__(self, job, dag, p_node)
		if background_bins_files is not None:
			self.add_var_opt("background-bins-file", pipeline_dot_py_append_opts_hack("background-bins-file", background_bins_files))
		self.add_var_opt("additional-trials-factor", additional_trials_factor)
		self.add_var_opt("tmp-space", inspiral_pipe.log_path())
		if FAP:
			self.add_var_opt("compute-fap", "")
		if FAR:
			self.add_var_opt("compute-far", "")
		if len(inj_input) > 0:
			self.add_var_opt("injection-dbs", pipeline_dot_py_append_opts_hack("injection-dbs", inj_input))
		self.add_var_opt("non-injection-db", pipeline_dot_py_append_opts_hack("non-injection-db", noninj_input))
		self.output_names = inj_input + noninj_input


#
# Utility functions
#


# FIXME surely this is in glue
def parse_cache_str(instr):
	dictcache = {}
	if instr is None: return dictcache
	for c in instr.split(','):
		ifo = c.split("=")[0]
		cache = c.replace(ifo+"=","")
		dictcache[ifo] = cache
	return dictcache

def pipeline_dot_py_append_opts_hack(opt, vals):
	out = vals[0]
	for v in vals[1:]:
		out += " --%s %s" % (opt, v)
	return out

def extract_all_nodes_by_inj(nodes, inj):
	out = []
	for v in nodes.values():
		out.extend([n[1] for n in v if n[0] == inj])
	return out

def parse_command_line():
	parser = OptionParser(description = __doc__)
	parser.add_option("--injections", metavar = "filename", action = "append", help = "Set the name of the xml file for injections, can be called multiple times")
	parser.add_option("--frame-cache", metavar = "filename", help = "Set the frame cache file")
	parser.add_option("--frame-segments-file", metavar = "filename", help = "Set the frame segments file")
	parser.add_option("--frame-segments-name", metavar = "name", help = "Set the frame segments name")
	parser.add_option("--reference-psd", metavar = "filename", help = "Set the reference psd file.")
	parser.add_option("--bank-cache", metavar = "filenames", help = "Set the bank cache files in format H1=H1.cache,H2=H2.cache, etc..")
	parser.add_option("--vetoes", metavar = "filename", help = "Set the veto xml file.")
	parser.add_option("--gps-start-time", metavar = "GPS", help = "Set the gps start time in seconds", type="int")
	parser.add_option("--gps-stop-time", metavar = "GPS", help = "Set the gps stop time in seconds", type="int")
	parser.add_option("--channel", metavar = "name", default = [], action = "append", help = "Set the name of the channel to process (optional).  The default is \"LSC-STRAIN\" for all detectors. Override with IFO=CHANNEL-NAME can be given multiple times")
	parser.add_option("--time-slide-file", metavar = "filename", help = "Set the time slide table xml file")
	parser.add_option("--cluster-sql-file", metavar = "filename", help = "Set the sql file to apply to noninjection databases")
	parser.add_option("--injection-sql-file", metavar = "filename", help = "Set the sql file to apply to injection databases")
	parser.add_option("--web-dir", metavar = "directory", help = "Set the web directory like /home/USER/public_html")
	parser.add_option("--fir-stride", type="int", metavar = "secs", default = 8, help = "Set the duration of the fft output blocks, default 8")
	parser.add_option("--control-peak-time", type="int", default = 8, metavar = "secs", help = "Set the peak finding time for the control signal, default 8")
	parser.add_option("--do-iir-pipeline", action="store_true", help = "run the iir pipeline instead of lloid")
	parser.add_option("--num-banks", type="int", metavar = "banks", default = 1, help = "Set the number of template banks per job, default 1")
	parser.add_option("--verbose", action = "store_true", help = "Be verbose")

	options, filenames = parser.parse_args()

	fail = ""
	for option in ("frame_cache", "reference_psd", "bank_cache", "gps_start_time", "gps_stop_time"):
		if getattr(options, option) is None:
			fail += "must provide option %s\n" % (option)
	if fail: raise ValueError, fail

	#FIXME add consistency check?
	bankcache = parse_cache_str(options.bank_cache)
	channel_dict = inspiral.channel_dict_from_channel_list(options.channel)

	return options, filenames, bankcache, channel_dict


#
# MAIN
#

options, filenames, bank_cache, channel_dict = parse_command_line()
# FIXME use glue function
instruments = "".join(sorted(bank_cache.keys()))

#
# Setup analysis segments
#

segsdoc = utils.load_filename(options.frame_segments_file, verbose = options.verbose)

# union of all single detector segments
segs = ligolw_segments.segmenttable_get_by_name(segsdoc, options.frame_segments_name).union(bank_cache.keys()).coalesce()
# get the requested start / stop boundary
boundary_seg = segments.segmentlist([segments.segment(LIGOTimeGPS(options.gps_start_time), LIGOTimeGPS(options.gps_stop_time))])
# intersect so we only analyze segments in the requested time 
segs &= boundary_seg

try: os.mkdir("logs")
except: pass
dag = inspiral_pipe.DAG("trigger_pipe")

#
# setup the job classes
#

if options.do_iir_pipeline is not None:
	gstlalInspiralJob = gstlal_inspiral_job(executable=inspiral_pipe.which('gstlal_iir_inspiral'))
	gstlalInspiralInjJob = gstlal_inspiral_inj_job(executable=inspiral_pipe.which('gstlal_iir_inspiral'))
else:
	gstlalInspiralJob = gstlal_inspiral_job()
	gstlalInspiralInjJob = gstlal_inspiral_inj_job()
calcLikelihoodJob = gstlal_inspiral_calc_likelihood_job()
gstlalInspiralComputeFarFromSnrChisqHistogramsJobFAP = gstlal_compute_far_from_snr_chisq_histograms_jobFAP()
gstlalInspiralComputeFarFromSnrChisqHistogramsJobFAR = gstlal_compute_far_from_snr_chisq_histograms_jobFAR()
ligolwInspinjFindJob = ligolw_inspinjfind_job()
toSqliteJob = ligolw_sqlite_from_xml_job()
toXMLJob = ligolw_sqlite_to_xml_job()
lalappsRunSqliteJob = lalapps_run_sqlite_job()
plotJob = gstlal_inspiral_plotsummary_job()
openpageJob = gstlal_s5_pbh_summary_page_job(tag_base = 'gstlal_s5_pbh_summary_page_open')
pageJob = gstlal_s5_pbh_summary_page_job()

inspiral_nodes = {}
likelihood_nodes = {}
clusternodes = {}

#
# loop over banks to run gstlal inspiral pre clustering and far computation
#

for s, trials_factor in inspiral_pipe.build_bank_string(bank_cache, [options.num_banks]):

	#
	# non injections
	#

	inspiral_nodes[s] = []
	clusternodes[s] = []
	likelihood_nodes[s] = []

	# inspiral jobs by segment
	for seg in segs:
		noninjnode = gstlal_inspiral_node(gstlalInspiralJob, dag, options.frame_cache, options.frame_segments_file, options.frame_segments_name, seg[0].seconds, seg[1].seconds, channel_dict, reference_psd=options.reference_psd, svd_bank=s, injections=None, vetoes=options.vetoes, time_slide_file=options.time_slide_file, control_peak_time = options.control_peak_time, fir_stride = options.fir_stride)
		inspiral_nodes[s].append((None, noninjnode))

	# likelihood jobs
	noninjnode_likelihood = gstlal_inspiral_calc_likelihood_node(calcLikelihoodJob, dag, likelihood_files = [node[1].background_name for node in inspiral_nodes[s] if node[0] is None], input = [node[1].output_name for node in inspiral_nodes[s] if node[0] is None], p_node=[node[1] for node in inspiral_nodes[s] if node[0] is None])
	likelihood_nodes[s].append((None, noninjnode_likelihood))
	
	#
	# injections
	#

	for injections in options.injections:
		# inspiral jobs by segment
		for seg in segs:
			injnode = gstlal_inspiral_node(gstlalInspiralInjJob, dag, options.frame_cache, options.frame_segments_file, options.frame_segments_name, seg[0].seconds, seg[1].seconds, channel_dict, reference_psd=options.reference_psd, svd_bank=s, injections=injections, vetoes=options.vetoes, control_peak_time = options.control_peak_time, fir_stride = options.fir_stride)
			inspiral_nodes[s].append((injections, injnode))	

		# Likelihood nodes
		
		injnode_likelihood = gstlal_inspiral_calc_likelihood_node(calcLikelihoodJob, dag, likelihood_files = [node[1].background_name for node in inspiral_nodes[s] if node[0] is None], input = [node[1].output_name for node in inspiral_nodes[s] if node[0] == injections], likelihood_output_name = "post_calc_likelihood_simulation", p_node = [node[1] for node in inspiral_nodes[s] if node[0] in (injections, None)])
		
		likelihood_nodes[s].append((injections, injnode_likelihood))
	
	# FIXME HERE DOWN
	farnode = gstlal_compute_far_from_snr_chisq_histograms_node(gstlalInspiralComputeFarFromSnrChisqHistogramsJobFAP, dag, background_bins_files = [node[1].background_name for node in likelihood_nodes[s] if node[0] is None], additional_trials_factor = trials_factor, FAP = True, noninj_input = [v[1].output_name for v in inspiral_nodes[s] if v[0] is None], inj_input = [v[1].output_name for v in inspiral_nodes[s] if v[0] is not None], p_node = [v[1] for v in likelihood_nodes[s]])

	#
	# Pre clustering
	#

	for (inj, node) in inspiral_nodes[s]:
		if inj is not None:
			# cluster injections and drop the sim inspiral table
			clusternode = lalapps_run_sqlite_node(lalappsRunSqliteJob, dag, options.injection_sql_file, input=[node.output_name], p_node=[farnode])
		else:
			# cluster non injection files
			clusternode = lalapps_run_sqlite_node(lalappsRunSqliteJob, dag, options.cluster_sql_file, input=[node.output_name], p_node=[farnode])
		clusternode = ligolw_sqlite_node(toXMLJob, dag, node.output_name, replace=False, extract=node.output_name.replace('.sqlite','.xml.gz'), p_node=[clusternode])
		clusternodes[s].append((inj, clusternode))

#
# after all of the FAR ranking is finished put everything into single databases based on the injection file (or lack thereof)
#

#
# non injection DB
#

# extract the non injectio nodes
noninj_nodes = extract_all_nodes_by_inj(clusternodes, None) # not an injection

# setup the final output names
noninjdb = "%s-ALL-%d-%d-LLOID.sqlite" % (instruments, options.gps_start_time, options.gps_stop_time)

# merge
sqlitenode = ligolw_sqlite_node(toSqliteJob, dag, noninjdb, input=[os.path.split(f.output_xml_name)[1] for f in noninj_nodes] + [options.vetoes, options.frame_segments_file], p_node=noninj_nodes)

# cluster
noninjsqlitenode = lalapps_run_sqlite_node(lalappsRunSqliteJob, dag, options.cluster_sql_file, input=[noninjdb], p_node=[sqlitenode])

#
# injection DBs
#

injdbs = []
p_nodes = [noninjsqlitenode]
for injections in options.injections:

	# Setup the final output names, etc.
	injdb = "%s-ALL-%d-%d-LLOID-%s.sqlite" % (instruments, options.gps_start_time, options.gps_stop_time, sim_tag_from_inj_file(injections))
	injdbs.append(injdb)
	injxml = injdb+".xml.gz"
	
	# extract only the ndoes that were used for injections
	inj_nodes = extract_all_nodes_by_inj(clusternodes, injections)

	# merge
	sqlitenode = ligolw_sqlite_node(toSqliteJob, dag, injdb, input=[os.path.split(f.output_xml_name)[1] for f in inj_nodes] + [options.vetoes, options.frame_segments_file, injections], p_node=inj_nodes)

	# cluster
	clusternode = lalapps_run_sqlite_node(lalappsRunSqliteJob, dag, options.cluster_sql_file, input=[injdb], p_node=[sqlitenode])

	# convert to XML
	clusternode = ligolw_sqlite_node(toXMLJob, dag, injdb, replace=False, extract=injxml, p_node=[clusternode])

	# find injections
	inspinjnode = ligolw_inspinjfind_node(ligolwInspinjFindJob, dag, injxml, p_node=[clusternode])

	# convert back to sqlite
	sqlitenode = ligolw_sqlite_node(toSqliteJob, dag, injdb, input=[injxml], p_node=[inspinjnode])
	p_nodes.append(sqlitenode)

# compute FARs
farnode = gstlal_compute_far_from_snr_chisq_histograms_node(gstlalInspiralComputeFarFromSnrChisqHistogramsJobFAR, dag, None, additional_trials_factor = 1, FAR = True, noninj_input = [noninjdb], inj_input = injdbs, p_node = p_nodes)

# make plots
name_tag = "plots/sub-solar-mass-%d-%d_" % (options.gps_start_time, options.gps_stop_time)
plotnode = gstlal_inspiral_plotsummary_node(plotJob, dag, name_tag, input=[noninjdb] + injdbs, p_node=[farnode])

# make a web page
gstlal_s5_pbh_summary_page_node(pageJob, dag, name_tag, options.web_dir, title="Sub-solar-mass-%d-%d-closed-box" % (options.gps_start_time, options.gps_stop_time), open_box = False, p_node=[plotnode])
gstlal_s5_pbh_summary_page_node(openpageJob, dag, name_tag, options.web_dir, title="Sub-solar-mass-%d-%d-open-box" % (options.gps_start_time, options.gps_stop_time), p_node=[plotnode])

#
# all done
#

dag.write_sub_files()
dag.write_dag()
dag.write_script()
dag.write_cache()
