#!/usr/bin/python
"""
This program makes a dag for ER1
"""

__author__ = 'Chad Hanna <channa@caltech.edu>'

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, copy, math
import subprocess, socket, tempfile, shutil

##############################################################################
# import the modules we need to build the pipeline
from glue import iterutils
from glue import pipeline
from glue import lal
from glue.ligolw import lsctables
from glue import segments
from glue.ligolw import array
import glue.ligolw.utils as utils
import glue.ligolw.utils.segments as ligolw_segments
from optparse import OptionParser
from gstlal.svd_bank import read_bank
from gstlal import inspiral
from gstlal import inspiral_pipe
import numpy


#
# gstlal_inspiral
#


class gstlal_inspiral_job(pipeline.CondorDAGJob):
	"""
	A gstlal_inspiral job
	"""
	def __init__(self, executable=inspiral_pipe.which('gstlal_ll_inspiral'), tag_base='gstlal_inspiral'):
		"""
		"""
		self.__prog__ = 'gstlal_inspiral'
		self.__executable = executable
		self.__universe = 'vanilla'
		pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
		self.add_condor_cmd('getenv','True')
		# these jobs gracefully shutdown with SIGINT
		self.add_condor_cmd("remove_kill_sig", "15")
		self.add_condor_cmd("kill_sig", "15")
		self.add_condor_cmd('+Online_CBC_GSTLAL', 'True')
		self.tag_base = tag_base
		self.set_sub_file(tag_base+'.sub')
		self.set_stdout_file('logs/'+tag_base+'-$(macroid)-$(macronodename)-$(cluster)-$(process).out')
		self.set_stderr_file('logs/'+tag_base+'-$(macroid)-$(macronodename)-$(cluster)-$(process).err')
		self.number = 1

class gstlal_inspiral_node(pipeline.CondorDAGNode):
	"""
	A gstlal_inspiral node
	"""
	#FIXME add frame segments, name and veto segments name
	def __init__(self, job, dag, channel_dict, reference_psd, svd_bank, tmp_space=inspiral_pipe.log_path(), ht_gate_thresh=10.0, control_peak_time = 5, fir_stride = 5, thinca_interval = 10, likelihood_file = None, trials_factor = 1, gracedb_far_threshold = None, p_node=[]):

		pipeline.CondorDAGNode.__init__(self,job)
		self.add_var_opt("channel-name", inspiral.pipeline_channel_list_from_channel_dict(channel_dict))
		if reference_psd is not None:
			self.add_var_opt("reference-psd", reference_psd)
		self.add_var_opt("trials-factor", trials_factor)
		self.add_var_opt("svd-bank", svd_bank)
		self.add_var_opt("tmp-space", tmp_space)
		self.add_var_opt("track-psd", "")
		self.add_var_opt("control-peak-time", control_peak_time)
		self.add_var_opt("fir-stride", fir_stride)
		self.add_var_opt("thinca-interval", thinca_interval)
		if ht_gate_thresh is not None:
			self.add_var_opt("ht-gate-threshold", ht_gate_thresh)
		if gracedb_far_threshold is not None:
			self.add_var_opt("gracedb-far-threshold", gracedb_far_threshold)
		self.add_var_opt("verbose", "") #Put this in for debugging
		likefile = os.path.split(likelihood_file)[1]
		path = os.getcwd()
		# make a new likelihood file
		likefile = "%s/%d-%s" % (path, job.number, likefile)
		shutil.copyfile(likelihood_file, likefile)
		self.add_var_opt("likelihood-file", likefile)
		svd_bank = os.path.split(svd_bank)[1].replace('.xml','')
		self.output_name = '%s/%s-%04d-LLOID.sqlite' % (path, "H1H2L1V1", job.number)
		self.background_name = '%s/%s-%04d-LLOID_snr_chi.xml.gz' % (path, "H1H2L1V1", job.number)
		job.number += 1
		self.add_var_opt("output",self.output_name)
		dag.output_cache.append(lal.CacheEntry("H1H2L1V1", "-", segments.segment(0, 2000000000), "file://localhost/%s" % (self.output_name,)))
		for p in p_node:
			self.add_parent(p)
		dag.add_node(self)


#
# Parse the command line
#


def parse_command_line():
	parser = OptionParser(description = __doc__)
	parser.add_option("--reference-psd", metavar = "filename", help = "Set the reference psd file.")
	parser.add_option("--bank-cache", metavar = "filenames", help = "Set the bank cache files in format H1=H1.cache,H2=H2.cache, etc..")
	parser.add_option("--channel", metavar = "name", default=[], action = "append", help = "Set the name of the channel to process (optional).  The default is \"LSC-STRAIN\" for all detectors. Override with IFO=CHANNEL-NAME can be given multiple times")
	parser.add_option("--ht-gate-threshold", metavar = "float", help = "Set the h(t) gate threshold to reject glitches", type="float")
	parser.add_option("--num-banks", metavar = "str", help = "the number of banks per job. can be given as a list like 1,2,3,4 then it will split up the bank cache into N groups with M banks each.")
	parser.add_option("--max-jobs", metavar = "num", type = "int", help = "stop parsing the cache after reaching a certain number of jobs")
	parser.add_option("--likelihood-file", help = "set the likelihood file, required")	
	parser.add_option("--control-peak-time", default = 5, metavar = "secs", help = "set the control peak time, default 5")
	parser.add_option("--fir-stride", default = 5, metavar = "secs", help = "set the fir bank stride, default 5")
	parser.add_option("--thinca-interval", default = 10, metavar = "secs", help = "set the thinca interval, default 10")
	parser.add_option("--gracedb-far-threshold", type = "float", help = "false alarm rate threshold for gracedb (Hz), if not given gracedb events are not sent")
	options, filenames = parser.parse_args()
	options.num_banks = [int(v) for v in options.num_banks.split(",")]

	fail = ""
	for option in ("bank_cache", "likelihood_file"):
		if getattr(options, option) is None:
			fail += "must provide option %s\n" % (option)
	if fail: raise ValueError, fail

	#FIXME add consistency check?
	bankcache = inspiral_pipe.parse_cache_str(options.bank_cache)
	channel_dict = inspiral.channel_dict_from_channel_list(options.channel)
	
	return options, filenames, bankcache, channel_dict


###############################################################################
# MAIN
###############################################################################

options, filenames, bank_cache, channel_dict = parse_command_line()

try: os.mkdir("logs")
except: pass
dag = inspiral_pipe.DAG("trigger_pipe")

#
# setup the job classes
#

gstlalInspiralJob = gstlal_inspiral_job()

###############################################################################
# loop over banks to run gstlal inspiral pre clustering and far computation
###############################################################################
num_jobs = inspiral_pipe.num_bank_files(bank_cache)
if options.max_jobs < num_jobs:
	num_jobs = options.max_jobs

indfac = 1./7. # conservative guess from SVD paper, this is slow to calculate otherwise #inspiral_pipe.get_independence_factor(bank_cache, maxjobs = num_jobs)

trials_factor = int(numpy.ceil(num_jobs * indfac))

for s in inspiral_pipe.build_bank_string(bank_cache, options.num_banks, options.max_jobs):
	gstlal_inspiral_node(gstlalInspiralJob, dag, channel_dict, reference_psd=options.reference_psd, svd_bank=s, ht_gate_thresh = options.ht_gate_threshold, fir_stride = options.fir_stride, thinca_interval = options.thinca_interval, likelihood_file = options.likelihood_file, control_peak_time = options.control_peak_time, trials_factor = trials_factor, gracedb_far_threshold = options.gracedb_far_threshold)


dag.write_sub_files()
dag.write_dag()
dag.write_script()
dag.write_cache()
