#!/usr/bin/python
"""
This program makes a dag to generate svd banks
"""

__author__ = 'Chad Hanna <channa@caltech.edu>'

##############################################################################
# import standard modules and append the lalapps prefix to the python path
import sys, os, copy, math
import subprocess, socket, tempfile

##############################################################################
# import the modules we need to build the pipeline
from glue import iterutils
from glue import pipeline
from glue import lal
from glue.ligolw import lsctables
from glue import segments
from optparse import OptionParser

def which(prog):
	which = subprocess.Popen(['which',prog], stdout=subprocess.PIPE)
	out = which.stdout.read().strip()
	if not out: 
		print >>sys.stderr, "ERROR: could not find %s in your path, have you built the proper software and source the proper env. scripts?" % (prog,prog)
		raise ValueError 
	return out

def log_path():
	host = socket.getfqdn()
	#FIXME add more hosts as you need them
	if 'caltech.edu' in host: return '/usr1/' + os.environ['USER']
	if 'phys.uwm.edu' in host: return '/localscratch/' + os.environ['USER']
	if 'aei.uni-hannover.de' in host: return '/local/user/' + os.environ['USER']
	if 'phy.syr.edu' in host: return '/usr1/' + os.environ['USER']


class bank_DAG(pipeline.CondorDAG):

	def __init__(self, name, logpath = log_path()):
		self.basename = name
		tempfile.tempdir = logpath
		tempfile.template = self.basename + '.dag.log.'
		logfile = tempfile.mktemp()
		fh = open( logfile, "w" )
		fh.close()
		pipeline.CondorDAG.__init__(self,logfile)
		self.set_dag_file(self.basename)
		self.jobsDict = {}
		self.node_id = 0
		self.output_cache = []

	def add_node(self, node):
		node.set_retry(3)
		self.node_id += 1
		node.add_macro("macroid", self.node_id)
		pipeline.CondorDAG.add_node(self, node)

	def write_cache(self):
		out = self.basename + ".cache"
		f = open(out,"w")
		for c in self.output_cache:
			f.write(str(c)+"\n")
		f.close()

class gstlal_svd_bank_job(pipeline.CondorDAGJob):
	"""
	A gstlal_svd_bank job
	"""
	def __init__(self, executable=which('gstlal_svd_bank'), tag_base='gstlal_svd_bank'):
		"""
		"""
		self.__prog__ = 'gstlal_svd_bank'
		self.__executable = executable
		self.__universe = 'vanilla'
		pipeline.CondorDAGJob.__init__(self,self.__universe,self.__executable)
		self.add_condor_cmd('getenv','True')
		self.add_condor_cmd('requirements', 'Memory > 1999') #FIXME is this enough?
		self.tag_base = tag_base
		self.add_condor_cmd('environment',"KMP_LIBRARY=serial;MKL_SERIAL=yes")
		self.set_sub_file(tag_base+'.sub')
		self.set_stdout_file('logs/'+tag_base+'-$(macroid)-$(process).out')
		self.set_stderr_file('logs/'+tag_base+'-$(macroid)-$(process).err')


class gstlal_svd_bank_node(pipeline.CondorDAGNode):
	"""
	"""
	def __init__(self, job, dag, template_bank, ifo, flow = 40, reference_psd = "psd.xml", tolerance = 0.9995, FAP = 0.5, snr = 4.0, clipleft = 0, clipright = 0, samples_min = 1024, samples_max_256 = 1024, samples_max_64 =  2048, samples_max = 4096, p_node=[]):

		pipeline.CondorDAGNode.__init__(self,job)
		self.add_var_opt("flow", flow)
		self.add_var_opt("snr-threshold", snr)
		self.add_var_opt("svd-tolerance", tolerance)
		self.add_var_opt("reference-psd", reference_psd)
		self.add_var_opt("template-bank", template_bank)
		self.add_var_opt("ortho-gate-fap", FAP)
		self.add_var_opt("samples-min", samples_min)
		self.add_var_opt("samples-max", samples_max)
		self.add_var_opt("samples-max-64", samples_max_64)
		self.add_var_opt("samples-max-256", samples_max_256)
		self.add_var_opt("clipleft", clipleft)
		self.add_var_opt("clipright", clipright)
		svd_bank_name_path = os.path.split(template_bank)
		svd_bank_name = svd_bank_name_path[0] + "/svd_" + svd_bank_name_path[1]
		self.add_var_opt("write-svd-bank", svd_bank_name)
		dag.output_cache.append(lal.CacheEntry(ifo, "-", segments.segment(0, 999999999), "file://localhost%s" % (svd_bank_name,)))
		for p in p_node:
			self.add_parent(p)
		dag.add_node(self)

def parse_command_line():
	parser = OptionParser()
	parser.add_option("--instrument", help = "set the name of the instrument, required")
	parser.add_option("--reference-psd", metavar = "file", help = "Set the name of the reference psd file, required")
	parser.add_option("--bank-cache", metavar = "file", help = "Set the name of the bank cache, required")
	parser.add_option("--overlap", metavar = "num", type = "int", default = 0, help = "set the factor that describes the overlap of the sub banks, must be even!")
	parser.add_option("--samples-min", type = "int", default = 1024, help = "The minimum number of samples to use for time slices default 1024")
	parser.add_option("--samples-max-256", type = "int", default = 1024, help = "The maximum number of samples to use for time slices with frequencies above 256Hz, default 1024")
	parser.add_option("--samples-max-64", type = "int", default = 2048, help = "The maximum number of samples to use for time slices with frequencies above 64Hz, default 2048")
	parser.add_option("--samples-max", type = "int", default = 4096, help = "The maximum number of samples to use for time slices with frequencies below 64Hz, default 4096")
	parser.add_option("--stagger", action = "store_true", help = "A hacky way to stagger the number of samples used in the SVD if you have a broad mass range and the cache is organized by chirp mass from small to big.  The first half gets 1024 the second gets 2048. This helps balance the number of slices with the number of principle components when the template duration varies wildly through the cache.  Using this option disables the samples-* options.  FIXME this will be removed once we know how to automate this in a better way.")
	parser.add_option("--flow", metavar = "num", type = "float", default = 40, help = "set the low frequency cutoff, default 40 (Hz)")
	parser.add_option("--output-name", help = "set the base name of the output, required")
	parser.add_option("-v", "--verbose", action = "store_true", help = "Be verbose.")
	options, filenames = parser.parse_args()

	if options.overlap % 2:
		raise ValueError("overlap must be even")

	return options, filenames

options, filenames = parse_command_line()


# get input arguments
ifo = options.instrument
ref_psd = options.reference_psd
input_cache = options.bank_cache
psdname = ref_psd.split('.')[0]

try: os.mkdir("logs")
except: pass
dag = bank_DAG(options.output_name)

svdJob = gstlal_svd_bank_job(tag_base=psdname + "_gstlal_svd_bank")
svdNode = {}

stagger = ((2048, 2048, 2048, 4096), (1024, 1024, 2048, 4096))

# assumes cache is sorted by chirpmass or whatever the SVD sorting algorithm that was chosen
files = [lal.CacheEntry(line).path() for line in open(input_cache)]

for i, f in enumerate(files):
	# handle the edges by not clipping so you retain the template bank as intended.  
	clipleft = clipright = options.overlap / 2 # overlap must be even
	if i == 0:
		clipleft = 0
	if i == len(files) - 1:
		clipright = 0
	if options.stagger:
		samps = stagger[int(float(i) / len(files) * len(stagger))]
		svdNode[f] = gstlal_svd_bank_node(svdJob, dag, f, ifo, reference_psd = ref_psd, flow = options.flow, clipleft = clipleft, clipright = clipright, samples_min = samps[0], samples_max_256 = samps[1], samples_max_64 = samps[2], samples_max = samps[3])
	else:
		svdNode[f] = gstlal_svd_bank_node(svdJob, dag, f, ifo, reference_psd = ref_psd, flow = options.flow, clipleft = clipleft, clipright = clipright, samples_min = options.samples_min, samples_max_256 = options.samples_max_256, samples_max_64 = options.samples_max, samples_max = options.samples_max)

dag.write_sub_files()
dag.write_dag()
dag.write_script()
dag.write_cache()



